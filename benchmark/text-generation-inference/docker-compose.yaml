version: '3.7'

services:
  tgi-1:
    image: neuronx-tgi:latest
    ports:
      - "8081:8081"
    environment:
      - PORT=8081
      - MODEL_ID=${MODEL_ID}
      - HF_BATCH_SIZE=${HF_BATCH_SIZE}
      - HF_SEQUENCE_LENGTH=${HF_SEQUENCE_LENGTH}
      - HF_AUTO_CAST_TYPE=${HF_AUTO_CAST_TYPE}
      - HF_NUM_CORES=8
      - MAX_BATCH_SIZE=${MAX_BATCH_SIZE}
      - MAX_INPUT_LENGTH=${MAX_INPUT_LENGTH}
      - MAX_TOTAL_TOKENS=${MAX_TOTAL_TOKENS}
      - MAX_CONCURRENT_REQUESTS=512
    devices:
      - "/dev/neuron0"
      - "/dev/neuron1"
      - "/dev/neuron2"
      - "/dev/neuron3"

  tgi-2:
    image: neuronx-tgi:latest
    ports:
      - "8082:8082"
    environment:
      - PORT=8082
      - MODEL_ID=${MODEL_ID}
      - HF_BATCH_SIZE=${HF_BATCH_SIZE}
      - HF_SEQUENCE_LENGTH=${HF_SEQUENCE_LENGTH}
      - HF_AUTO_CAST_TYPE=${HF_AUTO_CAST_TYPE}
      - HF_NUM_CORES=8
      - MAX_BATCH_SIZE=${MAX_BATCH_SIZE}
      - MAX_INPUT_LENGTH=${MAX_INPUT_LENGTH}
      - MAX_TOTAL_TOKENS=${MAX_TOTAL_TOKENS}
      - MAX_CONCURRENT_REQUESTS=512
    devices:
      - "/dev/neuron4"
      - "/dev/neuron5"
      - "/dev/neuron6"
      - "/dev/neuron7"

  tgi-3:
    image: neuronx-tgi:latest
    ports:
      - "8083:8083"
    environment:
      - PORT=8083
      - MODEL_ID=${MODEL_ID}
      - HF_BATCH_SIZE=${HF_BATCH_SIZE}
      - HF_SEQUENCE_LENGTH=${HF_SEQUENCE_LENGTH}
      - HF_AUTO_CAST_TYPE=${HF_AUTO_CAST_TYPE}
      - HF_NUM_CORES=8
      - MAX_BATCH_SIZE=${MAX_BATCH_SIZE}
      - MAX_INPUT_LENGTH=${MAX_INPUT_LENGTH}
      - MAX_TOTAL_TOKENS=${MAX_TOTAL_TOKENS}
      - MAX_CONCURRENT_REQUESTS=512
    devices:
      - "/dev/neuron8"
      - "/dev/neuron9"
      - "/dev/neuron10"
      - "/dev/neuron11"

  loadbalancer:
    image: nginx:alpine
    ports:
      - "8080:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - tgi-1
      - tgi-2
      - tgi-3
    deploy:
      placement:
        constraints: [node.role == manager]
