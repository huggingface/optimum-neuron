MODEL_ID=meta-llama/Llama-3.1-8B-Instruct
BATCH_SIZE=32
SEQUENCE_LENGTH=4096
TENSOR_PARALLEL_SIZE=4
