{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02e90d5c",
   "metadata": {},
   "source": [
    "# Deploy Embedding Models on AWS Inferentia2 with Amazon SageMaker\n",
    "\n",
    "[BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5) is a fine-tuned BERT model to map any text to a low-dimensional dense vector which can be used for tasks like retrieval, classification, clustering, or semantic search. It works perfectly for vector databases for LLMs.\n",
    "\n",
    "In this end-to-end tutorial, you will learn how to deploy and speed up Embeddings Model inference using AWS Inferentia2 with Hugging Face Optimum Neuron on Amazon SageMaker. We are going to use the Hugging Face Inference Neuron Container, a purpose-build Inference Container to easily deploy transformers and diffusers models on AWS Inferentia2 and Trainium. \n",
    "\n",
    "You will learn how to: \n",
    "1. Setup the development environment\n",
    "2. Pull from the Hub the pre-compiled model for AWS Neuron (Inferentia2)\n",
    "3. Create a custom `inference.py` script for `embeddings`\n",
    "4. Upload the neuron model and inference script to Amazon S3\n",
    "5. Deploy a Real-time Inference Endpoint on Amazon SageMaker\n",
    "6. Run and evaluate Inference performance of Embeddings Model on Inferentia2\n",
    "\n",
    "Let's get started! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3937fa9-bdb8-4bad-ac67-96b33595ba60",
   "metadata": {},
   "source": [
    "## 1. Setup development environment\n",
    "\n",
    "For this tutorial, we are going to use a Notebook Instance in Amazon SageMaker  with the Python 3 (ipykernel) and the sagemaker python SDK to deploy [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5) to a SageMaker inference endpoint.\n",
    "\n",
    "As a first step, make sur you have the latest version of `optimum-neuron`and the SageMaker SDK installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b13772b-24c5-46ab-9bc0-d09dbd299aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required packages\n",
    "%pip install sagemaker --upgrade --quiet\n",
    "%pip install optimum-neuron --upgrade --quiet\n",
    "# restart your kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a10a98-3513-43fd-aa18-17623acbd940",
   "metadata": {},
   "source": [
    "Then, instantiate the sagemaker role and session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f0802c-0a7c-4b93-86b7-bbaaaf94a589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    role = iam.get_role(RoleName=\"sagemaker_execution_role\")[\"Role\"][\"Arn\"]\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fba05e1-0d10-4a72-a1b6-b142e9daaa5d",
   "metadata": {},
   "source": [
    "Finally, you need to log in the Hugging Face Hub to access the model artefacts, using a [User Access Token](https://huggingface.co/docs/hub/en/security-tokens) with read access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ca391f-290d-4743-af24-2d5266227329",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea980be8-ce0e-4fb0-bfa6-8a889258d843",
   "metadata": {},
   "source": [
    "## 2. Pull from the Hub the pre-compiled model for AWS Neuron (Inferentia2)\n",
    "\n",
    "At the time of writing, the [AWS Inferentia2 does not support dynamic shapes for inference](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-features/dynamic-shapes.html?highlight=dynamic%20shapes#), which means that the input size needs to be static for compiling and inference. \n",
    "\n",
    "In simpler terms, this means we need to define the input shapes for our prompt (sequence length), batch size, height and width of the image.\n",
    "\n",
    "We precompiled the model with the following parameters and pushed it to the Hugging Face Hub [here](https://huggingface.co/aws-neuron/bge-base-en-v1-5-seqlen-384-bs-1): \n",
    "* `sequence_length`: 384\n",
    "* `batch_size`: 1\n",
    "* `neuron`: 2.21.1\n",
    "\n",
    "_Note: If you want to compile your own model or with other parameters, follow the guide on how to [export a model to Inferentia](https://huggingface.co/docs/optimum-neuron/en/guides/export_model)._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af8b246-0ada-4fb5-b41d-cfb7f48a2e6e",
   "metadata": {},
   "source": [
    "Let's download it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8b5857-ac8c-42fd-a5dd-a2798130cb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    " \n",
    "# compiled model id\n",
    "compiled_model_id = \"aws-neuron/bge-base-en-v1-5-seqlen-384-bs-1\"\n",
    " \n",
    "# save compiled model to local directory\n",
    "save_directory = \"/tmp/embedding_model\"\n",
    "# Downloads our compiled model from the HuggingFace Hub\n",
    "# using the revision as neuron version reference\n",
    "# and makes sure we exlcude the symlink files and \"hidden\" files, like .DS_Store, .gitignore, etc.\n",
    "snapshot_download(\n",
    "    compiled_model_id,\n",
    "    revision=\"2.21.1\",\n",
    "    local_dir=save_directory,\n",
    "    allow_patterns=[\"[!.]*.*\"]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9997e9db",
   "metadata": {},
   "source": [
    "## 3. Create a custom `inference.py` script for `embeddings`\n",
    "\n",
    "We need to provide a custom `inference.py` script which will override the default inference handler used in the endpoint. We will override `model_fn`to load a Neuron-compiled model and tokenizer, and `predict_fn` to generate and normalize sentence embeddings from input text with a Neuron model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4246c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir {save_directory}/code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce675df9",
   "metadata": {},
   "source": [
    "We are using the `NEURON_RT_NUM_CORES=1` to make sure that each HTTP worker uses 1 Neuron core to maximize throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce41529",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {save_directory}/code/inference.py\n",
    "import os\n",
    "# To use one neuron core per worker\n",
    "os.environ[\"NEURON_RT_NUM_CORES\"] = \"1\"\n",
    "from optimum.neuron import NeuronModelForFeatureExtraction\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch_neuronx\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    # load local converted model and  tokenizer\n",
    "    model = NeuronModelForFeatureExtraction.from_pretrained(model_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def predict_fn(data, pipeline):\n",
    "    model, tokenizer = pipeline\n",
    "  \n",
    "    # extract body \n",
    "    inputs = data.pop(\"inputs\", data)\n",
    "    \n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(inputs,return_tensors=\"pt\",truncation=True,max_length=model.config.neuron[\"static_sequence_length\"])\n",
    "\n",
    "    # Compute embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    \n",
    "    # Perform pooling. In this case, cls pooling.\n",
    "    sentence_embeddings = model_output[0][:, 0]\n",
    "    # normalize embeddings\n",
    "    sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)    \n",
    "    \n",
    "    return {\"embeddings\":sentence_embeddings[0].tolist()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "144d8ccb",
   "metadata": {},
   "source": [
    "## 3. Upload the neuron model and inference script to Amazon S3\n",
    "\n",
    "Before we can deploy our neuron model to Amazon SageMaker we need to create a `model.tar.gz` archive with all our model artifacts saved into, e.g.Â `model.neuron` and upload this to Amazon S3.\n",
    "\n",
    "Make sure you have suffiscient permissions attached to your Sagemaker Role to upload to an S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3808b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model.tar.gz archive with all the model artifacts and the inference.py script.\n",
    "%cd {save_directory}\n",
    "!tar zcvf model.tar.gz *\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a6f330",
   "metadata": {},
   "source": [
    "Now we can upload our `model.tar.gz` to our session S3 bucket with `sagemaker`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6146af09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "# create s3 uri\n",
    "s3_model_path = f\"s3://{sess.default_bucket()}/neuronx/embeddings\"\n",
    "\n",
    "# upload model.tar.gz\n",
    "s3_model_uri = S3Uploader.upload(local_path=f\"{save_directory}/model.tar.gz\",desired_s3_uri=s3_model_path)\n",
    "print(f\"model artifcats uploaded to {s3_model_uri}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04e1395a",
   "metadata": {},
   "source": [
    "## 4. Deploy a Real-time Inference Endpoint on Amazon SageMaker\n",
    "\n",
    "After we have uploaded ourÂ `model.tar.gz`Â to Amazon S3 can we create a customÂ `HuggingfaceModel`. This class will be used to create and deploy our real-time inference endpoint on Amazon SageMaker.\n",
    "\n",
    "The `inf2.xlarge` instance type is the smallest instance type with AWS Inferentia2 support. It comes with 1 Inferentia2 chip with 2 Neuron Cores. This means we can use 2 Model server workers to maximize throughput and run 2 inferences in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41522ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   model_data=s3_model_uri,        # path to your model.tar.gz on s3\n",
    "   role=role,                      # iam role with permissions to create an Endpoint\n",
    "#    transformers_version=\"4.34.1\",  # transformers version used\n",
    "#    pytorch_version=\"1.13.1\",       # pytorch version used\n",
    "#    py_version='py310',             # python version used\n",
    "   image_uri=\"\" #TODO\n",
    "   model_server_workers=2,         # number of workers for the model server\n",
    ")\n",
    "\n",
    "# deploy the endpoint endpoint\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,      # number of instances\n",
    "    instance_type=\"ml.inf2.xlarge\", # AWS Inferentia Instance\n",
    "    volume_size = 100\n",
    ")\n",
    "# ignore the \"Your model is not compiled. Please compile your model before using Inferentia.\" warning, we already compiled our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c858560",
   "metadata": {},
   "source": [
    "# 5. Run and evaluate Inference performance of Embeddings Model on Inferentia2\n",
    "\n",
    "TheÂ `.deploy()`Â returns anÂ `HuggingFacePredictor`Â object which can be used to request inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "da2ff049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lenght of embeddings: 768\n",
      "first 10 elements of embeddings: [-0.03262409567832947, -0.03421149030327797, 0.041609905660152435, -0.0013438157038763165, 0.027236895635724068, -0.05484848469495773, 0.02483028545975685, -0.029165517538785934, -0.02704770117998123, 0.004101182334125042]\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "  \"inputs\": \"the mesmerizing performances of the leads keep the film grounded and keep the audience riveted .\",\n",
    "}\n",
    "\n",
    "res = predictor.predict(data=data)\n",
    "\n",
    "\n",
    "# print some results \n",
    "print(f\"lenght of embeddings: {len(res['embeddings'])}\")\n",
    "print(f\"first 10 elements of embeddings: {res['embeddings'][:10]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a146346",
   "metadata": {},
   "source": [
    "Awesome we can now generate embeddings with our model, Lets test the performance of our model.\n",
    "\n",
    "A load test will we send 10,000 requests to our endpoint use threading with 10 concurrent threads. We will measure the average latency and throughput of our endpoint. We are going to sent an input of 300 tokens to have a total of 3 Million tokens, but remember the model is compiled with a `sequence_length` of 384. This means that the model will pad the input to 384 tokens, this increases the latency a bit. \n",
    "\n",
    "> We decided to use 300 tokens as input length to find the balance between shorter inputs which are padded and longer inputs, which are truncated. If you know your chunk size, we recommend to compile the model with that length to get maximum performance.\n",
    "\n",
    "_Note: When running the load test, the requests are sent from europe and the endpoint is deployed in us-east-2. This adds a network overhead to it._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "237f198c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of threads: 10\n",
      "number of requests per thread: 1000\n",
      "total time: 50 seconds\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import time \n",
    "number_of_threads = 10\n",
    "number_of_requests = int(10000 // number_of_threads)\n",
    "print(f\"number of threads: {number_of_threads}\")\n",
    "print(f\"number of requests per thread: {number_of_requests}\")\n",
    "\n",
    "def send_rquests():\n",
    "    for _ in range(number_of_requests):\n",
    "        # input counted at https://huggingface.co/spaces/Xenova/the-tokenizer-playground for 100 tokens\n",
    "        predictor.predict(data={\"inputs\": \"Hugging Face is a company and a popular platform in the field of natural language processing (NLP) and machine learning. They are known for their contributions to the development of state-of-the-art models for various NLP tasks and for providing a platform that facilitates the sharing and usage of pre-trained models. One of the key offerings from Hugging Face is the Transformers library, which is an open-source library for working with a variety of pre-trained transformer models, including those for text generation, translation, summarization, question answering, and more. The library is widely used in the research and development of NLP applications and is supported by a large and active community. Hugging Face also provides a model hub where users can discover, share, and download pre-trained models. Additionally, they offer tools and frameworks to make it easier for developers to integrate and use these models in their own projects. The company has played a significant role in advancing the field of NLP and making cutting-edge models more accessible to the broader community.\"})\n",
    "\n",
    "# Create multiple threads\n",
    "threads = [threading.Thread(target=send_rquests) for _ in range(number_of_threads) ]\n",
    "# start all threads\n",
    "start = time.time()\n",
    "[t.start() for t in threads]\n",
    "# wait for all threads to finish\n",
    "[t.join() for t in threads]\n",
    "print(f\"total time: {round(time.time() - start)} seconds\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0b8e25d",
   "metadata": {},
   "source": [
    "Sending 10,000 requests or generating 3 million tokens took around 50 seconds. This means we can run around ~200 inferences per second.\n",
    "When we inspect the latency of the endpoint through cloudwatch we can see that the average request latency is around 9ms. This means we can serve around 222 inferences per second (having 2 HTTP workers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4d916b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"https://console.aws.amazon.com/cloudwatch/home?region={sess.boto_region_name}#metricsV2:graph=~(metrics~(~(~'AWS*2fSageMaker~'ModelLatency~'EndpointName~'{predictor.endpoint_name}~'VariantName~'AllTraffic))~view~'timeSeries~stacked~false~region~'{sess.boto_region_name}~start~'-PT5M~end~'P0D~stat~'Average~period~30);query=~'*7bAWS*2fSageMaker*2cEndpointName*2cVariantName*7d*20{predictor.endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be140397",
   "metadata": {},
   "source": [
    "The average latency for our Embeddings model is `9ms`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f4690f-707d-4ff4-8601-a181b2059f84",
   "metadata": {},
   "source": [
    "**Price / performance ratio**\n",
    "\n",
    "In this post, we deployed a top open source Embeddings Model (BGE) on a single `inf2.xlarge` instance costing $0.76/hour on Amazon SageMaker using Optimum Neuron. We are able to run 2 replicas of the model on a single instance with a avg. model latency of 9ms for inputs of 300 tokens with a max sequence length of 384 and a throughput without network overhead of 222 inferences per second.\n",
    "\n",
    "This means we can create (300 tokens * 222 requests) 66,600 tokens per second, 3,996,000 tokens per minute and 239,760,000 tokens per hour. This leads to a cost of `~$0.003 1M/tokens` if utilized well. \n",
    "\n",
    "For startups and companies looking into GPU alternative for generating emebddings Inferentia2 is a great option for not only efficient and fast but also cost-effective inference."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1030c87f",
   "metadata": {},
   "source": [
    "### Delete model and endpoint\n",
    "\n",
    "To clean up, we can delete the model and endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8917d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c281c456f1b8161c8906f4af2c08ed2c40c50136979eaae69688b01f70e9f4a9"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
