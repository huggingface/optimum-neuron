{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ed8bd00-eda6-4fd5-9910-da9c671e5acf",
   "metadata": {},
   "source": [
    "# Deploy Mistral to SageMaker + Inferentia2 using HF Optimum Neuron and TGI\n",
    "\n",
    "This guide will detail how to compile, deploy and run a **Mistral 7B** on AWS inferentia 2 + Amazon SageMaker.\n",
    "\n",
    "You will learn how to:\n",
    "- set up your AWS instance,\n",
    "- compile Mistral-7B model to the Neuron format using a SageMaker Training Job,\n",
    "- deploy the model and use it in any application.\n",
    "\n",
    "Note: This tutorial was created on Amazon SageMaker.\n",
    "\n",
    "## Prerequisite: Setup AWS environment\n",
    "\n",
    "*you can skip that section if you are already this notebook on SageMaker Studio.*\n",
    "\n",
    "In your AWS Account, follow the [SageMaker Studio Getting Started tutorial](https://aws.amazon.com/sagemaker/studio/) to run this notebook. Then, clone this repo (https://github.com/huggingface/optimum-neuron) into your environment. Navigate through the directories and double click on this notebook.\n",
    "\n",
    "Select the most appropriate kernel to initialize the notebook.  \n",
    "**SageMaker Studio Kernel**: Python 3 (ipykernel)  \n",
    "**SageMaker Studio Classic Kernel**: Python 3 (PyTorch 1.13 Python 3.9 CPU Optimized) / **Instance**: ml.t3.medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c924fea1-f219-4d43-b60d-2b71b01a7069",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "\n",
    "print(sagemaker.__version__)\n",
    "if not sagemaker.__version__ >= \"2.146.0\": print(\"You need to upgrade or restart the kernel if you already upgraded\")\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sess.default_bucket()\n",
    "region = sess.boto_region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875c969e-ea5e-4893-af12-a9e416eaf618",
   "metadata": {},
   "source": [
    "## 1. Download and compile Milstral using a SageMaker Job\n",
    "In this step we'll kick-off a SageMaker training job to download and compile the model to inf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e0b384-3206-4b3e-a778-73e8b37ee003",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"src\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc6da42-94b2-4973-8c35-f7a55ecd4d9a",
   "metadata": {},
   "source": [
    "#### Python requirements for model compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49aeb8be-1548-4aa0-904d-6374eba1c77d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile src/requirements.txt\n",
    "--extra-index-url https://pip.repos.neuron.amazonaws.com\n",
    "transformers==4.36.2\n",
    "optimum-neuron==0.0.19\n",
    "neuronx-distributed==0.6.0\n",
    "transformers-neuronx==0.9.474"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b839061f-4a4c-494b-810b-8e2e3a179e10",
   "metadata": {},
   "source": [
    "#### Compilation script\n",
    "This script will be executed by SageMaker. It will download the weights and compile Mistral. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b00905-0f32-48e6-8608-50885fc22898",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile src/compile.py\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "from transformers import AutoTokenizer\n",
    "from optimum.neuron import NeuronModelForCausalLM\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()    \n",
    "    parser.add_argument(\"--compilation_params\", type=str, required=True)\n",
    "    parser.add_argument(\"--model_id\", type=str, required=False, default=\"yam-peleg/Experiment26-7B\")    \n",
    "    parser.add_argument(\"--model_dir\", type=str, default=os.environ[\"SM_MODEL_DIR\"])        \n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    # parse compilation params\n",
    "    compilation_params = json.loads(args.compilation_params)\n",
    "    print(compilation_params)\n",
    "    # compile and save the model\n",
    "    model = NeuronModelForCausalLM.from_pretrained(args.model_id, export=True, **compilation_params)\n",
    "    model.save_pretrained(args.model_dir)\n",
    "    # now load and export tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_id)\n",
    "    tokenizer.save_pretrained(args.model_dir)\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c486648-eecf-4355-951b-bf77c602ac48",
   "metadata": {},
   "source": [
    "#### SageMaker Estimator to kick-off model compilation\n",
    "If you want to deploy your own model, just change the parameter **--model_id** and point it to the correct HuggingFace repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6238f0-ac95-47f7-8edb-13d9e4f41fb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "from sagemaker.pytorch import PyTorch\n",
    "num_cores=2\n",
    "seq_len=2048\n",
    "compilation_params= {\"auto_cast_type\": \"bf16\", \"batch_size\": 1, \"sequence_length\": seq_len, \"num_cores\": num_cores}\n",
    "model_id='yam-peleg/Experiment26-7B'\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"compile.py\", # Specify your train script\n",
    "    source_dir=\"src\",\n",
    "    role=role,\n",
    "    sagemaker_session=sess,\n",
    "    container_log_level=logging.DEBUG,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.trn1.32xlarge',\n",
    "    output_path=f\"s3://{bucket}/output\",\n",
    "    disable_profiler=True,\n",
    "    disable_output_compression=True,\n",
    "    \n",
    "    image_uri=f\"763104351884.dkr.ecr.{region}.amazonaws.com/pytorch-training-neuronx:1.13.1-neuronx-py310-sdk2.17.0-ubuntu20.04\",\n",
    "    \n",
    "    volume_size = 512,\n",
    "    hyperparameters={         \n",
    "        \"compilation_params\": f\"'{json.dumps(compilation_params)}'\",\n",
    "        \"model_id\": model_id\n",
    "    }\n",
    ")\n",
    "estimator.framework_version = '1.13.1' # workround when using image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0be382-182d-496c-aa10-b690f1a37238",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585145f3-635a-4bea-b5d8-e6029133acc6",
   "metadata": {},
   "source": [
    "## 2. Deploy the compiled model to SageMaker Endpoint + Inferentia2 + TGI + HF ON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1336262d-44ee-463c-a7e1-71e62cd6527c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "# depending on the inf2 instance you deploy the model you'll have more or less accelerators\n",
    "# we'll ask SageMaker to launch 1 worker per core\n",
    "\n",
    "model_data=estimator.model_data\n",
    "print(f\"Model data: {model_data}\")\n",
    "\n",
    "instance_type_idx=1 # default ml.inf2.8xlarge\n",
    "instance_types=['ml.inf2.xlarge', 'ml.inf2.8xlarge', 'ml.inf2.24xlarge','ml.inf2.48xlarge']\n",
    "num_workers=[2//num_cores,2//num_cores,12//num_cores,24//num_cores]\n",
    "\n",
    "print(f\"Instance type: {instance_types[instance_type_idx]}. Num SM workers: {num_workers[instance_type_idx]}\")\n",
    "pytorch_model = PyTorchModel(\n",
    "    image_uri=f\"763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-tgi-inference:1.13.1-optimum0.0.18-neuronx-py310-ubuntu22.04-v1.0\",\n",
    "    model_data=model_data,\n",
    "    role=role,    \n",
    "    name=name_from_base('tgi-llm'),\n",
    "    sagemaker_session=sess,\n",
    "    container_log_level=logging.DEBUG,\n",
    "    model_server_workers=num_workers[instance_type_idx], # 1 worker per inferentia chip\n",
    "    framework_version=\"1.13.1\",\n",
    "    env = {\n",
    "        'SAGEMAKER_MODEL_SERVER_TIMEOUT': '3600',\n",
    "        'HF_MODEL_ID': '/opt/ml/model/',\n",
    "        ## https://huggingface.co/docs/text-generation-inference/basic_tutorials/launcher\n",
    "        'MAX_BATCH_PREFILL_TOKENS': '1024',\n",
    "        'MAX_INPUT_LENGTH': '1024',\n",
    "        'MAX_TOTAL_TOKENS': str(seq_len)\n",
    "    }\n",
    "    # for production it is important to define vpc_config and use a vpc_endpoint\n",
    "    #vpc_config={\n",
    "    #    'Subnets': ['<SUBNET1>', '<SUBNET2>'],\n",
    "    #    'SecurityGroupIds': ['<SECURITYGROUP1>', '<DEFAULTSECURITYGROUP>']\n",
    "    #}\n",
    ")\n",
    "pytorch_model._is_compiled_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba15301-0841-4334-8a19-6cba43e3f93e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor = pytorch_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_types[instance_type_idx],\n",
    "    model_data_download_timeout=3600, # it takes some time to download all the artifacts and load the model\n",
    "    container_startup_health_check_timeout=1800\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95ba3b3-a53b-4363-8f9e-7060479d71cc",
   "metadata": {},
   "source": [
    "## 3. Run some basic tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9eb09a-c65f-4d95-a9f1-6713fff91394",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "predictor.serializer = JSONSerializer()\n",
    "predictor.deserializer = JSONDeserializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d89ad75-648e-4a1f-b230-3d0528bc5fcc",
   "metadata": {},
   "source": [
    "#### Simple Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f06a06c9-1b1f-4094-84e9-5fd4fa2c1b77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '\\n\\nDeep Learning is a subset of Machine Learning, which is a subset of Artificial Intelligence. It is a method of teaching a computer to learn and make decisions by mimicking the way the human brain processes data.\\n\\nDeep Learning algorithms are designed to recognize patterns in large datasets by using multiple layers of artificial neurons, which are loosely modeled after the biological neurons in the human brain.\\n\\nThe term \"deep\" in Deep Learning refers to the depth or number of layers in the neural network. The more layers a network has, the more complex the patterns it can learn.\\n\\nDeep Learning has shown'}]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict({\"inputs\":\"What is Deep Learning?\",\"parameters\":{\"max_new_tokens\":128}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb5052c-aa71-4311-9499-a5a0fb6d8a95",
   "metadata": {},
   "source": [
    "#### Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2a657d80-9dc2-4a87-9144-94732dfbdfe1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Deep Learning is a subset of Machine Learning, which is a subset of Artificial Intelligence. It is a method of teaching a computer to learn from and make sense of large amounts of data, in a similar way to the human brain, using a layered structure with neurons that process information.\n",
      "\n",
      "The term “deep” refers to the depth or number of layers in a neural network, which is the core of deep learning. The more layers a network has, the more complex the patterns it can learn.\n",
      "\n",
      "Deep learning has been responsible for many recent breakthroughs in the field of AI, including image and speech recognition, natural language processing, and self-driving cars.\n",
      "\n",
      "The key to deep learning is the ability to automatically learn hierarchical representations of data, which means the system can identify patterns and features at multiple levels of abstraction. This is in contrast to traditional machine learning methods, which often require manual feature engineering.\n",
      "\n",
      "Deep learning algorithms are typically inspired by the biological neural networks of the human brain, with artificial neurons and their interconnections, known as weights, being adjusted through a process called backpropagation to minimize the error between the predicted output and the actual output.\n",
      "\n",
      "Some popular deep learning architectures include Convolutional Neural Networks (CNNs) for image recognition, Recurrent Neural Networks (RNNs) for sequence data, and Transformer models for natural language processing.\n",
      "\n",
      "In summary, deep learning is a powerful technique within the broader field of AI, which enables computers to learn complex patterns and make predictions from large, often unstructured, datasets, by mimicking the structure and function of the human brain."
     ]
    }
   ],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "sm_client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "body = json.dumps({\"inputs\":\"What is Deep Learning?\",\"parameters\":{\"max_new_tokens\":512}, \"stream\": True})\n",
    "\n",
    "resp = sm_client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=predictor.endpoint_name,\n",
    "    Body=body,\n",
    "    ContentType='application/json',\n",
    "    Accept='application/json',\n",
    ")\n",
    "text = \"\"\n",
    "for e in resp['Body']:\n",
    "    tok = e['PayloadPart']['Bytes'].decode('utf-8')\n",
    "    if tok.startswith('data'): \n",
    "        try:\n",
    "            tok = json.loads(tok[5:])\n",
    "            print(tok['token']['text'], end='')    \n",
    "        except Exception as e:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6070a08-9aed-4483-9982-96d54a516e6a",
   "metadata": {},
   "source": [
    "## 4. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70f6236-d7cd-45d9-89e5-fd38696fddcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
