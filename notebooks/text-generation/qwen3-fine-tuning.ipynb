{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune Qwen 3 on AWS Trainium \n",
    "\n",
    "This tutorial will teach how to fine-tune open LLMs like [Qwen 3](https://huggingface.co/Qwen/Qwen3-8B) on AWS Trainium.  In our example, we are going to leverage Hugging Face Optimum Neuron, [Transformers](https://huggingface.co/docs/transformers/index) and datasets.\n",
    "\n",
    "## Quick intro: AWS Trainium\n",
    "\n",
    "[AWS Trainium (Trn1)](https://aws.amazon.com/de/ec2/instance-types/trn1/) is a purpose-built EC2 for deep learning (DL) training workloads. Trainium is the successor of [AWS Inferentia](https://aws.amazon.com/ec2/instance-types/inf1/?nc1=h_ls) focused on high-performance training workloads. Trainium has been optimized for training natural language processing, computer vision, and recommender models used.\n",
    "\n",
    "The biggest Trainium instance, the `trn1.32xlarge` comes with over 500GB of memory, making it easy to fine-tune ~10B parameter models on a single instance. Below you will find an overview of the available instance types. More details [here](https://aws.amazon.com/en/ec2/instance-types/trn1/#Product_details):\n",
    "\n",
    "| instance size | accelerators | accelerator memory | vCPU | CPU Memory | price per hour |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| trn1.2xlarge | 1 | 32 | 8 | 32 | \\$1.34 |\n",
    "| trn1.32xlarge | 16 | 512 | 128 | 512 | \\$21.50 |\n",
    "| trn1n.32xlarge (2x bandwidth) | 16 | 512 | 128 | 512 | \\$24.78 |\n",
    "\n",
    "---\n",
    "\n",
    "*Note: This tutorial was created on a trn1.32xlarge AWS EC2 Instance.*\n",
    "\n",
    "\n",
    "## 1. Setup AWS environment\n",
    "\n",
    "In this example, we will use the `trn1.32xlarge` instance on AWS with 16 Accelerator, including 32 Neuron Cores and the [Hugging Face Neuron Deep Learning AMI](https://aws.amazon.com/marketplace/pp/prodview-gr3e6yiscria2). The Hugging Face AMI comes with all important libraries, like Transformers, Datasets, Optimum and Neuron packages pre-installed this makes it super easy to get started, since there is no need for environment management.\n",
    "\n",
    "If you want to know more about distributed training you can take a look at the [documentation](https://huggingface.co/docs/optimum-neuron/guides/distributed_training).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and prepare the dataset\n",
    "\n",
    "We will use a [simple recipes](https://huggingface.co/datasets/tengomucho/simple_recipes), to make LLM get better at suggesting delicious ideas.\n",
    "\n",
    "```python\n",
    "{\n",
    "    'recipes': \"- Preheat oven to 350 degrees\\n- Butter two 9x5' loaf pans\\n- Cream the sugar and the butter until light and whipped\\n- Add the bananas, eggs, lemon juice, orange rind\\n- Beat until blended uniformly\\n- Be patient, and beat until the banana lumps are gone\\n- Sift the dry ingredients together\\n- Fold lightly and thoroughly into the banana mixture\\n- Pour the batter into prepared loaf pans\\n- Bake for 45 to 55 minutes, until the loaves are firm in the middle and the edges begin to pull away from the pans\\n- Cool the loaves on racks for 30 minutes before removing from the pans\\n- Freezes well\",\n",
    "    'names': 'Beat this banana bread'\n",
    "}\n",
    "```\n",
    "\n",
    "To load theÂ `simple_recipes`Â dataset, we use theÂ `load_dataset()`Â method from the ðŸ¤— Datasets library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset_id = \"tengomucho/simple_recipes\"\n",
    "recipes = load_dataset(dataset_id, split=\"train\")\n",
    "\n",
    "dataset_size = len(recipes)\n",
    "print(f\"dataset size: {dataset_size}\")\n",
    "print(recipes[randrange(dataset_size)])\n",
    "# dataset size: 20000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tune our model we need to convert our structured examples into a collection of quotes with a given context, so we define our tokenization function that we will be able to map on the dataset.\n",
    "\n",
    "The dataset should be structured with input-output pairs, where each input is a prompt and the output is the expected response from the model. We will make use of the model's tokenizer chat template and preprocess the dataset to be fed to the trainer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"Qwen/Qwen3-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    recipes = examples[\"recipes\"]\n",
    "    names = examples[\"names\"]\n",
    "\n",
    "    chats = []\n",
    "    for recipe, name in zip(recipes, names):\n",
    "        # Append the EOS token to the response\n",
    "        recipe += tokenizer.eos_token\n",
    "\n",
    "        chat = [\n",
    "            {\"role\": \"user\", \"content\": f\"How can I make {name}?\"},\n",
    "            {\"role\": \"assistant\", \"content\": recipe},\n",
    "        ]\n",
    "        tokenizer.apply_chat_template(\n",
    "            chat, tokenize=False, add_generation_prompt=False\n",
    "        )\n",
    "\n",
    "        chats.append(chat)\n",
    "    return {\"messages\": chats}\n",
    "\n",
    "dataset = recipes.map(\n",
    "    preprocess_function, batched=True, remove_columns=recipes.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our formatting function on a random example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[randrange(dataset_size)])\n",
    "# {\n",
    "#     'messages': [\n",
    "#         {'content': 'How can I make Aunt liz s almond broccoli casserole', 'role': 'user'},\n",
    "#         {\n",
    "#             'content': '- Pre-stream broccoli for about 5 minutes\\n- Saute onions and garlic in butter\\n- Add soup, cheese whiz and mushrooms to sauteed onion mixture\\n- Put broccoli into a greased casserole dish and pour sauce over it\\n- Sprinkle the almonds over this and then sprinkle the croutons on top\\n- Bake at 350 degf for 30 \n",
    "# minutes<|im_end|>',\n",
    "#             'role': 'assistant'\n",
    "#         }\n",
    "#     ]\n",
    "# }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-tune Qwen 3 on AWS Trainium using the `NeuronSFTTrainer` and `PEFT`\n",
    "\n",
    "Usually, to fine-tune PyTorch-based transformer models you would use [PEFT](https://huggingface.co/PEFT) to use LoRA adapters to save memory and use the`SFTTrainer` the perform supervised fine-tuning.\n",
    "\n",
    "On AWS Trainium, `optimum-neuron` offers a 1-to-1 replacement with the `NeuronSFTTrainer`, optimized to take advantage of the multiple cores available on this setup.\n",
    "\n",
    "When it comes to distributed training on AWS Trainium there are few things we need to take care of. Since Qwen3 is a big model it does not fit on a single accelerator. The `NeuronSFTTrainer` supports different distributed training techniques (DDP, Tensor Parallelism, etc) to solve this.\n",
    "\n",
    "Loading the model an preparing the LoRA adapter is very similar to what you would do with other accelerators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import LoraConfig\n",
    "\n",
    "from optimum.neuron import NeuronSFTConfig, NeuronSFTTrainer, NeuronTrainingArguments\n",
    "from optimum.neuron.models.training import Qwen3ForCausalLM\n",
    "from optimum.neuron.models.training.config import TrainingNeuronConfig\n",
    "\n",
    "# This is necessary to pass the training configuration\n",
    "trn_config = TrainingNeuronConfig(tensor_parallel_size=8, pipeline_parallel_size=1)\n",
    "# Define your own training arguments\n",
    "training_args = NeuronTrainingArguments()\n",
    "\n",
    "dtype = torch.bfloat16 # This will allow to use mixed-precision\n",
    "model = Qwen3ForCausalLM.from_pretrained(model_id, trn_config, torch_dtype=dtype)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=128,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"embed_tokens\", \"q_proj\", \"v_proj\", \"o_proj\", \"k_proj\", \"up_proj\", \"down_proj\", \"gate_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "args = training_args.to_dict()\n",
    "packing = True\n",
    "sft_config = NeuronSFTConfig(\n",
    "    max_seq_length=8192,\n",
    "    packing=packing,\n",
    "    **args,\n",
    ")\n",
    "\n",
    "def formatting_function(examples):\n",
    "    return tokenizer.apply_chat_template(examples[\"messages\"], tokenize=False, add_generation_prompt=False)\n",
    "\n",
    "trainer = NeuronSFTTrainer(\n",
    "    args=sft_config,\n",
    "    model=model,\n",
    "    peft_config=config,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    formatting_func=formatting_function,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepared a script, [sft_finetuning_qwen3.py](https://github.com/huggingface/optimum-neuron/blob/main/notebooks/text-generation/scripts/sft_finetuning_qwen3.py) to fine-tune Qwen3 that contains everything mentioned in this tutorial. You can launch it with the torchrun command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "PROCESSES_PER_NODE=32\n",
    "NUM_EPOCHS=3\n",
    "TP_DEGREE=8\n",
    "BS=1\n",
    "GRADIENT_ACCUMULATION_STEPS=8\n",
    "LOGGING_STEPS=2\n",
    "MODEL_NAME=\"Qwen/Qwen3-3-8B\"\n",
    "OUTPUT_DIR=\"$(echo $MODEL_NAME | cut -d'/' -f2)-finetuned\"\n",
    "MAX_STEPS=-1\n",
    "DISTRIBUTED_ARGS=\"--nproc_per_node $PROCESSES_PER_NODE\"\n",
    "\n",
    "torchrun $DISTRIBUTED_ARGS notebooks/text-generation/scripts/sft_finetuning_qwen3.py \\\n",
    "  --model_id $MODEL_NAME \\\n",
    "  --num_train_epochs $NUM_EPOCHS \\\n",
    "  --do_train \\\n",
    "  --max_steps $MAX_STEPS \\\n",
    "  --per_device_train_batch_size $BS \\\n",
    "  --per_device_eval_batch_size $BS \\\n",
    "  --gradient_accumulation_steps $GRADIENT_ACCUMULATION_STEPS \\\n",
    "  --gradient_checkpointing true \\\n",
    "  --bf16 true \\\n",
    "  --save_steps 20 \\\n",
    "  --tensor_parallel_size $TP_DEGREE \\\n",
    "  --logging_steps $LOGGING_STEPS \\\n",
    "  --save_total_limit -1 \\\n",
    "  --output_dir $OUTPUT_DIR \\\n",
    "  --lr_scheduler_type \"cosine\" \\\n",
    "  --overwrite_output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we provide this command in a shell script called [sft_finetuning_qwen3.sh](https://github.com/huggingface/optimum-neuron/blob/main/notebooks/text-generation/scripts/sft_finetuning_qwen3.sh)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Consolidate and Test finetuned model\n",
    "\n",
    "Optimum Neuron trains and serializes model shard files separately, meaning that they need to be consolidated (i.e.: re-merged) to be used.\n",
    "\n",
    "To do this, you can use the Optimum CLI as suggested here:\n",
    "\n",
    "```bash\n",
    "optimum-cli neuron consolidate Qwen3-8B-finetuned Qwen3-8B-finetuned/adapter_default\n",
    "````\n",
    "\n",
    "This will create an `adapter_model.safetensors` file, the LoRA adapter weights that we trained in the previous step. We can now reload the model and merge it, so it can be loaded for evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen3-8B\"\n",
    "ADAPTER_PATH = 'Qwen3-8B-finetuned/adapter_default'\n",
    "MERGED_MODEL_PATH = 'Qwen3-8B-recipes'\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Load adapter configuration and model\n",
    "adapter_config = PeftConfig.from_pretrained(ADAPTER_PATH)\n",
    "finetuned_model = PeftModel.from_pretrained(model, ADAPTER_PATH, config=adapter_config)\n",
    "\n",
    "print(\"Saving tokenizer\")\n",
    "tokenizer.save_pretrained(MERGED_MODEL_PATH)\n",
    "print(\"Saving model\")\n",
    "finetuned_model = finetuned_model.merge_and_unload()\n",
    "finetuned_model.save_pretrained(MERGED_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this step is done, it is possible to test the model with a new prompt.\n",
    "\n",
    "You have successfully created a fine-tuned model from Qwen3!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_neuron_venv_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
