{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune Llama on AWS Trainium \n",
    "\n",
    "This tutorial will teach how to fine-tune open LLMs like [Llama 2](https://huggingface.co/meta-llama/Llama-2-70b-hf) on AWS Trainium.  In our example, we are going to leverage Hugging Face Optimum Neuron, [Transformers](https://huggingface.co/docs/transformers/index)and datasets. \n",
    "\n",
    "You will learn how to:\n",
    "\n",
    "1. [Setup AWS environment](#1-setup-aws-environment)\n",
    "2. [Load and process the dataset](#2-load-and-prepare-the-dataset)\n",
    "3. [Fine-tune Llama on AWS Trainium using the `NeuronTrainer`](#3-fine-tune-llama-on-aws-trainium-using-the-neurontrainer)\n",
    "4. [Evalaute and test fine-tuned Llama model](#4-evalaute-and-test-fine-tuned-llama-model)\n",
    "\n",
    "## Quick intro: AWS Trainium\n",
    "\n",
    "[AWS Trainium (Trn1)](https://aws.amazon.com/de/ec2/instance-types/trn1/) is a purpose-built EC2 for deep learning (DL) training workloads. Trainium is the successor of [AWS Inferentia](https://aws.amazon.com/ec2/instance-types/inf1/?nc1=h_ls) focused on high-performance training workloads. Trainium has been optimized for training natural language processing, computer vision, and recommender models used. The accelerator supports a wide range of data types, including FP32, TF32, BF16, FP16, UINT8, and configurable FP8. \n",
    "\n",
    "The biggest Trainium instance, the `trn1.32xlarge` comes with over 500GB of memory, making it easy to fine-tune ~10B parameter models on a single instance. Below you will find an overview of the available instance types. More details [here](https://aws.amazon.com/de/ec2/instance-types/trn1/#Product_details):\n",
    "\n",
    "| instance size | accelerators | accelerator memory | vCPU | CPU Memory | price per hour |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| trn1.2xlarge | 1 | 32 | 8 | 32 | $1.34 |\n",
    "| trn1.32xlarge | 16 | 512 | 128 | 512 | $21.50 |\n",
    "| trn1n.32xlarge (2x bandwidth) | 16 | 512 | 128 | 512 | $24.78 |\n",
    "\n",
    "---\n",
    "\n",
    "*Note: This tutorial was created on a trn1.32xlarge AWS EC2 Instance.* \n",
    "\n",
    "\n",
    "## 1. Setup AWS environment\n",
    "\n",
    "In this example, we will use the `trn1.32xlarge` instance on AWS with 16 Accelerator, including 32 Neuron Cores and the [Hugging Face Neuron Deep Learning AMI](https://aws.amazon.com/marketplace/pp/prodview-gr3e6yiscria2). The Hugging Face AMI comes with all important libraries, like Transformers, Datasets, Optimum and Neuron packages pre-installed this makes it super easy to get started, since there is no need for environment management.\n",
    "\n",
    "This blog post doesn’t cover how to create the instance in detail. You can check out my previous blog about [“Setting up AWS Trainium for Hugging Face Transformers”](https://www.philschmid.de/setup-aws-trainium), which includes a step-by-step guide on setting up the environment. \n",
    "\n",
    "Once the instance is up and running, we can ssh into it. But instead of developing inside a terminal we want to use a `Jupyter` environment, which we can use for preparing our dataset and launching the training. For this, we need to add a port for forwarding in the `ssh` command, which will tunnel our localhost traffic to the Trainium instance.\n",
    "\n",
    "```bash\n",
    "PUBLIC_DNS=\"\" # IP address, e.g. ec2-3-80-....\n",
    "KEY_PATH=\"\" # local path to key, e.g. ssh/trn.pem\n",
    "\n",
    "ssh -L 8080:localhost:8080 -i ${KEY_NAME}.pem ubuntu@$PUBLIC_DNS\n",
    "```\n",
    "\n",
    "Lets now pull the optimum repository with the [example notebook and scripts](https://github.com/huggingface/optimum-neuron/tree/main/notebooks/text-generation).\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/huggingface/optimum-neuron.git\n",
    "```\n",
    "\n",
    "Next we can change our directory to `notbooks/text-generation` and launch the `jupyter` environment.``\n",
    "\n",
    "\n",
    "```bash\n",
    "# change directory\n",
    "cd optimum-neuron/notebooks/text-generation\n",
    "# launch jupyter\n",
    "python -m notebook --allow-root --port=8080\n",
    "```\n",
    "\n",
    "You should see a familiar **`jupyter`** output with a URL to the notebook.\n",
    "\n",
    "**`http://localhost:8080/?token=8c1739aff1755bd7958c4cfccc8d08cb5da5234f61f129a9`**\n",
    "\n",
    "We can click on it, and a **`jupyter`** environment opens in our local browser. Open the notebook **`llama2-7b-fine-tuning.ipynb`** and lets get started.\n",
    "\n",
    "_Note: We are going to use the Jupyter environment only for preparing the dataset and then `torchrun` for launching our training script for  distributed training._"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use official Llama 2 checkpoint you need to login into our hugging face account, which has access to the model, to use your token for accessing the gated repository. We can do this by running the following command:\n",
    "\n",
    "_Note: We also provide an ungated checkpoint._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login --token YOUR_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo pip install optimum-neuron --upgrade"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and prepare the dataset\n",
    "\n",
    "We will use [Dolly](https://huggingface.co/datasets/databricks/databricks-dolly-15k) an open source dataset of instruction-following records on categories outlined in the [InstructGPT paper](https://arxiv.org/abs/2203.02155), including brainstorming, classification, closed QA, generation, information extraction, open QA, and summarization.\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"instruction\": \"What is world of warcraft\",\n",
    "  \"context\": \"\",\n",
    "  \"response\": \"World of warcraft is a massive online multi player role playing game. It was released in 2004 by bizarre entertainment\"\n",
    "}\n",
    "```\n",
    "\n",
    "To load the `dolly` dataset, we use the `load_dataset()` method from the 🤗 Datasets library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: 15011\n",
      "{'instruction': 'How deep is the Gulf of Mexico measured in Empire State buildings?', 'context': '', 'response': 'The Empire State building is 1,250 feet and the Gulf of Mexico at its deepest point is 14,383 feet.  It would take eleven and a half Empire State Buildings, stacked on top of each other, to measure the Gulf of Mexico at its deepest point.', 'category': 'open_qa'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "print(f\"dataset size: {len(dataset)}\")\n",
    "print(dataset[randrange(len(dataset))])\n",
    "# dataset size: 15011\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To instruct tune our model we need to convert our structured examples into a collection of tasks described via instructions. We define a `formatting_function` that takes a sample and returns a string with our format instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dolly(sample):\n",
    "    instruction = f\"### Instruction\\n{sample['instruction']}\"\n",
    "    context = f\"### Context\\n{sample['context']}\" if len(sample[\"context\"]) > 0 else None\n",
    "    response = f\"### Answer\\n{sample['response']}\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets test our formatting function on a random example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "What was the first Pink Floyd album\n",
      "\n",
      "### Answer\n",
      "Piper at the gates of dawn\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "\n",
    "print(format_dolly(dataset[randrange(len(dataset))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, to formatting our samples we also want to pack multiple samples to one sequence to have a more efficient training. This means that we are stacking multiple samples to one sequence and split them with an EOS Token. This makes the training more efficient. Packing/stacking samples can be done during training or before. We will do it before training to save time. We created a utility method [pack_dataset](./scripts/utils/pack_dataset.py) that takes a dataset and a packing function and returns a packed dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Hugging Face model id\n",
    "model_id = \"NousResearch/Llama-2-7b-hf\"  # non-gated\n",
    "# model_id = \"meta-llama/Llama-2-7b-hf\" # gated\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pack/stack our dataset we need to first tokenize it and then we can pack it with the `pack_dataset` method. To prepare our dataset we will now: \n",
    "1. Format our samples using the template method and add an EOS token at the end of each sample\n",
    "2. Tokenize our dataset to convert it from text to tokens\n",
    "3. Pack our dataset to 2048 tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "What's the largest bird in the world?\n",
      "\n",
      "### Answer\n",
      "Ostrich</s>\n",
      "Chunking dataset into chunks of 2048 tokens.\n",
      "Total number of samples: 1591\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "# add utils method to path for loading dataset\n",
    "import sys\n",
    "sys.path.append(\"./scripts/utils\") # make sure you change this to the correct path \n",
    "from pack_dataset import pack_dataset\n",
    "\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = f\"{format_dolly(sample)}{tokenizer.eos_token}\"\n",
    "    return sample\n",
    "\n",
    "# apply prompt template per sample\n",
    "dataset = dataset.map(template_dataset, remove_columns=list(dataset.features))\n",
    "# print random sample\n",
    "print(dataset[randint(0, len(dataset))][\"text\"])\n",
    "\n",
    "# tokenize dataset\n",
    "dataset = dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(dataset.features)\n",
    ")\n",
    "\n",
    "# chunk dataset\n",
    "lm_dataset = pack_dataset(dataset, chunk_length=2048) # We use 2048 as the maximum length for packing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we processed the datasets we are going save it to disk. You could also save it to S3 or the Hugging Face Hub for later use. \n",
    "\n",
    "_Note: Packing and preprocessing your dataset can be run outside of the Trainium instance._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 1591/1591 [00:00<00:00, 5680.55 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# save train_dataset to disk\n",
    "dataset_path = \"tokenized_dolly\"\n",
    "lm_dataset.save_to_disk(dataset_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-tune Llama on AWS Trainium using the `NeuronTrainer`\n",
    "\n",
    "Normally you would use the **[Trainer](https://huggingface.co/docs/transformers/v4.19.4/en/main_classes/trainer#transformers.Trainer)** and **[TrainingArguments](https://huggingface.co/docs/transformers/v4.19.4/en/main_classes/trainer#transformers.TrainingArguments)** to fine-tune PyTorch-based transformer models. \n",
    "\n",
    "But together with AWS, we have developed a `NeuronTrainer` to improve performance, robustness, and safety when training on Trainium instances. The `NeuronTrainer` is part of the `optimum-neuron` library and can be used as a 1-to-1 replacement for the `Trainer`.\n",
    "\n",
    "When it comes to distributed training on AWS Trainium there is a few things we need to take care of. Since Llama is a big model it might not fit on a single accelerator, thats why we added support for different distributed training strategies to the `NeuronTrainer` including: \n",
    "* [ZeRO-1](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/tutorials/training/zero1_gpt2.html): shards the optimizer state over multiple devices.\n",
    "* [Tensor Parallelism](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/neuronx-distributed/tensor_parallelism_overview.html): shards the model parameters along a given dimension on multiple devices, defined with `tensor_parallel_size`\n",
    "* [Sequence parallelism](https://arxiv.org/pdf/2205.05198.pdf) shards the activations on the sequence axis outside of the tensor parallel regions. It is useful because it saves memory by sharding the activations.\n",
    "* [Pipeline Parallelism](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/neuronx-distributed/pipeline_parallelism_overview.html): _coming soon_\n",
    "\n",
    "\n",
    "We prepared a [run_clm.py](./scripts/run_clm.py), which implements those distributed training strategies for you already. If you want to more about the details you can take a look at the [documentation](https://huggingface.co/docs/optimum-neuron/guides/distributed_training).\n",
    "\n",
    "When training models on AWS Accelerators we first need to compile our model with our training arguments. \n",
    "\n",
    "To overcome this we added a [model cache](https://huggingface.co/docs/optimum-neuron/guides/cache_system), which allows us to use precompiled models and configuration from Hugging Face Hub to skip the compilation step. But every change in the config, will lead to a new compilations, this could result in some cache misses. \n",
    "\n",
    "_Note: If your configuration is not cached please open a issue on [Github](https://github.com/huggingface/optimum-neuron/issues), we are happy to include it._\n",
    "\n",
    "We pre-compiled the config for our training already meaning you can skip the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-07 09:35:08.000293:  147814  INFO ||NEURON_PARALLEL_COMPILE||: Running trial run (add option to terminate trial run early; also ignore trial run's generated outputs, i.e. loss, checkpoints)\n",
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "> initializing tensor model parallel with size 8\n",
      "> initializing pipeline model parallel with size 1\n",
      "> initializing data parallel with size 4\n",
      "***** Running training *****\n",
      "  Num examples = 398\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 10\n",
      "  Number of trainable parameters = 842,534,912\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]2023-12-07 09:36:34.000017:  172531  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-12-07 09:36:34.000024:  172531  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_1503437627695910288+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "No Neuron cache name is saved locally. This means that only the official Neuron cache will be used. You can create a Neuron cache repo by running the following command: `optimum-cli neuron cache create`. If the Neuron cache already exists you can set it by running the following command: `optimum-cli neuron cache set -n [name]`.\n",
      "2023-12-07 09:36:55.000360:  184098  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-12-07 09:36:55.000459:  184101  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-12-07 09:36:55.000678:  184104  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-12-07 09:36:55.000765:  184098  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_2539352722715907447+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-12-07 09:36:55.000818:  184107  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-12-07 09:36:55.000819:  184109  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-12-07 09:36:55.000831:  184111  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-12-07 09:36:55.000845:  184114  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-12-07 09:36:55.000847:  184117  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-12-07 09:36:55.000970:  184101  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_7536821414867287675+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-12-07 09:36:56.000371:  184104  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_11220521096095644261+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-12-07 09:36:56.000776:  184111  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_17575860727371960526+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-12-07 09:36:56.000784:  184117  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_11737472936030846813+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-12-07 09:36:56.000998:  184109  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_5321993569601292553+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-12-07 09:36:57.000006:  184107  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_16398059650771470005+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-12-07 09:36:57.000007:  184114  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_8861683562612152874+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-12-07 09:37:02.000442:  184353  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-12-07 09:37:02.000608:  184356  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-12-07 09:37:02.000853:  184353  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_9039192882441986773+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-12-07 09:37:02.000861:  184356  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_17707561591382356723+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-12-07 09:37:03.000050:  184359  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-12-07 09:37:03.000171:  184362  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-12-07 09:37:03.000233:  184365  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-12-07 09:37:03.000477:  184368  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-12-07 09:37:03.000499:  184370  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-12-07 09:37:03.000513:  184373  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-12-07 09:37:03.000663:  184359  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_13703250961872211674+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-12-07 09:37:03.000671:  184362  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_13115043180190189797+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-12-07 09:37:04.000079:  184365  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_10601725819237734730+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-12-07 09:37:04.000288:  184370  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_2777767419094136584+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-12-07 09:37:04.000296:  184373  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_17277457361738373603+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-12-07 09:37:04.000305:  184368  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_17339519708772627254+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      " 10%|█         | 1/10 [00:37<05:35, 37.30s/it]2023-12-07 09:37:19.000954:  189956  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-12-07 09:37:20.000225:  189956  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_4356920434883426297+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-12-07 09:37:20.000260:  189959  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-12-07 09:37:20.000304:  189962  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-12-07 09:37:20.000365:  189965  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-12-07 09:37:20.000446:  189968  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-12-07 09:37:20.000619:  189971  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-12-07 09:37:20.000663:  189974  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-12-07 09:37:20.000830:  189977  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-12-07 09:37:21.000297:  189959  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_15050396075824270127+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-12-07 09:37:21.000563:  189962  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_7328290186745253959+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-12-07 09:37:21.000829:  189965  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_12788685928660698890+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-12-07 09:37:22.000115:  189968  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_13646819380798778518+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-12-07 09:37:22.000116:  189974  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_7943733490193408425+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-12-07 09:37:22.000127:  189977  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_299281182663011231+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-12-07 09:37:22.000127:  189971  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_10838314580783544082+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      " 20%|██        | 2/10 [00:58<03:44, 28.09s/it]2023-12-07 09:37:41.000049:  196016  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-12-07 09:37:41.000342:  196016  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_13275123067325918023+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-12-07 09:37:41.000417:  196019  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-12-07 09:37:41.000709:  196019  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_9911022956972173782+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-12-07 09:37:42.000200:  196022  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-12-07 09:37:42.000265:  196025  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-12-07 09:37:42.000295:  196028  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-12-07 09:37:42.000300:  196030  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-12-07 09:37:42.000327:  196033  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-12-07 09:37:42.000343:  196036  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-12-07 09:37:43.000318:  196022  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_7976391161847235125+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-12-07 09:37:43.000328:  196025  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_257409570912431573+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-12-07 09:37:43.000617:  196030  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_17181431344976025645+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-12-07 09:37:43.000916:  196033  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_13149319069762575170+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-12-07 09:37:43.000927:  196028  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_17008437971940686699+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-12-07 09:37:43.000928:  196036  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_1388554744546309082+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "100%|██████████| 10/10 [01:51<00:00,  5.26s/it]2023-12-07 09:38:24.000769:  247042  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-12-07 09:38:24.000770:  247042  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_10421090513981540266+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "{'loss': -2.0589517865311654e+23, 'learning_rate': 0.0, 'epoch': 0.4}\n",
      "100%|██████████| 10/10 [01:51<00:00,  5.26s/it]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "2023-12-07 09:38:24.000942:  247693  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-12-07 09:38:24.000943:  247693  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_931450859643720011+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "{'train_runtime': 111.5177, 'train_samples_per_second': 5.739, 'train_steps_per_second': 0.09, 'train_loss': -2.0589517865311654e+23, 'epoch': 0.4}\n",
      "100%|██████████| 10/10 [01:51<00:00, 11.15s/it]\n",
      "Saving model checkpoint to dolly_llama_sharded\n",
      "Model parallelism is enabled, only saving the model sharded state dict.\n",
      "[2023-12-07 09:38:25.020: I neuronx_distributed/parallel_layers/checkpointing.py:72] saving checkpoint to dolly_llama_sharded/tensor_parallel_shards\n",
      "2023-12-07 09:40:37.000669:  147814  INFO ||NEURON_PARALLEL_COMPILE||: New graph list from script: \n",
      "2023-12-07 09:40:37.000669:  147814  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-12-07 09:40:45.000146:  147814  INFO ||NEURON_CACHE||: Current remaining items are 0, locked are 0, failed are 0, done are 35, total is 35\n",
      "2023-12-07 09:40:45.000151:  147814  INFO ||NEURON_PARALLEL_COMPILE||: {\n",
      "    \"compilation_summary\": {},\n",
      "    \"compilation_report\": {}\n",
      "}\n",
      "2023-12-07 09:40:45.000151:  147814  INFO ||NEURON_PARALLEL_COMPILE||: Total graphs: 0\n",
      "2023-12-07 09:40:45.000151:  147814  INFO ||NEURON_PARALLEL_COMPILE||: Total successful compilations: 0\n",
      "2023-12-07 09:40:45.000151:  147814  INFO ||NEURON_PARALLEL_COMPILE||: Total failed compilations: 0\n"
     ]
    }
   ],
   "source": [
    "# precompilation command \n",
    "!MALLOC_ARENA_MAX=64 neuron_parallel_compile torchrun --nproc_per_node=32 scripts/run_clm.py \\\n",
    " --model_id {model_id} \\\n",
    " --dataset_path {dataset_path} \\\n",
    " --bf16 True \\\n",
    " --learning_rate 5e-5 \\\n",
    " --output_dir dolly_llama \\\n",
    " --overwrite_output_dir True \\\n",
    " --per_device_train_batch_size 1 \\\n",
    " --gradient_checkpointing True \\\n",
    " --tensor_parallel_size 8 \\\n",
    " --max_steps 10 \\\n",
    " --logging_steps 10 \\\n",
    " --gradient_accumulation_steps 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: Compiling with out a cache can take ~40minutes. It will also create dummy files in the `dolly_llama_sharded` during compilation you we have to remove them afterwards._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove dummy artifacts which are created by the precompilation command\n",
    "!rm -rf dolly_llama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the compilation is done we can start our training with a similar command, we just need to remove the `neuron_parallel_compile`. We will use `torchrun` to launch our training script. `torchrun` is a tool that automatically distributes a PyTorch model across multiple accelerators. We can pass the number of accelerators as `nproc_per_node` arguments alongside our hyperparameters.\n",
    "\n",
    "Launch the training, with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"NousResearch/Llama-2-7b-hf\"  # non-gated\n",
    "dataset_path = \"tokenized_dolly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "> initializing tensor model parallel with size 8\n",
      "> initializing pipeline model parallel with size 1\n",
      "> initializing data parallel with size 4\n",
      "***** Running training *****\n",
      "  Num examples = 398\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 24\n",
      "  Number of trainable parameters = 842,534,912\n",
      "  0%|                                                    | 0/24 [00:00<?, ?it/s]2023-12-07 14:35:13.000922:  586922  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      "2023-12-07 14:35:13.000928:  586922  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpsb6t9zej/neuronxcc-2.11.0.34+c5231f848/MODULE_1503437627695910288+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "No Neuron cache name is saved locally. This means that only the official Neuron cache will be used. You can create a Neuron cache repo by running the following command: `optimum-cli neuron cache create`. If the Neuron cache already exists you can set it by running the following command: `optimum-cli neuron cache set -n [name]`.\n",
      "2023-12-07 14:35:32.000808:  597479  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      "2023-12-07 14:35:32.000986:  597482  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      "2023-12-07 14:35:33.000024:  597485  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      "2023-12-07 14:35:33.000074:  597488  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      "2023-12-07 14:35:33.000161:  597491  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      "2023-12-07 14:35:33.000190:  597496  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      "2023-12-07 14:35:33.000190:  597494  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      "2023-12-07 14:35:33.000200:  597498  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      "2023-12-07 14:35:33.000202:  597479  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpsb6t9zej/neuronxcc-2.11.0.34+c5231f848/MODULE_170463763323439478+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-12-07 14:35:33.000785:  597482  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpsb6t9zej/neuronxcc-2.11.0.34+c5231f848/MODULE_14102442302739333995+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-12-07 14:35:34.000177:  597488  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpsb6t9zej/neuronxcc-2.11.0.34+c5231f848/MODULE_6109913704640654219+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-12-07 14:35:34.000186:  597491  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpsb6t9zej/neuronxcc-2.11.0.34+c5231f848/MODULE_16582291865971754472+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-12-07 14:35:34.000195:  597485  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpsb6t9zej/neuronxcc-2.11.0.34+c5231f848/MODULE_1685442370239714565+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-12-07 14:35:34.000204:  597494  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpsb6t9zej/neuronxcc-2.11.0.34+c5231f848/MODULE_15214075907291163652+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-12-07 14:35:34.000213:  597496  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpsb6t9zej/neuronxcc-2.11.0.34+c5231f848/MODULE_9184873463953336092+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-12-07 14:35:34.000417:  597498  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpsb6t9zej/neuronxcc-2.11.0.34+c5231f848/MODULE_8640612684014041011+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-Dec-07 14:36:01.0176 563730:564051 [18] nccl_net_ofi_init:1415 CCOM WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "2023-Dec-07 14:36:01.0176 563730:564051 [18] init.cc:138 CCOM WARN OFI plugin initNet() failed is EFA enabled?\n",
      "2023-12-07 14:37:36.000920:  597776  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      "2023-12-07 14:37:37.000126:  597776  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpsb6t9zej/neuronxcc-2.11.0.34+c5231f848/MODULE_4618510295310378119+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-12-07 14:37:37.000231:  597787  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      "2023-12-07 14:37:37.000347:  597790  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      "2023-12-07 14:37:37.000487:  597793  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      "2023-12-07 14:37:37.000632:  597787  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpsb6t9zej/neuronxcc-2.11.0.34+c5231f848/MODULE_7011542366841321750+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-12-07 14:37:37.000641:  597790  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpsb6t9zej/neuronxcc-2.11.0.34+c5231f848/MODULE_5939873730228245419+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-12-07 14:37:37.000830:  597796  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      "2023-12-07 14:37:37.000850:  597798  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      "2023-12-07 14:37:38.000050:  597793  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpsb6t9zej/neuronxcc-2.11.0.34+c5231f848/MODULE_4600018781814582840+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-12-07 14:37:38.000245:  597796  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpsb6t9zej/neuronxcc-2.11.0.34+c5231f848/MODULE_15506026666180631370+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-12-07 14:37:38.000256:  597798  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpsb6t9zej/neuronxcc-2.11.0.34+c5231f848/MODULE_4209014076703928936+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-12-07 14:37:41.000349:  597897  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      "2023-12-07 14:37:41.000555:  597897  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpsb6t9zej/neuronxcc-2.11.0.34+c5231f848/MODULE_13509193059479923048+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-12-07 14:37:41.000668:  597900  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      "2023-12-07 14:37:41.000874:  597900  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpsb6t9zej/neuronxcc-2.11.0.34+c5231f848/MODULE_7841456581692735436+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "  4%|█▋                                       | 1/24 [04:48<1:50:30, 288.29s/it]2023-12-07 14:39:49.000713:  602342  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      "2023-12-07 14:39:49.000718:  602342  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/ubuntu/neuroncc_compile_workdir/8e4f3a53-00bb-4b23-b9d6-1629ec2e20be/model.MODULE_17474272444879422847+d41d8cd9.hlo.pb', '--output', '/tmp/ubuntu/neuroncc_compile_workdir/8e4f3a53-00bb-4b23-b9d6-1629ec2e20be/model.MODULE_17474272444879422847+d41d8cd9.neff', '--verbose=35']\n",
      "2023-12-07 14:39:49.000874:  602346  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      "2023-12-07 14:39:49.000916:  602349  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      ".2023-12-07 14:39:50.000029:  602354  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      "2023-12-07 14:39:50.000142:  602346  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpsb6t9zej/neuronxcc-2.11.0.34+c5231f848/MODULE_5462105427334107375+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-12-07 14:39:50.000151:  602354  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/ubuntu/neuroncc_compile_workdir/3ecf5cc8-f453-49d9-81dc-66617bc240d5/model.MODULE_5345354544816623062+d41d8cd9.hlo.pb', '--output', '/tmp/ubuntu/neuroncc_compile_workdir/3ecf5cc8-f453-49d9-81dc-66617bc240d5/model.MODULE_5345354544816623062+d41d8cd9.neff', '--verbose=35']\n",
      "2023-12-07 14:39:50.000153:  602349  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/ubuntu/neuroncc_compile_workdir/160cf2a7-bd8a-4925-9fb6-6d3b3ede9293/model.MODULE_11169390618082329750+d41d8cd9.hlo.pb', '--output', '/tmp/ubuntu/neuroncc_compile_workdir/160cf2a7-bd8a-4925-9fb6-6d3b3ede9293/model.MODULE_11169390618082329750+d41d8cd9.neff', '--verbose=35']\n",
      "..2023-12-07 14:39:50.000620:  602370  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      "2023-12-07 14:39:50.000706:  602373  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      "2023-12-07 14:39:50.000817:  602376  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      "2023-12-07 14:39:50.000835:  602378  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      "2023-12-07 14:39:51.000141:  602370  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpsb6t9zej/neuronxcc-2.11.0.34+c5231f848/MODULE_17281250592024170363+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-12-07 14:39:51.000154:  602373  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpsb6t9zej/neuronxcc-2.11.0.34+c5231f848/MODULE_5189404435296855907+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-12-07 14:39:51.000157:  602376  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/ubuntu/neuroncc_compile_workdir/966f5a57-c9fc-4602-be67-da2983802a55/model.MODULE_14381378507642207430+d41d8cd9.hlo.pb', '--output', '/tmp/ubuntu/neuroncc_compile_workdir/966f5a57-c9fc-4602-be67-da2983802a55/model.MODULE_14381378507642207430+d41d8cd9.neff', '--verbose=35']\n",
      "2023-12-07 14:39:51.000160:  602378  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/ubuntu/neuroncc_compile_workdir/ee07449f-13cf-4ce3-99ee-186355c27c7f/model.MODULE_5144257572637010018+d41d8cd9.hlo.pb', '--output', '/tmp/ubuntu/neuroncc_compile_workdir/ee07449f-13cf-4ce3-99ee-186355c27c7f/model.MODULE_5144257572637010018+d41d8cd9.neff', '--verbose=35']\n",
      "................................2023-Dec-07 14:42:06.0213 563730:564051 [18] include/socket.h:579 CCOM WARN Timeout waiting for RX (waited 120 sec) - retrying, [[gid: 18] barrier: peer(16) recv ack from peer(18): replica_grp[0], rank: 2/8, mod: /tmp/ubuntu/neuroncc_compile_workdir/28e4ab51-45ea-4470-9b20-9121567366e2/model.MODULE_5462105427334107375+d41d8cd9.neff/16]\n",
      "2023-Dec-07 14:42:06.0304 563730:563939 [2] include/socket.h:579 CCOM WARN Timeout waiting for RX (waited 120 sec) - retrying, [[gid: 2] barrier: peer(0) recv ack from peer(2): replica_grp[0], rank: 2/8, mod: /tmp/ubuntu/neuroncc_compile_workdir/28e4ab51-45ea-4470-9b20-9121567366e2/model.MODULE_5462105427334107375+d41d8cd9.neff/0]\n",
      "2023-Dec-07 14:42:07.0488 563730:564142 [31] include/socket.h:579 CCOM WARN Timeout waiting for RX (waited 120 sec) - retrying, [[gid: 31] barrier: peer(24) recv ack from peer(31): replica_grp[0], rank: 7/8, mod: /tmp/ubuntu/neuroncc_compile_workdir/8a93e129-a19f-4288-a59b-4bf56bb5e8a1/model.MODULE_5189404435296855907+d41d8cd9.neff/24]\n",
      "2023-Dec-07 14:42:07.0564 563730:563974 [7] include/socket.h:579 CCOM WARN Timeout waiting for RX (waited 120 sec) - retrying, [[gid: 7] barrier: peer(0) recv ack from peer(7): replica_grp[0], rank: 7/8, mod: /tmp/ubuntu/neuroncc_compile_workdir/8a93e129-a19f-4288-a59b-4bf56bb5e8a1/model.MODULE_5189404435296855907+d41d8cd9.neff/0]\n",
      "2023-Dec-07 14:42:08.0114 563730:564079 [22] include/socket.h:579 CCOM WARN Timeout waiting for RX (waited 120 sec) - retrying, [[gid: 22] barrier: peer(16) recv ack from peer(22): replica_grp[0], rank: 6/8, mod: /tmp/ubuntu/neuroncc_compile_workdir/536459d7-4218-410e-8090-80583ab66a34/model.MODULE_17281250592024170363+d41d8cd9.neff/16]\n",
      "2023-Dec-07 14:42:08.0151 563730:564135 [30] include/socket.h:579 CCOM WARN Timeout waiting for RX (waited 120 sec) - retrying, [[gid: 30] barrier: peer(24) recv ack from peer(30): replica_grp[0], rank: 6/8, mod: /tmp/ubuntu/neuroncc_compile_workdir/536459d7-4218-410e-8090-80583ab66a34/model.MODULE_17281250592024170363+d41d8cd9.neff/24]\n",
      "2023-Dec-07 14:42:08.0445 563730:564023 [14] include/socket.h:579 CCOM WARN Timeout waiting for RX (waited 120 sec) - retrying, [[gid: 14] barrier: peer(8) recv ack from peer(14): replica_grp[0], rank: 6/8, mod: /tmp/ubuntu/neuroncc_compile_workdir/536459d7-4218-410e-8090-80583ab66a34/model.MODULE_17281250592024170363+d41d8cd9.neff/8]\n",
      ".....2023-Dec-07 14:42:13.0618 563730:563995 [10] include/socket.h:579 CCOM WARN Timeout waiting for RX (waited 120 sec) - retrying, [[gid: 10] barrier: peer(8) recv ack from peer(10): replica_grp[0], rank: 2/8, mod: /tmp/ubuntu/neuroncc_compile_workdir/28e4ab51-45ea-4470-9b20-9121567366e2/model.MODULE_5462105427334107375+d41d8cd9.neff/8]\n",
      ".....2023-Dec-07 14:42:41.0362 563730:564086 [23] include/socket.h:579 CCOM WARN Timeout waiting for RX (waited 120 sec) - retrying, [[gid: 23] barrier: peer(16) recv ack from peer(23): replica_grp[0], rank: 7/8, mod: /tmp/ubuntu/neuroncc_compile_workdir/8a93e129-a19f-4288-a59b-4bf56bb5e8a1/model.MODULE_5189404435296855907+d41d8cd9.neff/16]\n",
      "2023-Dec-07 14:42:41.0365 563730:564030 [15] include/socket.h:579 CCOM WARN Timeout waiting for RX (waited 120 sec) - retrying, [[gid: 15] barrier: peer(8) recv ack from peer(15): replica_grp[0], rank: 7/8, mod: /tmp/ubuntu/neuroncc_compile_workdir/8a93e129-a19f-4288-a59b-4bf56bb5e8a1/model.MODULE_5189404435296855907+d41d8cd9.neff/8]\n",
      "2023-Dec-07 14:42:47.0686 563730:564107 [26] include/socket.h:579 CCOM WARN Timeout waiting for RX (waited 120 sec) - retrying, [[gid: 26] barrier: peer(24) recv ack from peer(26): replica_grp[0], rank: 2/8, mod: /tmp/ubuntu/neuroncc_compile_workdir/28e4ab51-45ea-4470-9b20-9121567366e2/model.MODULE_5462105427334107375+d41d8cd9.neff/24]\n",
      "2023-Dec-07 14:42:49.0625 563730:563967 [6] include/socket.h:579 CCOM WARN Timeout waiting for RX (waited 120 sec) - retrying, [[gid: 6] barrier: peer(0) recv ack from peer(6): replica_grp[0], rank: 6/8, mod: /tmp/ubuntu/neuroncc_compile_workdir/536459d7-4218-410e-8090-80583ab66a34/model.MODULE_17281250592024170363+d41d8cd9.neff/0]\n",
      "....................2023-Dec-07 14:44:06.0313 563730:564051 [18] include/socket.h:579 CCOM WARN Timeout waiting for RX (waited 240 sec) - retrying, [[gid: 18] barrier: peer(16) recv ack from peer(18): replica_grp[0], rank: 2/8, mod: /tmp/ubuntu/neuroncc_compile_workdir/28e4ab51-45ea-4470-9b20-9121567366e2/model.MODULE_5462105427334107375+d41d8cd9.neff/16]\n",
      "2023-Dec-07 14:44:06.0404 563730:563939 [2] include/socket.h:579 CCOM WARN Timeout waiting for RX (waited 240 sec) - retrying, [[gid: 2] barrier: peer(0) recv ack from peer(2): replica_grp[0], rank: 2/8, mod: /tmp/ubuntu/neuroncc_compile_workdir/28e4ab51-45ea-4470-9b20-9121567366e2/model.MODULE_5462105427334107375+d41d8cd9.neff/0]\n",
      "2023-Dec-07 14:44:07.0533 563730:564142 [31] include/socket.h:579 CCOM WARN Timeout waiting for RX (waited 240 sec) - retrying, [[gid: 31] barrier: peer(24) recv ack from peer(31): replica_grp[0], rank: 7/8, mod: /tmp/ubuntu/neuroncc_compile_workdir/8a93e129-a19f-4288-a59b-4bf56bb5e8a1/model.MODULE_5189404435296855907+d41d8cd9.neff/24]\n",
      "2023-Dec-07 14:44:07.0580 563730:563974 [7] include/socket.h:579 CCOM WARN Timeout waiting for RX (waited 240 sec) - retrying, [[gid: 7] barrier: peer(0) recv ack from peer(7): replica_grp[0], rank: 7/8, mod: /tmp/ubuntu/neuroncc_compile_workdir/8a93e129-a19f-4288-a59b-4bf56bb5e8a1/model.MODULE_5189404435296855907+d41d8cd9.neff/0]\n",
      "2023-Dec-07 14:44:08.0213 563730:564079 [22] include/socket.h:579 CCOM WARN Timeout waiting for RX (waited 240 sec) - retrying, [[gid: 22] barrier: peer(16) recv ack from peer(22): replica_grp[0], rank: 6/8, mod: /tmp/ubuntu/neuroncc_compile_workdir/536459d7-4218-410e-8090-80583ab66a34/model.MODULE_17281250592024170363+d41d8cd9.neff/16]\n",
      "2023-Dec-07 14:44:08.0249 563730:564135 [30] include/socket.h:579 CCOM WARN Timeout waiting for RX (waited 240 sec) - retrying, [[gid: 30] barrier: peer(24) recv ack from peer(30): replica_grp[0], rank: 6/8, mod: /tmp/ubuntu/neuroncc_compile_workdir/536459d7-4218-410e-8090-80583ab66a34/model.MODULE_17281250592024170363+d41d8cd9.neff/24]\n",
      "2023-Dec-07 14:44:08.0545 563730:564023 [14] include/socket.h:579 CCOM WARN Timeout waiting for RX (waited 240 sec) - retrying, [[gid: 14] barrier: peer(8) recv ack from peer(14): replica_grp[0], rank: 6/8, mod: /tmp/ubuntu/neuroncc_compile_workdir/536459d7-4218-410e-8090-80583ab66a34/model.MODULE_17281250592024170363+d41d8cd9.neff/8]\n",
      ".....2023-Dec-07 14:44:13.0656 563730:563995 [10] include/socket.h:579 CCOM WARN Timeout waiting for RX (waited 240 sec) - retrying, [[gid: 10] barrier: peer(8) recv ack from peer(10): replica_grp[0], rank: 2/8, mod: /tmp/ubuntu/neuroncc_compile_workdir/28e4ab51-45ea-4470-9b20-9121567366e2/model.MODULE_5462105427334107375+d41d8cd9.neff/8]\n",
      ".....2023-Dec-07 14:44:41.0462 563730:564086 [23] include/socket.h:579 CCOM WARN Timeout waiting for RX (waited 240 sec) - retrying, [[gid: 23] barrier: peer(16) recv ack from peer(23): replica_grp[0], rank: 7/8, mod: /tmp/ubuntu/neuroncc_compile_workdir/8a93e129-a19f-4288-a59b-4bf56bb5e8a1/model.MODULE_5189404435296855907+d41d8cd9.neff/16]\n",
      "2023-Dec-07 14:44:41.0465 563730:564030 [15] include/socket.h:579 CCOM WARN Timeout waiting for RX (waited 240 sec) - retrying, [[gid: 15] barrier: peer(8) recv ack from peer(15): replica_grp[0], rank: 7/8, mod: /tmp/ubuntu/neuroncc_compile_workdir/8a93e129-a19f-4288-a59b-4bf56bb5e8a1/model.MODULE_5189404435296855907+d41d8cd9.neff/8]\n",
      "2023-Dec-07 14:44:47.0785 563730:564107 [26] include/socket.h:579 CCOM WARN Timeout waiting for RX (waited 240 sec) - retrying, [[gid: 26] barrier: peer(24) recv ack from peer(26): replica_grp[0], rank: 2/8, mod: /tmp/ubuntu/neuroncc_compile_workdir/28e4ab51-45ea-4470-9b20-9121567366e2/model.MODULE_5462105427334107375+d41d8cd9.neff/24]\n",
      "2023-Dec-07 14:44:49.0725 563730:563967 [6] include/socket.h:579 CCOM WARN Timeout waiting for RX (waited 240 sec) - retrying, [[gid: 6] barrier: peer(0) recv ack from peer(6): replica_grp[0], rank: 6/8, mod: /tmp/ubuntu/neuroncc_compile_workdir/536459d7-4218-410e-8090-80583ab66a34/model.MODULE_17281250592024170363+d41d8cd9.neff/0]\n",
      ".....\n",
      "Compiler status PASS\n",
      "\n",
      "Compiler status PASS\n",
      "\n",
      "Compiler status PASS\n",
      ".\n",
      "Compiler status PASS\n",
      "\n",
      "Compiler status PASS\n",
      "  8%|███▍                                     | 2/24 [11:30<2:10:16, 355.28s/it]2023-12-07 14:46:31.000427:  607881  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      "2023-12-07 14:46:31.000432:  607881  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/ubuntu/neuroncc_compile_workdir/9ad7a678-0f55-463b-ab54-e9a51cbb4b30/model.MODULE_5718646300080132491+d41d8cd9.hlo.pb', '--output', '/tmp/ubuntu/neuroncc_compile_workdir/9ad7a678-0f55-463b-ab54-e9a51cbb4b30/model.MODULE_5718646300080132491+d41d8cd9.neff', '--verbose=35']\n",
      ".2023-12-07 14:46:32.000221:  607890  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      "2023-12-07 14:46:32.000226:  607890  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/ubuntu/neuroncc_compile_workdir/fa0dd7a2-9beb-4681-b6a2-abf1da7b6881/model.MODULE_2201210410865753920+d41d8cd9.hlo.pb', '--output', '/tmp/ubuntu/neuroncc_compile_workdir/fa0dd7a2-9beb-4681-b6a2-abf1da7b6881/model.MODULE_2201210410865753920+d41d8cd9.neff', '--verbose=35']\n",
      "2023-12-07 14:46:32.000401:  607894  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      "2023-12-07 14:46:32.000406:  607894  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/ubuntu/neuroncc_compile_workdir/613a41bf-ca84-43cd-a6c5-ec00429eb1b3/model.MODULE_8431941341911079171+d41d8cd9.hlo.pb', '--output', '/tmp/ubuntu/neuroncc_compile_workdir/613a41bf-ca84-43cd-a6c5-ec00429eb1b3/model.MODULE_8431941341911079171+d41d8cd9.neff', '--verbose=35']\n",
      "..2023-12-07 14:46:32.000682:  607904  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      "2023-12-07 14:46:32.000687:  607904  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/ubuntu/neuroncc_compile_workdir/315fe0f2-f77e-4c45-a0c8-b1e0c943c68d/model.MODULE_13357871049806549880+d41d8cd9.hlo.pb', '--output', '/tmp/ubuntu/neuroncc_compile_workdir/315fe0f2-f77e-4c45-a0c8-b1e0c943c68d/model.MODULE_13357871049806549880+d41d8cd9.neff', '--verbose=35']\n",
      "2023-12-07 14:46:32.000839:  607910  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      "2023-12-07 14:46:32.000844:  607910  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/ubuntu/neuroncc_compile_workdir/0b774750-10ef-4c40-97be-d17e67486b0f/model.MODULE_7174826982254267942+d41d8cd9.hlo.pb', '--output', '/tmp/ubuntu/neuroncc_compile_workdir/0b774750-10ef-4c40-97be-d17e67486b0f/model.MODULE_7174826982254267942+d41d8cd9.neff', '--verbose=35']\n",
      "2023-12-07 14:46:32.000905:  607916  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      "2023-12-07 14:46:32.000905:  607914  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      ".2023-12-07 14:46:32.000912:  607916  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/ubuntu/neuroncc_compile_workdir/9a21018d-b326-471b-a8f5-c8a361f0d645/model.MODULE_16486258655177218299+d41d8cd9.hlo.pb', '--output', '/tmp/ubuntu/neuroncc_compile_workdir/9a21018d-b326-471b-a8f5-c8a361f0d645/model.MODULE_16486258655177218299+d41d8cd9.neff', '--verbose=35']\n",
      "2023-12-07 14:46:32.000913:  607918  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      "2023-12-07 14:46:32.000915:  607914  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/ubuntu/neuroncc_compile_workdir/ffb9df0c-425d-4daf-82c3-d0c5bfbc3127/model.MODULE_13908331554409799463+d41d8cd9.hlo.pb', '--output', '/tmp/ubuntu/neuroncc_compile_workdir/ffb9df0c-425d-4daf-82c3-d0c5bfbc3127/model.MODULE_13908331554409799463+d41d8cd9.neff', '--verbose=35']\n",
      "2023-12-07 14:46:32.000919:  607918  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/ubuntu/neuroncc_compile_workdir/2fe10b2d-714c-4354-9ad6-56d325a59afa/model.MODULE_2776941313405211798+d41d8cd9.hlo.pb', '--output', '/tmp/ubuntu/neuroncc_compile_workdir/2fe10b2d-714c-4354-9ad6-56d325a59afa/model.MODULE_2776941313405211798+d41d8cd9.neff', '--verbose=35']\n",
      "............................................................................................................................................................................\n",
      "Compiler status PASS\n",
      ".......\n",
      "Compiler status PASS\n",
      "\n",
      "Compiler status PASS\n",
      "\n",
      "Compiler status PASS\n",
      "\n",
      "Compiler status PASS\n",
      "\n",
      "Compiler status PASS\n",
      "\n",
      "Compiler status PASS\n",
      "\n",
      "Compiler status PASS\n",
      " 42%|█████████████████▉                         | 10/24 [22:52<09:25, 40.40s/it]2023-12-07 14:57:44.000855:  644474  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      "2023-12-07 14:57:44.000856:  644474  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpsb6t9zej/neuronxcc-2.11.0.34+c5231f848/MODULE_10421090513981540266+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "{'loss': 1.475, 'learning_rate': 2.916666666666667e-05, 'epoch': 0.4}           \n",
      " 42%|█████████████████▉                         | 10/24 [22:54<09:25, 40.40s/it]2023-12-07 14:57:45.000566:  644596  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      "2023-12-07 14:57:45.000567:  644596  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpsb6t9zej/neuronxcc-2.11.0.34+c5231f848/MODULE_931450859643720011+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "{'loss': 1.3125, 'learning_rate': 8.333333333333334e-06, 'epoch': 0.8}          \n",
      "100%|███████████████████████████████████████████| 24/24 [26:32<00:00, 15.56s/it]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "2023-12-07 15:01:23.000555:  705485  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      "2023-12-07 15:01:23.000556:  705485  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/ubuntu/neuroncc_compile_workdir/03fc64d1-1284-4f2a-8455-73651f8f62be/model.MODULE_9578681836437553238+d41d8cd9.hlo.pb', '--output', '/tmp/ubuntu/neuroncc_compile_workdir/03fc64d1-1284-4f2a-8455-73651f8f62be/model.MODULE_9578681836437553238+d41d8cd9.neff', '--verbose=35']\n",
      "2023-12-07 15:01:23.000611:  705488  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      "2023-12-07 15:01:23.000613:  705488  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/ubuntu/neuroncc_compile_workdir/63738f4f-37b6-43a8-b380-9a27f9b27539/model.MODULE_566040879854907305+d41d8cd9.hlo.pb', '--output', '/tmp/ubuntu/neuroncc_compile_workdir/63738f4f-37b6-43a8-b380-9a27f9b27539/model.MODULE_566040879854907305+d41d8cd9.neff', '--verbose=35']\n",
      "2023-12-07 15:01:23.000628:  705490  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      "2023-12-07 15:01:23.000630:  705490  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/ubuntu/neuroncc_compile_workdir/c81d1fd9-e1dd-48c4-88c0-c423a57fba75/model.MODULE_14634928721954922116+d41d8cd9.hlo.pb', '--output', '/tmp/ubuntu/neuroncc_compile_workdir/c81d1fd9-e1dd-48c4-88c0-c423a57fba75/model.MODULE_14634928721954922116+d41d8cd9.neff', '--verbose=35']\n",
      "2023-12-07 15:01:23.000650:  705493  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      "2023-12-07 15:01:23.000652:  705493  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/ubuntu/neuroncc_compile_workdir/b91a6811-ab0c-499d-a950-e1bf70b7c91b/model.MODULE_18033660054871559947+d41d8cd9.hlo.pb', '--output', '/tmp/ubuntu/neuroncc_compile_workdir/b91a6811-ab0c-499d-a950-e1bf70b7c91b/model.MODULE_18033660054871559947+d41d8cd9.neff', '--verbose=35']\n",
      "2023-12-07 15:01:23.000672:  705496  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      "2023-12-07 15:01:23.000674:  705496  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/ubuntu/neuroncc_compile_workdir/cfafa6e5-b4a4-48f9-9121-f6f2532787cf/model.MODULE_1394264861273997600+d41d8cd9.hlo.pb', '--output', '/tmp/ubuntu/neuroncc_compile_workdir/cfafa6e5-b4a4-48f9-9121-f6f2532787cf/model.MODULE_1394264861273997600+d41d8cd9.neff', '--verbose=35']\n",
      "2023-12-07 15:01:23.000687:  705500  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      "2023-12-07 15:01:23.000689:  705500  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/ubuntu/neuroncc_compile_workdir/09358508-94a8-40de-bae4-232957dc60f1/model.MODULE_2142026627649143707+d41d8cd9.hlo.pb', '--output', '/tmp/ubuntu/neuroncc_compile_workdir/09358508-94a8-40de-bae4-232957dc60f1/model.MODULE_2142026627649143707+d41d8cd9.neff', '--verbose=35']\n",
      "2023-12-07 15:01:23.000691:  705502  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      "2023-12-07 15:01:23.000693:  705502  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/ubuntu/neuroncc_compile_workdir/7cc8a065-fe4b-4434-a433-ec0d88d11f08/model.MODULE_13348877947035367927+d41d8cd9.hlo.pb', '--output', '/tmp/ubuntu/neuroncc_compile_workdir/7cc8a065-fe4b-4434-a433-ec0d88d11f08/model.MODULE_13348877947035367927+d41d8cd9.neff', '--verbose=35']\n",
      "2023-12-07 15:01:23.000712:  705505  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      "2023-12-07 15:01:23.000714:  705505  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/ubuntu/neuroncc_compile_workdir/0be21844-0322-4b00-acda-bd25792a904b/model.MODULE_17188627161674600259+d41d8cd9.hlo.pb', '--output', '/tmp/ubuntu/neuroncc_compile_workdir/0be21844-0322-4b00-acda-bd25792a904b/model.MODULE_17188627161674600259+d41d8cd9.neff', '--verbose=35']\n",
      "................................................................................................................\n",
      "Compiler status PASS\n",
      "\n",
      "Compiler status PASS\n",
      "\n",
      "Compiler status PASS\n",
      "\n",
      "Compiler status PASS\n",
      "\n",
      "Compiler status PASS\n",
      "\n",
      "Compiler status PASS\n",
      "\n",
      "Compiler status PASS\n",
      ".\n",
      "Compiler status PASS\n",
      "{'train_runtime': 1898.2067, 'train_samples_per_second': 0.21, 'train_steps_per_second': 0.013, 'train_loss': 1.3932291666666667, 'epoch': 0.96}\n",
      "100%|███████████████████████████████████████████| 24/24 [31:38<00:00, 79.09s/it]\n",
      "registry.json: 100%|███████████████████████| 22.7k/22.7k [00:00<00:00, 6.89MB/s]\n",
      "model.neff: 100%|██████████████████████████| 20.0M/20.0M [00:00<00:00, 48.7MB/s]\n",
      "model.hlo.pb: 100%|████████████████████████| 4.06M/4.06M [00:00<00:00, 25.4MB/s]\n",
      "model.neff: 100%|██████████████████████████| 19.9M/19.9M [00:00<00:00, 54.3MB/s]\n",
      "model.hlo.pb: 100%|████████████████████████| 4.06M/4.06M [00:00<00:00, 28.0MB/s]\n",
      "model.neff: 100%|██████████████████████████| 19.9M/19.9M [00:00<00:00, 43.8MB/s]\n",
      "model.hlo.pb: 100%|████████████████████████| 4.06M/4.06M [00:00<00:00, 31.4MB/s]\n",
      "model.neff: 100%|██████████████████████████| 19.9M/19.9M [00:00<00:00, 56.1MB/s]\n",
      "model.hlo.pb: 100%|████████████████████████| 4.06M/4.06M [00:00<00:00, 23.4MB/s]\n",
      "model.neff: 100%|██████████████████████████| 19.9M/19.9M [00:00<00:00, 45.5MB/s]\n",
      "model.hlo.pb: 100%|████████████████████████| 3.97M/3.97M [00:00<00:00, 21.6MB/s]\n",
      "model.neff: 100%|██████████████████████████| 21.4M/21.4M [00:00<00:00, 47.2MB/s]\n",
      "model.hlo.pb: 100%|████████████████████████| 3.97M/3.97M [00:00<00:00, 26.3MB/s]\n",
      "model.neff: 100%|██████████████████████████| 21.4M/21.4M [00:00<00:00, 81.5MB/s]\n",
      "model.hlo.pb: 100%|████████████████████████| 3.97M/3.97M [00:00<00:00, 24.0MB/s]\n",
      "model.neff: 100%|██████████████████████████| 21.4M/21.4M [00:00<00:00, 50.0MB/s]\n",
      "model.hlo.pb: 100%|████████████████████████| 3.97M/3.97M [00:00<00:00, 22.1MB/s]\n",
      "model.neff: 100%|██████████████████████████| 21.4M/21.4M [00:00<00:00, 61.9MB/s]\n",
      "model.hlo.pb: 100%|████████████████████████| 3.97M/3.97M [00:00<00:00, 29.4MB/s]\n",
      "model.neff: 100%|██████████████████████████| 21.4M/21.4M [00:00<00:00, 55.5MB/s]\n",
      "model.hlo.pb: 100%|████████████████████████| 3.97M/3.97M [00:00<00:00, 18.7MB/s]\n",
      "model.neff: 100%|██████████████████████████| 21.4M/21.4M [00:00<00:00, 51.7MB/s]\n",
      "model.hlo.pb: 100%|████████████████████████| 3.97M/3.97M [00:00<00:00, 24.9MB/s]\n",
      "model.neff: 100%|██████████████████████████| 21.5M/21.5M [00:00<00:00, 51.9MB/s]\n",
      "model.hlo.pb: 100%|████████████████████████| 3.97M/3.97M [00:00<00:00, 21.5MB/s]\n",
      "model.neff: 100%|██████████████████████████| 21.4M/21.4M [00:00<00:00, 41.3MB/s]\n",
      "Saving model checkpoint to dolly_llama\n",
      "Model parallelism is enabled, only saving the model sharded state dict.\n",
      "2023-12-07 15:07:43.000623:  706902  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      "2023-12-07 15:07:43.000628:  706902  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/ubuntu/neuroncc_compile_workdir/4745660e-e26b-485c-9bf1-2f4a74a3b7d3/model.MODULE_14226937348309729270+d41d8cd9.hlo.pb', '--output', '/tmp/ubuntu/neuroncc_compile_workdir/4745660e-e26b-485c-9bf1-2f4a74a3b7d3/model.MODULE_14226937348309729270+d41d8cd9.neff', '--verbose=35']\n",
      ".2023-12-07 15:07:44.000775:  706911  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      "2023-12-07 15:07:44.000780:  706911  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/ubuntu/neuroncc_compile_workdir/f3a66d78-e363-4fc6-b5b7-de56a2ea85db/model.MODULE_5465793518522810193+d41d8cd9.hlo.pb', '--output', '/tmp/ubuntu/neuroncc_compile_workdir/f3a66d78-e363-4fc6-b5b7-de56a2ea85db/model.MODULE_5465793518522810193+d41d8cd9.neff', '--verbose=35']\n",
      "2023-12-07 15:07:44.000790:  706913  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      "2023-12-07 15:07:44.000795:  706913  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/ubuntu/neuroncc_compile_workdir/948df827-309e-472d-a72d-98f2f3a5057f/model.MODULE_16025666463746925842+d41d8cd9.hlo.pb', '--output', '/tmp/ubuntu/neuroncc_compile_workdir/948df827-309e-472d-a72d-98f2f3a5057f/model.MODULE_16025666463746925842+d41d8cd9.neff', '--verbose=35']\n",
      "2023-12-07 15:07:44.000839:  706917  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      "2023-12-07 15:07:44.000845:  706917  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/ubuntu/neuroncc_compile_workdir/2837c7f0-d84b-415a-99f5-ee2586bd1f6d/model.MODULE_7938567517976356390+d41d8cd9.hlo.pb', '--output', '/tmp/ubuntu/neuroncc_compile_workdir/2837c7f0-d84b-415a-99f5-ee2586bd1f6d/model.MODULE_7938567517976356390+d41d8cd9.neff', '--verbose=35']\n",
      "2023-12-07 15:07:44.000889:  706920  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      "2023-12-07 15:07:44.000895:  706920  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/ubuntu/neuroncc_compile_workdir/dd2fec7b-d6db-4b1b-8a77-473615d070a9/model.MODULE_10692605452252046599+d41d8cd9.hlo.pb', '--output', '/tmp/ubuntu/neuroncc_compile_workdir/dd2fec7b-d6db-4b1b-8a77-473615d070a9/model.MODULE_10692605452252046599+d41d8cd9.neff', '--verbose=35']\n",
      "2023-12-07 15:07:44.000957:  706925  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      "2023-12-07 15:07:44.000962:  706925  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/ubuntu/neuroncc_compile_workdir/c549025d-3f8c-4b68-ab71-70aadd53e67b/model.MODULE_5519274497465507389+d41d8cd9.hlo.pb', '--output', '/tmp/ubuntu/neuroncc_compile_workdir/c549025d-3f8c-4b68-ab71-70aadd53e67b/model.MODULE_5519274497465507389+d41d8cd9.neff', '--verbose=35']\n",
      ".....2023-12-07 15:07:49.000101:  706957  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      "2023-12-07 15:07:49.000106:  706957  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/ubuntu/neuroncc_compile_workdir/9efe90b5-f7aa-47c7-ab17-d12ed948fdcc/model.MODULE_11399003196678454047+d41d8cd9.hlo.pb', '--output', '/tmp/ubuntu/neuroncc_compile_workdir/9efe90b5-f7aa-47c7-ab17-d12ed948fdcc/model.MODULE_11399003196678454047+d41d8cd9.neff', '--verbose=35']\n",
      "2023-12-07 15:07:49.000106:  706959  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpsb6t9zej\n",
      "2023-12-07 15:07:49.000111:  706959  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/ubuntu/neuroncc_compile_workdir/4c64b7e3-e16b-4c31-9b0c-d0bb92bdcd52/model.MODULE_3665122704317668951+d41d8cd9.hlo.pb', '--output', '/tmp/ubuntu/neuroncc_compile_workdir/4c64b7e3-e16b-4c31-9b0c-d0bb92bdcd52/model.MODULE_3665122704317668951+d41d8cd9.neff', '--verbose=35']\n",
      "..................................................................................................................................................................................\n",
      "Compiler status PASS\n",
      "\n",
      "Compiler status PASS\n",
      "\n",
      "Compiler status PASS\n",
      "\n",
      "Compiler status PASS\n",
      "\n",
      "Compiler status PASS\n",
      "\n",
      "Compiler status PASS\n",
      "\n",
      "Compiler status PASS\n",
      "\n",
      "Compiler status PASS\n",
      "[2023-12-07 15:15:40.625: I neuronx_distributed/parallel_layers/checkpointing.py:72] saving checkpoint to dolly_llama/tensor_parallel_shards\n",
      "Converting sharded checkpoint to consolidated format\n"
     ]
    }
   ],
   "source": [
    "!MALLOC_ARENA_MAX=64 torchrun --nproc_per_node=32 scripts/run_clm.py \\\n",
    " --model_id {model_id} \\\n",
    " --dataset_path {dataset_path} \\\n",
    " --bf16 True \\\n",
    " --learning_rate 5e-5 \\\n",
    " --output_dir dolly_llama \\\n",
    " --overwrite_output_dir True \\\n",
    " --per_device_train_batch_size 1 \\\n",
    " --gradient_checkpointing True \\\n",
    " --tensor_parallel_size 8 \\\n",
    " --num_train_epochs 1 \\\n",
    " --logging_steps 10 \\\n",
    " --gradient_accumulation_steps 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training is done we have to consolidate our model. Tensor Parallelism consists sharded the model weights accross different workers, only sharded checkpoints will be saved during training. We need to consolidate the sharded checkpoints to be able to share and use the model. \n",
    "\n",
    "The Optimum CLI provides a way of doing that very easily via the `optimum neuron consolidate`` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidating checkpoints from dolly_llama_sharded/tensor_parallel_shards to the safetensors format...\n",
      "Consolidated checkpoint saved at dolly_llama_sharded\n"
     ]
    }
   ],
   "source": [
    "!optimum-cli neuron consolidate dolly_llama/tensor_parallel_shards dolly_llama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets remove our \"sharded\" checkpoints as we have consolidated them already to safetensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf dolly_llama/tensor_parallel_shards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats it, we successfully trained Llama on AWS Trainium and can now share it, evaluate it or use it for inference. In the next section we will compile our trained model for inference use and test it.\n",
    "\n",
    "The training took for 3 epochs leading to a cost of ~1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evalaute and test fine-tuned Llama model\n",
    "\n",
    "Similar to training to be able to run inferece on AWS Trainium or AWS Inferentia2 we need to compile our model for the correct use. We will use our Trainium instance for the inference test, but we recommend customer to switch to Inferentia2 for inference. \n",
    "\n",
    "Optimum Neuron implements similar to Transformers AutoModel classes for easy inference use. We will use  the `NeuronModelForCausalLM` class to load our vanilla transformers checkpoint and convert it to neuron. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.93s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for LlamaForCausalLM:\n\tsize mismatch for model.embed_tokens.weight: copying a param with shape torch.Size([32000, 4096]) from checkpoint, the shape in current model is torch.Size([4000, 4096]).\n\tsize mismatch for lm_head.weight: copying a param with shape torch.Size([32000, 4096]) from checkpoint, the shape in current model is torch.Size([4000, 4096]).\n\tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/optimum-neuron/notebooks/text-generation/llama2-7b-fine-tuning.ipynb Cell 30\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btrn1/home/ubuntu/optimum-neuron/notebooks/text-generation/llama2-7b-fine-tuning.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m input_shapes \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m1\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msequence_length\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m2048\u001b[39m}\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btrn1/home/ubuntu/optimum-neuron/notebooks/text-generation/llama2-7b-fine-tuning.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mdolly_llama\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Btrn1/home/ubuntu/optimum-neuron/notebooks/text-generation/llama2-7b-fine-tuning.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m model \u001b[39m=\u001b[39m NeuronModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btrn1/home/ubuntu/optimum-neuron/notebooks/text-generation/llama2-7b-fine-tuning.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mdolly_llama\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btrn1/home/ubuntu/optimum-neuron/notebooks/text-generation/llama2-7b-fine-tuning.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m         export\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btrn1/home/ubuntu/optimum-neuron/notebooks/text-generation/llama2-7b-fine-tuning.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcompiler_args,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btrn1/home/ubuntu/optimum-neuron/notebooks/text-generation/llama2-7b-fine-tuning.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minput_shapes)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/optimum/modeling_base.py:372\u001b[0m, in \u001b[0;36mOptimizedModel.from_pretrained\u001b[0;34m(cls, model_id, export, force_download, use_auth_token, cache_dir, subfolder, config, local_files_only, trust_remote_code, revision, **kwargs)\u001b[0m\n\u001b[1;32m    369\u001b[0m     trust_remote_code \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    371\u001b[0m from_pretrained_method \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_from_transformers \u001b[39mif\u001b[39;00m export \u001b[39melse\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_from_pretrained\n\u001b[0;32m--> 372\u001b[0m \u001b[39mreturn\u001b[39;00m from_pretrained_method(\n\u001b[1;32m    373\u001b[0m     model_id\u001b[39m=\u001b[39;49mmodel_id,\n\u001b[1;32m    374\u001b[0m     config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    375\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    376\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    377\u001b[0m     force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    378\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    379\u001b[0m     subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m    380\u001b[0m     local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    381\u001b[0m     trust_remote_code\u001b[39m=\u001b[39;49mtrust_remote_code,\n\u001b[1;32m    382\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    383\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/optimum/neuron/modeling_decoder.py:119\u001b[0m, in \u001b[0;36mNeuronDecoderModel._from_transformers\u001b[0;34m(cls, model_id, config, use_auth_token, revision, force_download, cache_dir, subfolder, local_files_only, trust_remote_code, task, batch_size, sequence_length, num_cores, auto_cast_type, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     task \u001b[39m=\u001b[39m TasksManager\u001b[39m.\u001b[39minfer_task_from_model(\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mauto_model_class)\n\u001b[1;32m    118\u001b[0m \u001b[39m# Instantiate the transformers model checkpoint\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m model \u001b[39m=\u001b[39m TasksManager\u001b[39m.\u001b[39;49mget_model_from_task(\n\u001b[1;32m    120\u001b[0m     task\u001b[39m=\u001b[39;49mtask,\n\u001b[1;32m    121\u001b[0m     model_name_or_path\u001b[39m=\u001b[39;49mmodel_id,\n\u001b[1;32m    122\u001b[0m     subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m    123\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    124\u001b[0m     framework\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    125\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    126\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    127\u001b[0m     local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    128\u001b[0m     force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    129\u001b[0m     trust_remote_code\u001b[39m=\u001b[39;49mtrust_remote_code,\n\u001b[1;32m    130\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    131\u001b[0m )\n\u001b[1;32m    133\u001b[0m \u001b[39m# Save the model checkpoint in a temporary directory\u001b[39;00m\n\u001b[1;32m    134\u001b[0m checkpoint_dir \u001b[39m=\u001b[39m TemporaryDirectory()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/optimum/exporters/tasks.py:1752\u001b[0m, in \u001b[0;36mTasksManager.get_model_from_task\u001b[0;34m(task, model_name_or_path, subfolder, revision, framework, cache_dir, torch_dtype, device, library_name, **model_kwargs)\u001b[0m\n\u001b[1;32m   1750\u001b[0m             model \u001b[39m=\u001b[39m model_class\u001b[39m.\u001b[39mfrom_pretrained(model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1751\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1752\u001b[0m         model \u001b[39m=\u001b[39m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m   1753\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1754\u001b[0m     model \u001b[39m=\u001b[39m model_class\u001b[39m.\u001b[39mfrom_pretrained(model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/auto/auto_factory.py:566\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    565\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 566\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    567\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    568\u001b[0m     )\n\u001b[1;32m    569\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    570\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    572\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/modeling_utils.py:3480\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3471\u001b[0m     \u001b[39mif\u001b[39;00m dtype_orig \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3472\u001b[0m         torch\u001b[39m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3473\u001b[0m     (\n\u001b[1;32m   3474\u001b[0m         model,\n\u001b[1;32m   3475\u001b[0m         missing_keys,\n\u001b[1;32m   3476\u001b[0m         unexpected_keys,\n\u001b[1;32m   3477\u001b[0m         mismatched_keys,\n\u001b[1;32m   3478\u001b[0m         offload_index,\n\u001b[1;32m   3479\u001b[0m         error_msgs,\n\u001b[0;32m-> 3480\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_load_pretrained_model(\n\u001b[1;32m   3481\u001b[0m         model,\n\u001b[1;32m   3482\u001b[0m         state_dict,\n\u001b[1;32m   3483\u001b[0m         loaded_state_dict_keys,  \u001b[39m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3484\u001b[0m         resolved_archive_file,\n\u001b[1;32m   3485\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   3486\u001b[0m         ignore_mismatched_sizes\u001b[39m=\u001b[39;49mignore_mismatched_sizes,\n\u001b[1;32m   3487\u001b[0m         sharded_metadata\u001b[39m=\u001b[39;49msharded_metadata,\n\u001b[1;32m   3488\u001b[0m         _fast_init\u001b[39m=\u001b[39;49m_fast_init,\n\u001b[1;32m   3489\u001b[0m         low_cpu_mem_usage\u001b[39m=\u001b[39;49mlow_cpu_mem_usage,\n\u001b[1;32m   3490\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   3491\u001b[0m         offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   3492\u001b[0m         offload_state_dict\u001b[39m=\u001b[39;49moffload_state_dict,\n\u001b[1;32m   3493\u001b[0m         dtype\u001b[39m=\u001b[39;49mtorch_dtype,\n\u001b[1;32m   3494\u001b[0m         is_quantized\u001b[39m=\u001b[39;49m(\u001b[39mgetattr\u001b[39;49m(model, \u001b[39m\"\u001b[39;49m\u001b[39mquantization_method\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m) \u001b[39m==\u001b[39;49m QuantizationMethod\u001b[39m.\u001b[39;49mBITS_AND_BYTES),\n\u001b[1;32m   3495\u001b[0m         keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[1;32m   3496\u001b[0m     )\n\u001b[1;32m   3498\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_4bit \u001b[39m=\u001b[39m load_in_4bit\n\u001b[1;32m   3499\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_8bit \u001b[39m=\u001b[39m load_in_8bit\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/modeling_utils.py:3931\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3927\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39msize mismatch\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m error_msg:\n\u001b[1;32m   3928\u001b[0m         error_msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m   3929\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39mYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3930\u001b[0m         )\n\u001b[0;32m-> 3931\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{\u001b[39;00merror_msg\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   3933\u001b[0m \u001b[39mif\u001b[39;00m is_quantized:\n\u001b[1;32m   3934\u001b[0m     unexpected_keys \u001b[39m=\u001b[39m [elem \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m unexpected_keys \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mSCB\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m elem]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for LlamaForCausalLM:\n\tsize mismatch for model.embed_tokens.weight: copying a param with shape torch.Size([32000, 4096]) from checkpoint, the shape in current model is torch.Size([4000, 4096]).\n\tsize mismatch for lm_head.weight: copying a param with shape torch.Size([32000, 4096]) from checkpoint, the shape in current model is torch.Size([4000, 4096]).\n\tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method."
     ]
    }
   ],
   "source": [
    "from optimum.neuron import NeuronModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "compiler_args = {\"num_cores\": 2, \"auto_cast_type\": 'fp16'}\n",
    "input_shapes = {\"batch_size\": 1, \"sequence_length\": 2048}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dolly_llama\")\n",
    "model = NeuronModelForCausalLM.from_pretrained(\n",
    "        \"dolly_llama\",\n",
    "        export=True,\n",
    "        **compiler_args,\n",
    "        **input_shapes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: Inference compilation can take ~45minutes. Luckily, you need to only run this onces. Since you can save the model afterwards._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMENT IN if you want to save the compiled model\n",
    "# model.save_pretrained(\"compiled_dolly_llama\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now test inference, but have to make sure we format our input to our prompt format we used for fine-tuning. Therefore we created a helper method, which accepts a `dict` with our `instruction` and optionally a `context`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dolly_infernece(sample):\n",
    "    instruction = f\"### Instruction\\n{sample['instruction']}\"\n",
    "    context = f\"### Context\\n{sample['context']}\" if len(sample[\"context\"]) > 0 else None\n",
    "    response = f\"### Answer\\n\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def generate(sample): \n",
    "    prompt = format_dolly_infernece(sample)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs,\n",
    "                         max_new_tokens=512,\n",
    "                         do_sample=True,\n",
    "                         temperature=0.9,\n",
    "                         top_k=50,\n",
    "                         top_p=0.9)\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets test inference. First we test without a context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = {\n",
    "  \"instruction\": \"Can you tell me something about AWS?\"\n",
    "}\n",
    "res = generate(prompt)\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks correct. Now, lets add some context, e.g. as you would do for RAG applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = {\n",
    "  \"instruction\": \"How can train models on AWS Trainium?\",\n",
    "  \"context\": \"🤗 Optimum Neuron is the interface between the 🤗 Transformers library and AWS Accelerators including [AWS Trainium](https://aws.amazon.com/machine-learning/trainium/?nc1=h_ls) and [AWS Inferentia](https://aws.amazon.com/machine-learning/inferentia/?nc1=h_ls). It provides a set of tools enabling easy model loading, training and inference on single- and multi-Accelerator settings for different downstream tasks.\"\n",
    "}\n",
    "res = generate(prompt)\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, our model also correctly uses the provided context. We are done. Congrats on fine-tuning Llama on AWS Trainium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
