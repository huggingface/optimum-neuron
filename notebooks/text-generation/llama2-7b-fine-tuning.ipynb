{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune Llama on AWS Trainium \n",
    "\n",
    "This tutorial will teach how to fine-tune open LLMs like [Llama 2](https://huggingface.co/meta-llama/Llama-2-70b-hf) on AWS Trainium.  In our example, we are going to leverage Hugging Face Optimum Neuron, [Transformers](https://huggingface.co/docs/transformers/index)and datasets. \n",
    "\n",
    "You will learn how to:\n",
    "\n",
    "1. [Setup AWS environment](#1-setup-aws-environment)\n",
    "2. [Load and process the dataset](#2-load-and-prepare-the-dataset)\n",
    "3. [Fine-tune Llama on AWS Trainium using the `NeuronTrainer`](#3-fine-tune-llama-on-aws-trainium-using-the-neurontrainer)\n",
    "4. [Evalaute and test fine-tuned Llama model](#4-evalaute-and-test-fine-tuned-llama-model)\n",
    "\n",
    "## Quick intro: AWS Trainium\n",
    "\n",
    "[AWS Trainium (Trn1)](https://aws.amazon.com/de/ec2/instance-types/trn1/) is a purpose-built EC2 for deep learning (DL) training workloads. Trainium is the successor of [AWS Inferentia](https://aws.amazon.com/ec2/instance-types/inf1/?nc1=h_ls) focused on high-performance training workloads. Trainium has been optimized for training natural language processing, computer vision, and recommender models used. The accelerator supports a wide range of data types, including FP32, TF32, BF16, FP16, UINT8, and configurable FP8. \n",
    "\n",
    "The biggest Trainium instance, the `trn1.32xlarge` comes with over 500GB of memory, making it easy to fine-tune ~10B parameter models on a single instance. Below you will find an overview of the available instance types. More details [here](https://aws.amazon.com/de/ec2/instance-types/trn1/#Product_details):\n",
    "\n",
    "| instance size | accelerators | accelerator memory | vCPU | CPU Memory | price per hour |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| trn1.2xlarge | 1 | 32 | 8 | 32 | $1.34 |\n",
    "| trn1.32xlarge | 16 | 512 | 128 | 512 | $21.50 |\n",
    "| trn1n.32xlarge (2x bandwidth) | 16 | 512 | 128 | 512 | $24.78 |\n",
    "\n",
    "---\n",
    "\n",
    "*Note: This tutorial was created on a trn1.32xlarge AWS EC2 Instance.* \n",
    "\n",
    "\n",
    "## 1. Setup AWS environment\n",
    "\n",
    "In this example, we will use the `trn1.32xlarge` instance on AWS with 16 Accelerator, including 32 Neuron Cores and the [Hugging Face Neuron Deep Learning AMI](https://aws.amazon.com/marketplace/pp/prodview-gr3e6yiscria2). The Hugging Face AMI comes with all important libraries, like Transformers, Datasets, Optimum and Neuron packages pre-installed this makes it super easy to get started, since there is no need for environment management.\n",
    "\n",
    "This blog post doesnâ€™t cover how to create the instance in detail. You can check out my previous blog about [â€œSetting up AWS Trainium for Hugging Face Transformersâ€](https://www.philschmid.de/setup-aws-trainium), which includes a step-by-step guide on setting up the environment. \n",
    "\n",
    "Once the instance is up and running, we can ssh into it. But instead of developing inside a terminal we want to use a `Jupyter` environment, which we can use for preparing our dataset and launching the training. For this, we need to add a port for forwarding in theÂ `ssh` command, which will tunnel our localhost traffic to the Trainium instance.\n",
    "\n",
    "```bash\n",
    "PUBLIC_DNS=\"\" # IP address, e.g. ec2-3-80-....\n",
    "KEY_PATH=\"\" # local path to key, e.g. ssh/trn.pem\n",
    "\n",
    "ssh -L 8080:localhost:8080 -i ${KEY_NAME}.pem ubuntu@$PUBLIC_DNS\n",
    "```\n",
    "\n",
    "Lets now pull the optimum repository with the [example notebook and scripts](https://github.com/huggingface/optimum-neuron/tree/main/notebooks/text-generation).\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/huggingface/optimum-neuron.git\n",
    "```\n",
    "\n",
    "Next we can change our directory to `notbooks/text-generation` and launch the `jupyter` environment.``\n",
    "\n",
    "\n",
    "```bash\n",
    "# change directory\n",
    "cd optimum-neuron/notebooks/text-generation\n",
    "# launch jupyter\n",
    "python -m notebook --allow-root --port=8080\n",
    "```\n",
    "\n",
    "You should see a familiarÂ **`jupyter`**Â output with a URL to the notebook.\n",
    "\n",
    "**`http://localhost:8080/?token=8c1739aff1755bd7958c4cfccc8d08cb5da5234f61f129a9`**\n",
    "\n",
    "We can click on it, and aÂ **`jupyter`**Â environment opens in our local browser. Open the notebookÂ **`llama2-7b-fine-tuning.ipynb`**Â and lets get started.\n",
    "\n",
    "_Note: We are going to use the Jupyter environment only for preparing the dataset and then `torchrun` for launching our training script for  distributed training._"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use official Llama 2 checkpoint you need to login into our hugging face account, which has access to the model, to use your token for accessing the gated repository. We can do this by running the following command:\n",
    "\n",
    "_Note: We also provide an ungated checkpoint._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login --token YOUR_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo pip install optimum-neuron --upgrade"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and prepare the dataset\n",
    "\n",
    "We will useÂ [Dolly](https://huggingface.co/datasets/databricks/databricks-dolly-15k)Â an open source dataset of instruction-following records on categories outlined in theÂ [InstructGPT paper](https://arxiv.org/abs/2203.02155), including brainstorming, classification, closed QA, generation, information extraction, open QA, and summarization.\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"instruction\": \"What is world of warcraft\",\n",
    "  \"context\": \"\",\n",
    "  \"response\": \"World of warcraft is a massive online multi player role playing game. It was released in 2004 by bizarre entertainment\"\n",
    "}\n",
    "```\n",
    "\n",
    "To load theÂ `dolly`Â dataset, we use theÂ `load_dataset()`Â method from the ðŸ¤— Datasets library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: 15011\n",
      "{'instruction': 'How deep is the Gulf of Mexico measured in Empire State buildings?', 'context': '', 'response': 'The Empire State building is 1,250 feet and the Gulf of Mexico at its deepest point is 14,383 feet.  It would take eleven and a half Empire State Buildings, stacked on top of each other, to measure the Gulf of Mexico at its deepest point.', 'category': 'open_qa'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "print(f\"dataset size: {len(dataset)}\")\n",
    "print(dataset[randrange(len(dataset))])\n",
    "# dataset size: 15011\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To instruct tune our model we need to convert our structured examples into a collection of tasks described via instructions. We define a `formatting_function` that takes a sample and returns a string with our format instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dolly(sample):\n",
    "    instruction = f\"### Instruction\\n{sample['instruction']}\"\n",
    "    context = f\"### Context\\n{sample['context']}\" if len(sample[\"context\"]) > 0 else None\n",
    "    response = f\"### Answer\\n{sample['response']}\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets test our formatting function on a random example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "What was the first Pink Floyd album\n",
      "\n",
      "### Answer\n",
      "Piper at the gates of dawn\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "\n",
    "print(format_dolly(dataset[randrange(len(dataset))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, to formatting our samples we also want to pack multiple samples to one sequence to have a more efficient training. This means that we are stacking multiple samples to one sequence and split them with an EOS Token. This makes the training more efficient. Packing/stacking samples can be done during training or before. We will do it before training to save time. We created a utility method [pack_dataset](./scripts/utils/pack_dataset.py) that takes a dataset and a packing function and returns a packed dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Hugging Face model id\n",
    "model_id = \"NousResearch/Llama-2-7b-hf\"  # non-gated\n",
    "# model_id = \"meta-llama/Llama-2-7b-hf\" # gated\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pack/stack our dataset we need to first tokenize it and then we can pack it with the `pack_dataset` method. To prepare our dataset we will now: \n",
    "1. Format our samples using the template method and add an EOS token at the end of each sample\n",
    "2. Tokenize our dataset to convert it from text to tokens\n",
    "3. Pack our dataset to 2048 tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "What's the largest bird in the world?\n",
      "\n",
      "### Answer\n",
      "Ostrich</s>\n",
      "Chunking dataset into chunks of 2048 tokens.\n",
      "Total number of samples: 1591\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "# add utils method to path for loading dataset\n",
    "import sys\n",
    "sys.path.append(\"./scripts/utils\") # make sure you change this to the correct path \n",
    "from pack_dataset import pack_dataset\n",
    "\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = f\"{format_dolly(sample)}{tokenizer.eos_token}\"\n",
    "    return sample\n",
    "\n",
    "# apply prompt template per sample\n",
    "dataset = dataset.map(template_dataset, remove_columns=list(dataset.features))\n",
    "# print random sample\n",
    "print(dataset[randint(0, len(dataset))][\"text\"])\n",
    "\n",
    "# tokenize dataset\n",
    "dataset = dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(dataset.features)\n",
    ")\n",
    "\n",
    "# chunk dataset\n",
    "lm_dataset = pack_dataset(dataset, chunk_length=2048) # We use 2048 as the maximum length for packing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we processed the datasets we are going save it to disk. You could also save it to S3 or the Hugging Face Hub for later use. \n",
    "\n",
    "_Note: Packing and preprocessing your dataset can be run outside of the Trainium instance._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1591/1591 [00:00<00:00, 5680.55 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# save train_dataset to disk\n",
    "dataset_path = \"tokenized_dolly\"\n",
    "lm_dataset.save_to_disk(dataset_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-tune Llama on AWS Trainium using the `NeuronTrainer`\n",
    "\n",
    "Normally you would use theÂ **[Trainer](https://huggingface.co/docs/transformers/v4.19.4/en/main_classes/trainer#transformers.Trainer)**Â andÂ **[TrainingArguments](https://huggingface.co/docs/transformers/v4.19.4/en/main_classes/trainer#transformers.TrainingArguments)**Â to fine-tune PyTorch-based transformer models. \n",
    "\n",
    "But together with AWS, we have developed a `NeuronTrainer` to improve performance, robustness, and safety when training on Trainium instances. The `NeuronTrainer` is part of the `optimum-neuron` library and can be used as a 1-to-1 replacement for the `Trainer`.\n",
    "\n",
    "When it comes to distributed training on AWS Trainium there is a few things we need to take care of. Since Llama is a big model it might not fit on a single accelerator, thats why we added support for different distributed training strategies to the `NeuronTrainer` including: \n",
    "* [ZeRO-1](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/tutorials/training/zero1_gpt2.html): shards the optimizer state over multiple devices.\n",
    "* [Tensor Parallelism](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/neuronx-distributed/tensor_parallelism_overview.html): shards the model parameters along a given dimension on multiple devices, defined with `tensor_parallel_size`\n",
    "* [Sequence parallelism](https://arxiv.org/pdf/2205.05198.pdf) shards the activations on the sequence axis outside of the tensor parallel regions. It is useful because it saves memory by sharding the activations.\n",
    "* [Pipeline Parallelism](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/neuronx-distributed/pipeline_parallelism_overview.html): _coming soon_\n",
    "\n",
    "\n",
    "We prepared a [run_clm.py](./scripts/run_clm.py), which implements those distributed training strategies for you already. If you want to more about the details you can take a look at the [documentation](https://huggingface.co/docs/optimum-neuron/guides/distributed_training).\n",
    "\n",
    "When training models on AWS Accelerators we first need to compile our model with our training arguments. \n",
    "\n",
    "To overcome this we added a [model cache](https://huggingface.co/docs/optimum-neuron/guides/cache_system), which allows us to use precompiled models and configuration from Hugging Face Hub to skip the compilation step. But every change in the config, will lead to a new compilations, this could result in some cache misses. \n",
    "\n",
    "_Note: If your configuration is not cached please open a issue on [Github](https://github.com/huggingface/optimum-neuron/issues), we are happy to include it._\n",
    "\n",
    "We pre-compiled the config for our training already meaning you can skip the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-28 19:04:41.000241:  608350  INFO ||NEURON_PARALLEL_COMPILE||: Removing existing workdir /tmp/ubuntu/parallel_compile_workdir\n",
      "2023-11-28 19:04:41.000245:  608350  INFO ||NEURON_PARALLEL_COMPILE||: Running trial run (add option to terminate trial run early; also ignore trial run's generated outputs, i.e. loss, checkpoints)\n",
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# precompilation command \n",
    "!neuron_parallel_compile torchrun --nproc_per_node=32 scripts/run_clm.py \\\n",
    " --model_id {model_id} \\\n",
    " --dataset_path {dataset_path} \\\n",
    " --bf16 True \\\n",
    " --learning_rate 5e-5 \\\n",
    " --output_dir dolly_llama_sharded \\\n",
    " --overwrite_output_dir True \\\n",
    " --per_device_train_batch_size 1 \\\n",
    " --gradient_checkpointing True \\\n",
    " --tensor_parallel_size 8 \\\n",
    " --max_steps 10 \\\n",
    " --logging_steps 10 \\\n",
    " --gradient_accumulation_steps 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: Compiling with out a cache can take ~40minutes. It will also create dummy files in the `dolly_llama_sharded` during compilation you we have to remove them afterwards._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove dummy artifacts which are created by the precompilation command\n",
    "!rm -rf dolly_llama_sharded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the compilation is done we can start our training with a similar command, we just need to remove the `neuron_parallel_compile`. We will use `torchrun` to launch our training script. `torchrun` is a tool that automatically distributes a PyTorch model across multiple accelerators. We can pass the number of accelerators as `nproc_per_node` arguments alongside our hyperparameters.\n",
    "\n",
    "Launch the training, with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"NousResearch/Llama-2-7b-hf\"  # non-gated\n",
    "dataset_path = \"tokenized_dolly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "> initializing tensor model parallel with size 8\n",
      "> initializing pipeline model parallel with size 1\n",
      "> initializing data parallel with size 4\n",
      "***** Running training *****\n",
      "  Num examples = 398\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 24\n",
      "  Number of trainable parameters = 842,534,912\n",
      "  0%|                                                    | 0/24 [00:00<?, ?it/s]2023-11-28 19:06:59.000365:  638478  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:06:59.000376:  638478  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpioecyc91/neuronxcc-2.11.0.34+c5231f848/MODULE_1503437627695910288+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "No Neuron cache name is saved locally. This means that only the official Neuron cache will be used. You can create a Neuron cache repo by running the following command: `optimum-cli neuron cache create`. If the Neuron cache already exists you can set it by running the following command: `optimum-cli neuron cache set -n [name]`.\n",
      "2023-11-28 19:07:20.000172:  649031  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:07:20.000410:  649036  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:07:20.000414:  649034  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:07:20.000541:  649040  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:07:20.000652:  649043  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:07:20.000653:  649045  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:07:20.000672:  649047  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:07:20.000673:  649049  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:07:20.000674:  649031  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpioecyc91/neuronxcc-2.11.0.34+c5231f848/MODULE_11220521096095644261+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-11-28 19:07:21.000437:  649036  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpioecyc91/neuronxcc-2.11.0.34+c5231f848/MODULE_2539352722715907447+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-11-28 19:07:21.000693:  649034  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpioecyc91/neuronxcc-2.11.0.34+c5231f848/MODULE_17575860727371960526+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-11-28 19:07:21.000960:  649049  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpioecyc91/neuronxcc-2.11.0.34+c5231f848/MODULE_8861683562612152874+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-11-28 19:07:22.000226:  649040  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpioecyc91/neuronxcc-2.11.0.34+c5231f848/MODULE_11737472936030846813+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-11-28 19:07:22.000226:  649045  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpioecyc91/neuronxcc-2.11.0.34+c5231f848/MODULE_7536821414867287675+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-11-28 19:07:22.000235:  649047  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpioecyc91/neuronxcc-2.11.0.34+c5231f848/MODULE_16398059650771470005+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-11-28 19:07:22.000243:  649043  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpioecyc91/neuronxcc-2.11.0.34+c5231f848/MODULE_5321993569601292553+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-Nov-28 19:07:42.0854 615220:615443 [4] nccl_net_ofi_init:1415 CCOM WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "2023-Nov-28 19:07:42.0854 615220:615443 [4] init.cc:138 CCOM WARN OFI plugin initNet() failed is EFA enabled?\n",
      "2023-11-28 19:07:58.000605:  649325  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:07:58.000869:  649325  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpioecyc91/neuronxcc-2.11.0.34+c5231f848/MODULE_17339519708772627254+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-11-28 19:07:59.000075:  649328  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:07:59.000145:  649331  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:07:59.000147:  649333  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:07:59.000223:  649337  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:07:59.000266:  649340  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:07:59.000272:  649344  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:07:59.000274:  649342  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:08:00.000368:  649328  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpioecyc91/neuronxcc-2.11.0.34+c5231f848/MODULE_13115043180190189797+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-11-28 19:08:00.000376:  649333  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpioecyc91/neuronxcc-2.11.0.34+c5231f848/MODULE_17707561591382356723+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-11-28 19:08:00.000385:  649337  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpioecyc91/neuronxcc-2.11.0.34+c5231f848/MODULE_10601725819237734730+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-11-28 19:08:00.000393:  649344  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpioecyc91/neuronxcc-2.11.0.34+c5231f848/MODULE_9039192882441986773+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-11-28 19:08:00.000918:  649340  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpioecyc91/neuronxcc-2.11.0.34+c5231f848/MODULE_13703250961872211674+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-11-28 19:08:00.000927:  649342  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpioecyc91/neuronxcc-2.11.0.34+c5231f848/MODULE_17277457361738373603+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-11-28 19:08:00.000927:  649331  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpioecyc91/neuronxcc-2.11.0.34+c5231f848/MODULE_2777767419094136584+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "  4%|â–ˆâ–Š                                         | 1/24 [01:57<45:02, 117.51s/it]2023-11-28 19:09:06.000326:  653646  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:09:06.000663:  653646  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpioecyc91/neuronxcc-2.11.0.34+c5231f848/MODULE_12788685928660698890+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-11-28 19:09:07.000350:  653649  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:09:07.000565:  653652  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:09:07.000690:  653655  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:09:07.000700:  653657  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:09:07.000754:  653661  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:09:07.000756:  653663  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:09:07.000778:  653665  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:09:08.000007:  653649  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpioecyc91/neuronxcc-2.11.0.34+c5231f848/MODULE_7943733490193408425+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-11-28 19:09:08.000698:  653652  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpioecyc91/neuronxcc-2.11.0.34+c5231f848/MODULE_7328290186745253959+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-11-28 19:09:09.000365:  653663  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpioecyc91/neuronxcc-2.11.0.34+c5231f848/MODULE_4356920434883426297+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-11-28 19:09:09.000376:  653657  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpioecyc91/neuronxcc-2.11.0.34+c5231f848/MODULE_299281182663011231+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-11-28 19:09:09.000376:  653661  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpioecyc91/neuronxcc-2.11.0.34+c5231f848/MODULE_15050396075824270127+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-11-28 19:09:09.000709:  653665  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpioecyc91/neuronxcc-2.11.0.34+c5231f848/MODULE_13646819380798778518+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-11-28 19:09:09.000720:  653655  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpioecyc91/neuronxcc-2.11.0.34+c5231f848/MODULE_10838314580783544082+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "  8%|â–ˆâ–ˆâ–ˆâ–‹                                        | 2/24 [03:03<32:04, 87.46s/it]2023-11-28 19:10:09.000403:  658147  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:10:09.000767:  658147  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpioecyc91/neuronxcc-2.11.0.34+c5231f848/MODULE_7976391161847235125+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-11-28 19:10:10.000543:  658150  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:10:10.000895:  658150  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpioecyc91/neuronxcc-2.11.0.34+c5231f848/MODULE_17008437971940686699+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-11-28 19:10:11.000153:  658153  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:10:11.000223:  658156  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:10:11.000224:  658158  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:10:11.000378:  658162  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:10:11.000381:  658164  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:10:11.000388:  658166  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:10:12.000897:  658153  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpioecyc91/neuronxcc-2.11.0.34+c5231f848/MODULE_257409570912431573+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-11-28 19:10:12.000908:  658164  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpioecyc91/neuronxcc-2.11.0.34+c5231f848/MODULE_1388554744546309082+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-11-28 19:10:12.000931:  658162  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpioecyc91/neuronxcc-2.11.0.34+c5231f848/MODULE_17181431344976025645+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-11-28 19:10:13.000287:  658158  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpioecyc91/neuronxcc-2.11.0.34+c5231f848/MODULE_13149319069762575170+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-11-28 19:10:13.000299:  658166  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpioecyc91/neuronxcc-2.11.0.34+c5231f848/MODULE_13275123067325918023+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-11-28 19:10:13.000299:  658156  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpioecyc91/neuronxcc-2.11.0.34+c5231f848/MODULE_9911022956972173782+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                         | 10/24 [06:13<04:35, 19.69s/it]2023-11-28 19:13:11.000340:  694269  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:13:11.000342:  694269  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpioecyc91/neuronxcc-2.11.0.34+c5231f848/MODULE_10421090513981540266+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "{'loss': 1.475, 'learning_rate': 2.916666666666667e-05, 'epoch': 0.4}           \n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                         | 10/24 [06:15<04:35, 19.69s/it]2023-11-28 19:13:12.000013:  694536  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:13:12.000015:  694536  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /tmp/tmpioecyc91/neuronxcc-2.11.0.34+c5231f848/MODULE_931450859643720011+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "{'loss': 1.3125, 'learning_rate': 8.333333333333334e-06, 'epoch': 0.8}          \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [09:51<00:00, 15.63s/it]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "2023-11-28 19:16:48.000318:  754759  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:16:48.000320:  754759  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/ubuntu/neuroncc_compile_workdir/248c1a71-d4d8-44ae-a61b-8c71e59dd429/model.MODULE_4181745505209985616+d41d8cd9.hlo.pb', '--output', '/tmp/ubuntu/neuroncc_compile_workdir/248c1a71-d4d8-44ae-a61b-8c71e59dd429/model.MODULE_4181745505209985616+d41d8cd9.neff', '--verbose=35']\n",
      "2023-11-28 19:16:48.000385:  754762  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:16:48.000387:  754762  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/ubuntu/neuroncc_compile_workdir/f872fc07-6c1f-4cec-a38c-c6767daa9f28/model.MODULE_5554713681523575992+d41d8cd9.hlo.pb', '--output', '/tmp/ubuntu/neuroncc_compile_workdir/f872fc07-6c1f-4cec-a38c-c6767daa9f28/model.MODULE_5554713681523575992+d41d8cd9.neff', '--verbose=35']\n",
      "2023-11-28 19:16:48.000402:  754764  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:16:48.000404:  754764  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/ubuntu/neuroncc_compile_workdir/c42eb1ba-2b52-4962-9d12-ba96e5aef748/model.MODULE_9181959185951820998+d41d8cd9.hlo.pb', '--output', '/tmp/ubuntu/neuroncc_compile_workdir/c42eb1ba-2b52-4962-9d12-ba96e5aef748/model.MODULE_9181959185951820998+d41d8cd9.neff', '--verbose=35']\n",
      "2023-11-28 19:16:48.000406:  754766  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:16:48.000408:  754766  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/ubuntu/neuroncc_compile_workdir/951de3f7-cdc7-4ec8-b958-e29386c97c5b/model.MODULE_1735128424555952831+d41d8cd9.hlo.pb', '--output', '/tmp/ubuntu/neuroncc_compile_workdir/951de3f7-cdc7-4ec8-b958-e29386c97c5b/model.MODULE_1735128424555952831+d41d8cd9.neff', '--verbose=35']\n",
      "2023-11-28 19:16:48.000507:  754772  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:16:48.000509:  754772  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/ubuntu/neuroncc_compile_workdir/07deee6f-5c9c-4aa9-8c3b-2a40210dc5ab/model.MODULE_16903007951538702365+d41d8cd9.hlo.pb', '--output', '/tmp/ubuntu/neuroncc_compile_workdir/07deee6f-5c9c-4aa9-8c3b-2a40210dc5ab/model.MODULE_16903007951538702365+d41d8cd9.neff', '--verbose=35']\n",
      "2023-11-28 19:16:48.000536:  754777  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:16:48.000538:  754777  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/ubuntu/neuroncc_compile_workdir/2686a0db-ea2f-4d06-8b35-ae2f99f88d7d/model.MODULE_3270500491026416947+d41d8cd9.hlo.pb', '--output', '/tmp/ubuntu/neuroncc_compile_workdir/2686a0db-ea2f-4d06-8b35-ae2f99f88d7d/model.MODULE_3270500491026416947+d41d8cd9.neff', '--verbose=35']\n",
      "2023-11-28 19:16:48.000540:  754783  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:16:48.000542:  754783  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/ubuntu/neuroncc_compile_workdir/f857c4d3-8128-4f00-b646-8f5aac31f3ca/model.MODULE_7685172286141693958+d41d8cd9.hlo.pb', '--output', '/tmp/ubuntu/neuroncc_compile_workdir/f857c4d3-8128-4f00-b646-8f5aac31f3ca/model.MODULE_7685172286141693958+d41d8cd9.neff', '--verbose=35']\n",
      "2023-11-28 19:16:48.000542:  754782  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:16:48.000544:  754782  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/ubuntu/neuroncc_compile_workdir/0a899257-73d3-4bab-a66e-95cc4e0fe503/model.MODULE_8375812054860065886+d41d8cd9.hlo.pb', '--output', '/tmp/ubuntu/neuroncc_compile_workdir/0a899257-73d3-4bab-a66e-95cc4e0fe503/model.MODULE_8375812054860065886+d41d8cd9.neff', '--verbose=35']\n",
      "........................................................................................................................\n",
      "Compiler status PASS\n",
      "\n",
      "Compiler status PASS\n",
      "\n",
      "Compiler status PASS\n",
      "\n",
      "Compiler status PASS\n",
      "\n",
      "Compiler status PASS\n",
      "\n",
      "Compiler status PASS\n",
      "..\n",
      "Compiler status PASS\n",
      "\n",
      "Compiler status PASS\n",
      "{'train_runtime': 906.7438, 'train_samples_per_second': 0.439, 'train_steps_per_second': 0.026, 'train_loss': 1.3932291666666667, 'epoch': 0.96}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [15:06<00:00, 37.78s/it]\n",
      "Saving model checkpoint to dolly_llama_sharded\n",
      "Model parallelism is enabled, only saving the model sharded state dict.\n",
      "2023-11-28 19:22:16.000552:  756130  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:22:16.000559:  756130  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/ubuntu/neuroncc_compile_workdir/be8b625d-6eaf-4b2a-b2c9-b92ea20b0189/model.MODULE_14420174662896292478+d41d8cd9.hlo.pb', '--output', '/tmp/ubuntu/neuroncc_compile_workdir/be8b625d-6eaf-4b2a-b2c9-b92ea20b0189/model.MODULE_14420174662896292478+d41d8cd9.neff', '--verbose=35']\n",
      "2023-11-28 19:22:16.000666:  756133  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:22:16.000672:  756133  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/ubuntu/neuroncc_compile_workdir/3a3b28c2-1eaa-4415-af08-ee89a668e6d2/model.MODULE_3124599143928466401+d41d8cd9.hlo.pb', '--output', '/tmp/ubuntu/neuroncc_compile_workdir/3a3b28c2-1eaa-4415-af08-ee89a668e6d2/model.MODULE_3124599143928466401+d41d8cd9.neff', '--verbose=35']\n",
      "2023-11-28 19:22:16.000746:  756137  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:22:16.000751:  756137  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/ubuntu/neuroncc_compile_workdir/8bde6bc5-bba2-4606-9e04-1af3b79fd57b/model.MODULE_10833474347515123183+d41d8cd9.hlo.pb', '--output', '/tmp/ubuntu/neuroncc_compile_workdir/8bde6bc5-bba2-4606-9e04-1af3b79fd57b/model.MODULE_10833474347515123183+d41d8cd9.neff', '--verbose=35']\n",
      "2023-11-28 19:22:16.000853:  756141  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:22:16.000859:  756141  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/ubuntu/neuroncc_compile_workdir/af2608bc-8687-4a37-a8f5-74cb16d3d677/model.MODULE_9214541117211669183+d41d8cd9.hlo.pb', '--output', '/tmp/ubuntu/neuroncc_compile_workdir/af2608bc-8687-4a37-a8f5-74cb16d3d677/model.MODULE_9214541117211669183+d41d8cd9.neff', '--verbose=35']\n",
      "....2023-11-28 19:22:17.000726:  756162  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:22:17.000732:  756162  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/ubuntu/neuroncc_compile_workdir/2010148b-1a1d-474c-a2da-8d2b6265e322/model.MODULE_10069751981625194898+d41d8cd9.hlo.pb', '--output', '/tmp/ubuntu/neuroncc_compile_workdir/2010148b-1a1d-474c-a2da-8d2b6265e322/model.MODULE_10069751981625194898+d41d8cd9.neff', '--verbose=35']\n",
      "2023-11-28 19:22:17.000882:  756166  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:22:17.000888:  756166  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/ubuntu/neuroncc_compile_workdir/db97e47f-f4a5-44c7-82f8-8f018a16405c/model.MODULE_8024549571506811765+d41d8cd9.hlo.pb', '--output', '/tmp/ubuntu/neuroncc_compile_workdir/db97e47f-f4a5-44c7-82f8-8f018a16405c/model.MODULE_8024549571506811765+d41d8cd9.neff', '--verbose=35']\n",
      ".2023-11-28 19:22:17.000966:  756170  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:22:17.000966:  756172  INFO ||NEURON_CACHE||: Compile cache path: /tmp/tmpioecyc91\n",
      "2023-11-28 19:22:17.000974:  756170  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/ubuntu/neuroncc_compile_workdir/af19c97e-cb0c-49ba-9dc1-6652fdb9ba0f/model.MODULE_15176729306763045219+d41d8cd9.hlo.pb', '--output', '/tmp/ubuntu/neuroncc_compile_workdir/af19c97e-cb0c-49ba-9dc1-6652fdb9ba0f/model.MODULE_15176729306763045219+d41d8cd9.neff', '--verbose=35']\n",
      "2023-11-28 19:22:17.000977:  756172  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/ubuntu/neuroncc_compile_workdir/d65291ce-1c5a-42f1-becb-5e65cc8629ee/model.MODULE_4929319483898486377+d41d8cd9.hlo.pb', '--output', '/tmp/ubuntu/neuroncc_compile_workdir/d65291ce-1c5a-42f1-becb-5e65cc8629ee/model.MODULE_4929319483898486377+d41d8cd9.neff', '--verbose=35']\n",
      "..................................................................................................................................."
     ]
    }
   ],
   "source": [
    "!torchrun --nproc_per_node=32 scripts/run_clm.py \\\n",
    " --model_id {model_id} \\\n",
    " --dataset_path {dataset_path} \\\n",
    " --bf16 True \\\n",
    " --learning_rate 5e-5 \\\n",
    " --output_dir dolly_llama_sharded \\\n",
    " --overwrite_output_dir True \\\n",
    " --per_device_train_batch_size 1 \\\n",
    " --gradient_checkpointing True \\\n",
    " --tensor_parallel_size 8 \\\n",
    " --num_train_epochs 1 \\\n",
    " --logging_steps 10 \\\n",
    " --gradient_accumulation_steps 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training is done we have to consolidate our model. Tensor Parallelism consists sharded the model weights accross different workers, only sharded checkpoints will be saved during training. We need to consolidate the sharded checkpoints to be able to share and use the model. \n",
    "\n",
    "The Optimum CLI provides a way of doing that very easily via the `optimum neuron consolidate`` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!optimum-cli neuron consolidate dolly_llama_sharded dolly_llama\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats it, we successfully trained Llama on AWS Trainium and can now share it, evaluate it or use it for inference. In the next section we will compile our trained model for inference use and test it.\n",
    "\n",
    "The training took for 3 epochs leading to a cost of ~1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evalaute and test fine-tuned Llama model\n",
    "\n",
    "Similar to training to be able to run inferece on AWS Trainium or AWS Inferentia2 we need to compile our model for the correct use. We will use our Trainium instance for the inference test, but we recommend customer to switch to Inferentia2 for inference. \n",
    "\n",
    "Optimum Neuron implements similar to Transformers AutoModel classes for easy inference use. We will use  the `NeuronModelForCausalLM` class to load our vanilla transformers checkpoint and convert it to neuron. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.neuron import NeuronModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "compiler_args = {\"num_cores\": 24, \"auto_cast_type\": 'fp16'}\n",
    "input_shapes = {\"batch_size\": 1, \"sequence_length\": 2048}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dolly_llama\")\n",
    "model = NeuronModelForCausalLM.from_pretrained(\n",
    "        \"dolly_llama\",\n",
    "        export=True,\n",
    "        **compiler_args,\n",
    "        **input_shapes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: Inference compilation can take ~45minutes. Luckily, you need to only run this onces. Since you can save the model afterwards._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMENT IN if you want to save the compiled model\n",
    "# model.save_pretrained(\"compiled_dolly_llama\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now test inference, but have to make sure we format our input to our prompt format we used for fine-tuning. Therefore we created a helper method, which accepts a `dict` with our `instruction` and optionally a `context`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dolly_infernece(sample):\n",
    "    instruction = f\"### Instruction\\n{sample['instruction']}\"\n",
    "    context = f\"### Context\\n{sample['context']}\" if len(sample[\"context\"]) > 0 else None\n",
    "    response = f\"### Answer\\n\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def generate(sample): \n",
    "    prompt = format_dolly_infernece(sample)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs,\n",
    "                         max_new_tokens=512,\n",
    "                         do_sample=True,\n",
    "                         temperature=0.9,\n",
    "                         top_k=50,\n",
    "                         top_p=0.9)\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets test inference. First we test without a context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = {\n",
    "  \"instruction\": \"Can you tell me something about AWS?\"\n",
    "}\n",
    "res = generate(prompt)\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks correct. Now, lets add some context, e.g. as you would do for RAG applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = {\n",
    "  \"instruction\": \"How can train models on AWS Trainium?\",\n",
    "  \"context\": \"ðŸ¤— Optimum Neuron is the interface between the ðŸ¤— Transformers library and AWS AcceleratorsÂ including [AWS Trainium](https://aws.amazon.com/machine-learning/trainium/?nc1=h_ls) and [AWS Inferentia](https://aws.amazon.com/machine-learning/inferentia/?nc1=h_ls). It provides a set of tools enabling easy model loading, training and inference on single- and multi-Accelerator settings for different downstream tasks.\"\n",
    "}\n",
    "res = generate(prompt)\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, our model also correctly uses the provided context. We are done. Congrats on fine-tuning Llama on AWS Trainium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
