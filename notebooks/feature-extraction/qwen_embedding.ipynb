{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "637da493-becd-4e26-9030-ecaeb880a054",
   "metadata": {},
   "source": [
    "# Qwen3 Embedding on AWS Trainium with Optimum Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8983af9f-81a7-4e45-a7d6-907aded0b4bd",
   "metadata": {},
   "source": [
    "This guide explains how to convert, load, and use Qwen Embedding models on AWS Trainium and Inferentia2 using Optimum Neuron. The Qwen3 Embedding model series is the latest proprietary model of the Qwen family, specifically designed for text embedding and ranking tasks. Building upon the dense foundational models of the Qwen3 series, it provides a comprehensive range of text embeddings and reranking models in various sizes (0.6B, 4B, and 8B). The Qwen3 Embedding series offer support for over 100 languages, thanks to the multilingual capabilites of Qwen3 models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526e884a-690b-4f82-94af-19f7a7c51492",
   "metadata": {},
   "source": [
    "## Prerequisite: Setup Environment \n",
    "\n",
    "You can run this notebook on a AWS EC2 instance with the HF DLAMI. To create an instance with the DLAMI, you can follow the [EC2 Setup guide](https://huggingface.co/docs/optimum-neuron/en/ec2-setup). Alternatively if you are on a AWS Trainium and Inferentia instance, you can manually install `optimum-neuron` using the steps in the [manual installation guide](https://huggingface.co/docs/optimum-neuron/en/ec2-setup#alternative-manual-installation). \n",
    "\n",
    "This guide is written using a `trn2.3xlarge` AWS Trainium2 instance. But you can use the same code to run the model using a AWS Inferentia2 instance like `inf2.48xlarge`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ea763e-63f7-4e1e-9f23-7f001ff541d6",
   "metadata": {},
   "source": [
    "## Compile Qwen Embedding Models for AWS Trainium\n",
    "First, you need to convert the model to a format compatible with AWS Trainium and Inferentia2. You can compile Qwen3 Embedding models with Optimum Neuron using the `optimum-cli` or `NeuronModelForEmbedding` class. Below you will find an example for both approaches. \n",
    "\n",
    "In the below example, we illustrate this using [Qwen3 Embedding 8B](https://huggingface.co/Qwen/Qwen3-Embedding-8B) but you can follow the same steps for the 0.8B and the 4B versions of the embedding models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9682db-a346-4ba3-906e-108264eec65c",
   "metadata": {},
   "source": [
    "### Option A: Compile using the `NeuronModelForEmbedding` class\n",
    "Here we will use the `NeuronModelForEmbedding` class, which can convert Qwen3 Embedding models to a format compatible with AWS Trainium and Inferentia2 or load already converted models. When exporting models with `NeuronModelForEmbedding` you need to define the the `sequence_length` and `batch size` in the neuron config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57097b5e-3fe8-43dd-839c-526d30c4de2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from optimum.neuron import NeuronModelForEmbedding\n",
    "\n",
    "model_id = \"Qwen/Qwen3-Embedding-8B\"\n",
    "neuron_model_dir = \"qwen_embedding_8B_tp4\"\n",
    "\n",
    "# If you are using a AWS Inferentia2 instance and use 'tensor_parallel_size=4', you should set the following environment variable as well.\n",
    "#import os\n",
    "#os.environ[\"LOCAL_WORLD_SIZE\"] = '4'\n",
    "\n",
    "neuron_config = NeuronModelForEmbedding.get_neuron_config(\n",
    "        model_id, batch_size=2, sequence_length=1024, tensor_parallel_size=4\n",
    "    )\n",
    "\n",
    "neuron_model = NeuronModelForEmbedding.export(\n",
    "        model_id=model_id, neuron_config=neuron_config, load_weights=False\n",
    "    )\n",
    "\n",
    "# Save model to disk\n",
    "neuron_model.save_pretrained(neuron_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3bbf30-a127-4590-a5d8-23cace6124ad",
   "metadata": {},
   "source": [
    "### Option B: Compile using the `optimum-cli` tool\n",
    "Here we will use the `optimum-cli` tool to convert the model. Similar to the `NeuronModelForEmbedding` we need to define our sequence length and batch size. The `optimum-cli` will automatically convert the model to a format compatible with AWS Trainium and Inferentia2 and save it to the specified output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4bd273-3417-4299-a5d5-e10796d8b125",
   "metadata": {},
   "outputs": [],
   "source": [
    "! optimum-cli export neuron --model Qwen/Qwen3-Embedding-8B --batch_size 2 --sequence_length 1024 --auto_cast matmul --instance_type trn2 --tensor_parallel_size 4 qwen_embedding_8B_tp4/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecc7c42-c0a1-4b98-b03d-4576c095f971",
   "metadata": {},
   "source": [
    "## Load compiled Qwen3 Embedding model and run inference\n",
    "Once we have a compiled the model, for loading the model we can use the `NeuronModelForEmbedding` class to load the model and run inference.\n",
    "In the below example, we first compute embeddings for two queries and documents; and then compute the similarity score for the queries and documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83452f1-2036-441a-b564-4c8ce063b456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from optimum.neuron import NeuronModelForEmbedding\n",
    "\n",
    "#set this to the path that has the compiled model from above\n",
    "model_id_or_path = neuron_model_dir\n",
    "\n",
    "\n",
    "def last_token_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "    left_padding = attention_mask[:, -1].sum() == attention_mask.shape[0]\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
    "\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = NeuronModelForEmbedding.from_pretrained(model_id_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"left\")\n",
    "\n",
    "# Input text to embed\n",
    "queries = [\n",
    "    \"What is the capital of China?\",\n",
    "    \"Explain gravity\"\n",
    "]\n",
    "# No need to add instruction for retrieval documents\n",
    "documents = [\n",
    "    \"The capital of China is Beijing.\",\n",
    "    \"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\"\n",
    "]\n",
    "\n",
    "\n",
    "# Tokenize the input texts\n",
    "queries_tokens = tokenizer(\n",
    "    queries,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=8192,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "documents_tokens = tokenizer(\n",
    "    documents,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=8192,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "outputs = model(**queries_tokens)\n",
    "queries_embeddings = last_token_pool(outputs, queries_tokens['attention_mask'])\n",
    "\n",
    "outputs = model(**documents_tokens)\n",
    "documents_embeddings = last_token_pool(outputs, documents_tokens['attention_mask'])\n",
    "\n",
    "# normalize embeddings and compute similarity scores\n",
    "queries_embeddings = F.normalize(queries_embeddings, p=2, dim=1)\n",
    "documents_embeddings = F.normalize(documents_embeddings, p=2, dim=1)\n",
    "\n",
    "scores = (queries_embeddings @ documents_embeddings.T)\n",
    "print(\"Similarity Scores -->\", scores.tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
