# This is a template file for testing model parallelization.

import os
from contextlib import nullcontext
from inspect import signature

import torch
import neuronx_distributed
from neuronx_distributed import parallel_layers
import torch_xla.core.xla_model as xm

from transformers import AutoConfig, AutoTokenizer, {model_class}
from transformers.trainer_utils import set_seed

import optimum
from optimum.neuron.distributed import ParallelizersManager, lazy_load_for_parallelism

from utils import gather_along_last_dim, generate_dummy_labels


if os.environ.get("TORCHELASTIC_RUN_ID"):
    import torch_xla.distributed.xla_backend as xbn

    if not isinstance(torch.distributed.group.WORLD, xbn.ProcessGroupXla):
        torch.distributed.init_process_group(backend="xla")

SEED = 42

from_config = os.environ["from_config"] == "true"
lazy_load = os.environ["lazy_load"] == "true"
is_parallel = os.environ["is_parallel"] == "true"
config_overwrite = os.environ.get("config_overwrite", "")
parallelize_embeddings = is_parallel and os.environ["parallelize_embeddings"] == "true"

# This is required to prevent `parallel_cross_entropy` to mutate the logits (which would make them not comparable).
if is_parallel and parallelize_embeddings:
    optimum.neuron.distributed.parallel_layers._PARALLEL_CROSS_ENTROPY_SHOULD_PRESERVE_INPUT = True

# Initialize TP
if is_parallel:
  neuronx_distributed.parallel_layers.parallel_state.initialize_model_parallel(tensor_model_parallel_size={tp_size})


config = AutoConfig.from_pretrained("{model_name_or_path}")
config_overwrite = config_overwrite.split(",")
for overwrite_info in config_overwrite:
    if overwrite_info == "":
      continue
    attr_name, attr_value = overwrite_info.split("=")
    attr_type = type(getattr(config, attr_name))
    setattr(config, attr_name, attr_type(attr_value))

if xm.get_ordinal() == 0:
  print(config)

preprocessor = AutoTokenizer.from_pretrained("{model_name_or_path}")

inputs = preprocessor("This is a test to check that TP is working.", return_tensors="pt")

def load_model_with_seed(seed: int, from_config: bool):
    set_seed(seed)
    if from_config:
        model = {model_class}(config)
    else:
      ctx = lazy_load_for_parallelism(tensor_parallel_size={tp_size}) if lazy_load else nullcontext()
      with ctx:
          model = {model_class}.from_pretrained("{model_name_or_path}", config=config, ignore_mismatched_sizes=True)
    return model


model = load_model_with_seed(SEED, from_config)
model = model.eval()

vocab_size = getattr(model.config, "vocab_size", None)

if is_parallel:
    model = ParallelizersManager.parallelizer_for_model(model).parallelize(model, parallelize_embeddings=parallelize_embeddings)
    parallel_layers.move_model_to_device(model, "xla")
    filename = "parallel.bin"
else:
    model = model.to("xla")
    filename = "original.bin"

xla_inputs = dict()
sig = signature(model.forward)
for k, v in inputs.items():
    if k not in sig.parameters:
        continue
    xla_inputs[k] = v.to("xla")
    decoder_input_name = "decoder_" + k
    if model.config.is_encoder_decoder and decoder_input_name in sig.parameters:
        xla_inputs[decoder_input_name] = v.to("xla")

# We take the shape of the first input to "predict" the shape of the labels.
# Might not work for every tasks.
shape = list(xla_inputs.values())[0].shape

xla_inputs.update(generate_dummy_labels(model, shape, vocab_size=vocab_size, device="xla", seed=SEED))

model_outputs = model(**xla_inputs, return_dict=True)
xm.mark_step()

if is_parallel and parallelize_embeddings:
    gathered_model_outputs =  dict()
    for name, output in model_outputs.items():
        if name == "loss" or output is None or not isinstance(output, torch.Tensor) or output.shape[-1] != (vocab_size // {tp_size}):
            gathered_model_outputs[name] = output
        else:
            gathered_model_outputs[name] = gather_along_last_dim(output)
    model_outputs = gathered_model_outputs

xm.save(model_outputs, "{output_path}" + "/" + filename)
