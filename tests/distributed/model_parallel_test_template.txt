# This is a template file for testing model parallelization.

import os
from contextlib import nullcontext
from inspect import signature

import torch
import neuronx_distributed
from neuronx_distributed import parallel_layers
from neuronx_distributed.utils.model_utils import move_model_to_device
import torch_xla.core.xla_model as xm

from transformers import AutoConfig, AutoTokenizer, {model_class}
from transformers.trainer_utils import set_seed

import optimum
from optimum.neuron.utils.training_utils import set_neuron_cc_optlevel_for_model
from optimum.neuron.distributed import ParallelizersManager, lazy_load_for_parallelism

from utils import gather_along_dim, generate_dummy_labels, create_static_seed_patcher


if os.environ.get("TORCHELASTIC_RUN_ID"):
    import torch_xla.distributed.xla_backend as xbn

    if not isinstance(torch.distributed.group.WORLD, xbn.ProcessGroupXla):
        torch.distributed.init_process_group(backend="xla")

SEED = 42

from_config = os.environ["from_config"] == "true"
lazy_load = os.environ["lazy_load"] == "true"
is_parallel = os.environ["is_parallel"] == "true"
config_overwrite = os.environ.get("config_overwrite", "")
parallelize_embeddings = is_parallel and os.environ["parallelize_embeddings"] == "true"
sequence_parallel_enabled = os.environ["sequence_parallel_enabled"] == "true"
computing_loss_is_supported = os.environ["computing_loss_is_supported"] == "true"

# This is required to prevent `parallel_cross_entropy` to mutate the logits (which would make them not comparable).
if is_parallel and parallelize_embeddings:
    optimum.neuron.distributed.parallel_layers._PARALLEL_CROSS_ENTROPY_SHOULD_PRESERVE_INPUT = True

# Initialize TP
if is_parallel:
  neuronx_distributed.parallel_layers.parallel_state.initialize_model_parallel(tensor_model_parallel_size={tp_size})


config = AutoConfig.from_pretrained("{model_name_or_path}")
config_overwrite = config_overwrite.split(",")
for overwrite_info in config_overwrite:
    if overwrite_info == "":
      continue
    attr_name, attr_value = overwrite_info.split("=")
    attr_type = type(getattr(config, attr_name))
    setattr(config, attr_name, attr_type(attr_value))

if getattr(config, "problem_type", None) is None:
    config.problem_type = "single_label_classification"

if xm.get_ordinal() == 0:
  print(config)

preprocessor = AutoTokenizer.from_pretrained("{model_name_or_path}")

inputs = preprocessor("This is a test to check that TP is working.", return_tensors="pt")

if sequence_parallel_enabled:
    for name, tensor in inputs.items():
        if tensor.shape[1] % {tp_size} != 0:
            tensor = torch.nn.functional.pad(
              tensor, pad=(0, tensor.shape[1] % {tp_size}), value=1,
            )
            inputs[name] = tensor

def load_model_with_seed(seed: int, from_config: bool):
    set_seed(seed)
    if from_config:
        model = {model_class}(config)
    else:
      tp_size = {tp_size} if is_parallel else 1
      ctx = lazy_load_for_parallelism(tensor_parallel_size=tp_size) if lazy_load else nullcontext()
      with ctx:
          model = {model_class}.from_pretrained("{model_name_or_path}", config=config, ignore_mismatched_sizes=True)
    return model

static_seed_patcher = create_static_seed_patcher({model_class}, SEED)
with static_seed_patcher:
    model = load_model_with_seed(SEED, from_config)
    
    set_neuron_cc_optlevel_for_model(model)
    
    vocab_size = getattr(model.config, "vocab_size", None)
    
    if is_parallel:
        model = ParallelizersManager.parallelizer_for_model(model).parallelize(
            model, 
            parallelize_embeddings=parallelize_embeddings, 
            sequence_parallel_enabled=sequence_parallel_enabled,
        )
        filename = "parallel.bin"
    else:
        filename = "original.bin"

move_model_to_device(model, "xla")
model = model.eval()

xla_inputs = dict()
sig = signature(model.forward)
for k, v in inputs.items():
    if k not in sig.parameters:
        continue
    xla_inputs[k] = v.to("xla")
    decoder_input_name = "decoder_" + k
    if model.config.is_encoder_decoder and decoder_input_name in sig.parameters:
        xla_inputs[decoder_input_name] = v.to("xla")

# We take the shape of the first input to "predict" the shape of the labels.
# Might not work for every tasks.
shape = list(xla_inputs.values())[0].shape

if computing_loss_is_supported:
    xla_inputs.update(generate_dummy_labels(model, shape, vocab_size=vocab_size, device="xla", seed=SEED))

model_outputs = model(**xla_inputs, return_dict=True)
xm.mark_step()

axis_to_gather = dict()
axis_to_gather["default"] = -1
axis_to_gather["past_key_values"] = 1

def gather_output(output, gather_dim):
    if isinstance(output, (tuple, list, set)):
        output_type = type(output)
        gathered_output = []
        for t in output:
            gathered_output.append(gather_output(t, gather_dim))
        result = output_type(gathered_output)
    else:
        result = gather_along_dim(output, gather_dim)
    return result

if is_parallel:
    # Because of parallelism (embeddings and sequence parallelism), some outputs need to be gathered.
    # Since it is not possible to generically know which one, we save both the "regular" output and the gathered 
    # version of it. We then compare both of them to the original output and fail if both do not match.
    gathered_model_outputs =  dict()
    for name, output in model_outputs.items():
        gathered_model_outputs[name] = output
        if name == "loss" or output is None: 
            gathered_output = output
        else:
            gathered_output = gather_output(output, axis_to_gather.get(name, axis_to_gather["default"]))
        gathered_output_name = "gathered_" + name
        gathered_model_outputs[gathered_output_name] = gathered_output
    model_outputs = gathered_model_outputs

xm.save(model_outputs, "{output_path}" + "/" + filename)
