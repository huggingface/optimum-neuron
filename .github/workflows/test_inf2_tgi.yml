name: Optimum neuron / Test TGI on INF2

on:
  push:
    branches: [ main ]
    paths:
      - "setup.py"
      - "optimum/**.py"
      - "text-generation-inference/**"
      - ".github/workflows/test_inf2_tgi.yml"
  pull_request:
    branches: [ main ]
    paths:
      - "setup.py"
      - "optimum/**.py"
      - "text-generation-inference/**"
      - ".github/workflows/test_inf2_tgi.yml"

concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}
  cancel-in-progress: true

jobs:
  do-the-job:
    name: Run TGI tests
    runs-on:
      group: aws-inf2-8xlarge
    env:
      HF_HUB_ENABLE_HF_TRANSFER: 1
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Install Neuronx runtime
        uses: ./.github/actions/install_neuronx_runtime
      - name: Prepare virtual environment
        uses: ./.github/actions/prepare_venv
      - name: Install integration tests prerequisites
        run: |
          source aws_neuron_venv_pytorch/bin/activate
          python -m pip install -r text-generation-inference/tests/requirements.txt
      - name: Run TGI server python tests
        run: |
          # gawk is required when invoking the Makefile targets
          sudo apt install gawk -y
          source aws_neuron_venv_pytorch/bin/activate
          HF_TOKEN=${{ secrets.HF_TOKEN_OPTIMUM_NEURON_CI }} make tgi_test
      - name: Build docker image
        shell: bash
        run: |
          source aws_neuron_venv_pytorch/bin/activate
          make neuronx-tgi
      - name: Run TGI docker tests
        shell: bash
        run: |
          source aws_neuron_venv_pytorch/bin/activate
          HF_TOKEN=${{ secrets.HF_TOKEN_OPTIMUM_NEURON_CI }} python -m pytest -sv text-generation-inference/tests -k integration
