name: Optimum neuron / Test INF2 vLLM

on:
  # Trigger after sanity check completes
  workflow_run:
    workflows: ["Optimum neuron / Sanity check"]
    types:
      - completed

concurrency:
  group: ${{ github.workflow }}-${{ github.event.workflow_run.head_branch || github.ref_name || github.run_id }}
  cancel-in-progress: true

jobs:
  check-and-gate:
    name: Check Sanity and Paths
    runs-on: ubuntu-22.04
    outputs:
      should_run: ${{ steps.check.outputs.should_run }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Check sanity and paths
        id: check
        uses: ./.github/actions/check-sanity-and-paths
        with:
          path_patterns: |
            ^pyproject\.toml$
            ^optimum/commands/neuron/serve\.py$
            ^optimum/exporters/neuron/__main__\.py$
            ^optimum/neuron/configuration_utils\.py$
            ^optimum/neuron/modeling_base\.py$
            ^optimum/neuron/models/[^/]+\.py$
            ^optimum/neuron/models/inference/auto_models\.py$
            ^optimum/neuron/models/inference/modeling_utils\.py$
            ^optimum/neuron/models/inference/backend/.*\.py$
            ^optimum/neuron/models/inference/(granite|llama|llama4|mixtral|phi3|qwen2|qwen3|qwen3_moe)/.*\.py$
            ^optimum/neuron/vllm/.*\.py$
            ^optimum/neuron/utils/.*\.py$
            ^optimum/neuron/version\.py$
            ^tests/fixtures/llm/.*\.py$
            ^tests/vllm/.*\.py$
            ^docker/vllm/Dockerfile$
            ^\.github/workflows/test_inf2_vllm\.yml$

  do-the-job:
    name: Run INF2 vLLM tests
    needs: check-and-gate
    if: needs.check-and-gate.outputs.should_run == 'true'
    runs-on:
      group: aws-inf2-8xlarge
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Install Neuronx runtime
        uses: ./.github/actions/install_neuronx_runtime
      - name: Setup virtual environment
        uses: ./.github/actions/setup_venv
      - name: Install vLLM and test prerequisites
        run: |
          source aws_neuron_venv_pytorch/bin/activate
          uv pip install .[vllm,vllm-tests]
      - name: Export test models
        run: |
          source aws_neuron_venv_pytorch/bin/activate
          export HF_TOKEN=${{ secrets.HF_TOKEN_OPTIMUM_NEURON_CI }}
          python tests/fixtures/llm/export_models.py
      - name: Run vLLM engine embedding tests
        run: |
          source aws_neuron_venv_pytorch/bin/activate
          export HF_TOKEN=${{ secrets.HF_TOKEN_OPTIMUM_NEURON_CI }}
          pytest -sv tests/vllm/engine/test_vllm_engine_embedding.py
      - name: Run vLLM engine generation tests
        run: |
          source aws_neuron_venv_pytorch/bin/activate
          export HF_TOKEN=${{ secrets.HF_TOKEN_OPTIMUM_NEURON_CI }}
          pytest -sv tests/vllm/engine/test_vllm_engine_generate.py
      - name: Run vLLM service tests
        run: |
          source aws_neuron_venv_pytorch/bin/activate
          export HF_TOKEN=${{ secrets.HF_TOKEN_OPTIMUM_NEURON_CI }}
          pytest -sv tests/vllm/service
      - name: Build docker image
        run: |
          sudo apt-get install -y gawk
          make optimum-neuron-vllm
      - name: Run vLLM docker tests
        run: |
          source aws_neuron_venv_pytorch/bin/activate
          export HF_TOKEN=${{ secrets.HF_TOKEN_OPTIMUM_NEURON_CI }}
          pytest -sv tests/vllm/docker
