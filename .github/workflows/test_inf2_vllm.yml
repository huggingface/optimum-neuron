name: Optimum neuron / Test INF2 vLLM

on:
  push:
    branches: [ main ]
    paths:
      - "pyproject.toml"
      - "optimum/commands/**.py"
      - "optimum/exporters/neuron/__main__.py"
      - "optimum/neuron/configuration_utils.py"
      - "optimum/neuron/modeling_base.py"
      - "optimum/neuron/modeling_decoder.py"
      - "optimum/neuron/models/*.py"
      - "optimum/neuron/models/inference/auto_models.py"
      - "optimum/neuron/models/inference/backend/**.py"
      - "optimum/neuron/models/inference/granite/**.py"
      - "optimum/neuron/models/inference/llama/**.py"
      - "optimum/neuron/models/inference/llama4/**.py"
      - "optimum/neuron/models/inference/mixtral/**.py"
      - "optimum/neuron/models/inference/phi3/**.py"
      - "optimum/neuron/models/inference/qwen2/**.py"
      - "optimum/neuron/models/inference/qwen3/**.py"
      - "optimum/neuron/models/inference/qwen3_moe/**.py"
      - "optimum/neuron/vllm/**.py"
      - "optimum/neuron/utils/**.py"
      - "optimum/neuron/version.py"
      - "tests/vllm/**.py"
      - ".github/workflows/test_inf2_vllm.yml"
  pull_request:
    branches: [ main ]
    paths:
      - "pyproject.toml"
      - "optimum/commands/**.py"
      - "optimum/exporters/neuron/__main__.py"
      - "optimum/neuron/configuration_utils.py"
      - "optimum/neuron/modeling_base.py"
      - "optimum/neuron/modeling_decoder.py"
      - "optimum/neuron/models/*.py"
      - "optimum/neuron/models/inference/auto_models.py"
      - "optimum/neuron/models/inference/backend/**.py"
      - "optimum/neuron/models/inference/granite/**.py"
      - "optimum/neuron/models/inference/llama/**.py"
      - "optimum/neuron/models/inference/llama4/**.py"
      - "optimum/neuron/models/inference/mixtral/**.py"
      - "optimum/neuron/models/inference/phi3/**.py"
      - "optimum/neuron/models/inference/qwen2/**.py"
      - "optimum/neuron/models/inference/qwen3/**.py"
      - "optimum/neuron/models/inference/qwen3_moe/**.py"
      - "optimum/neuron/pipelines/transformers/base.py"
      - "optimum/neuron/utils/**.py"
      - "optimum/neuron/version.py"
      - "tests/vllm/**.py"
      - ".github/workflows/test_inf2_llm.yml"

concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}
  cancel-in-progress: true

jobs:
  do-the-job:
    name: Run INF2 vLLM tests
    runs-on:
      group: aws-inf2-8xlarge
    env:
      HF_XET_HIGH_PERFORMANCE: 1
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Install Neuronx runtime
        uses: ./.github/actions/install_neuronx_runtime
      - name: Prepare virtual environment
        uses: ./.github/actions/prepare_venv
      - name: Install optimum-neuron
        uses: ./.github/actions/install_optimum_neuron
      - name: Install vLLM and test prerequisites
        run: |
          source aws_neuron_venv_pytorch/bin/activate
          pip install .[vllm,vllm-tests]
      - name: Export test models
        run: |
          source aws_neuron_venv_pytorch/bin/activate
          export HF_TOKEN=${{ secrets.HF_TOKEN_OPTIMUM_NEURON_CI }}
          python tests/fixtures/llm/export_models.py
      - name: Run vLLM engine tests
        run: |
          source aws_neuron_venv_pytorch/bin/activate
          export HF_TOKEN=${{ secrets.HF_TOKEN_OPTIMUM_NEURON_CI }}
          pytest -sv tests/vllm/engine
      - name: Run vLLM service tests
        run: |
          source aws_neuron_venv_pytorch/bin/activate
          export HF_TOKEN=${{ secrets.HF_TOKEN_OPTIMUM_NEURON_CI }}
          pytest -sv tests/vllm/service
      - name: Build docker image
        run: |
          sudo apt-get install -y gawk
          make optimum-neuron-vllm
      - name: Run vLLM docker tests
        run: |
          source aws_neuron_venv_pytorch/bin/activate
          export HF_TOKEN=${{ secrets.HF_TOKEN_OPTIMUM_NEURON_CI }}
          pytest -sv tests/vllm/docker
