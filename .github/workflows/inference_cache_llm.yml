name: Optimum neuron LLM inference cache builder

on:
  workflow_dispatch:
  schedule:
    # Schedule the workflow to run every day at midnight UTC
    - cron: '0 0 * * *'
  push:
    tags:
      - 'v[0-9]+.[0-9]+.[0-9]+'
  pull_request:
    branches: [ main ]
    paths:
      - "tools/cache/auto_fill_llm_cache.py"

concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}

jobs:
  cache:
    name: Create optimum-neuron LLM inference cache
    runs-on:
      group: aws-inf2-8xlarge
    env:
      HF_HUB_ENABLE_HF_TRANSFER: 1
    strategy:
      fail-fast: false
      matrix:
        config: [
          llama,
          llama4,
          qwen,
          qwen-moe,
          qwen2.5,
          granite,
          phi4,
          llama3.1-70b,
          qwen2.5-large,
          llama-variants,
          smollm3,
        ]
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Install Neuronx runtime
        uses: ./.github/actions/install_neuronx_runtime
      - name: Prepare virtual environment
        uses: ./.github/actions/prepare_venv
      - name: Install optimum-neuron
        uses: ./.github/actions/install_optimum_neuron
      - name: Create cache for ${{matrix.config}} models
        run: |
          source aws_neuron_venv_pytorch/bin/activate
          config_prefix_url=https://huggingface.co/aws-neuron/optimum-neuron-cache/raw/main/inference-cache-config
          HF_TOKEN=${{secrets.HF_TOKEN_OPTIMUM_NEURON_CACHE}} \
            python tools/cache/auto_fill_llm_cache.py --config_file ${config_prefix_url}/${{matrix.config}}.json
