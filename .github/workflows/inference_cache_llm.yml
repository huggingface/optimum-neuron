name: Optimum neuron LLM inference cache builder

on:
  workflow_dispatch:
  schedule:
    # Schedule the workflow to run every day at midnight UTC
    - cron: '0 0 * * *'
  push:
    tags:
      - 'v[0-9]+.[0-9]+.[0-9]+'

concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}

jobs:
  cache:
    name: Create optimum-neuron inference cache
    runs-on:
      group: aws-inf2-48xlarge
    env:
      AWS_REGION: us-east-1
      HF_HUB_ENABLE_HF_TRANSFER: 1
    strategy:
      fail-fast: false
      matrix:
        config: [
          llama,
          qwen,
          qwen-moe,
          qwen2.5,
          granite,
          phi4,
          llama3.1-70b,
          qwen2.5-large,
          llama-variants,
          smollm3,
        ]
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Install Neuronx runtime
        uses: ./.github/actions/install_neuronx_runtime
      - name: Prepare virtual environment
        uses: ./.github/actions/prepare_venv
      - name: Install optimum-neuron
        uses: ./.github/actions/install_optimum_neuron
      - name: Install cv2 dependencies
        run: |
          sudo apt-get install python3-opencv -y
      - name: Create cache for ${{matrix.config}} models
        run: |
          source aws_neuron_venv_pytorch/bin/activate
          config_prefix_url=https://huggingface.co/aws-neuron/optimum-neuron-cache/raw/main/inference-cache-config
          HF_TOKEN=${{secrets.HF_TOKEN_OPTIMUM_NEURON_CACHE}} \
            python tools/auto_fill_inference_cache.py --config_file ${config_prefix_url}/${{matrix.config}}.json
