<!---
Copyright 2024 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

# [DRAFT] Llama 3.2 Continuous Pretraining on SageMaker Hyperpod with Pre-built Containers

This tutorial demonstrates how to continuously pre-train the Llama 3.2 1B model using the Hugging Face Optimum Neuron library on AWS SageMaker Hyperpod. We leverage several performance optimizations such as tensor parallelism, sequence parallelism, and ZeRO-1 to efficiently train large language models on Trainium-powered instances.

One of the key benefits of using SageMaker Hyperpod is the ability to leverage the pre-built Optimum Neuron containers provided by Hugging Face. These containers come with all the necessary libraries and dependencies pre-installed, making it easy to get started with training on AWS Trainium instances.


By using the SageMaker pre-built containers, you can avoid the hassle of manually setting up the environment and focus on the core training and fine-tuning tasks. The containers are optimized for performance and include various optimization techniques, such as tensor parallelism and selective checkpointing, to efficiently train large language models like Llama 3.2 1B.


## 1. Setup AWS Environment

Before starting this tutorial, you will need to set up your AWS environment:

1. Create an AWS SageMaker Hyperpod cluster with at least one `trn1.32xlarge` instance. You can follow the [Hyperpod EKS workshop](https://catalog.workshops.aws/sagemaker-hyperpod-eks/en-US/00-setup/02-own-account) to set up the cluster.
2. Since Llama 3.2 is a gated model users have to register in Hugging Face and obtain an [access token](https://huggingface.co/docs/hub/en/security-tokens) before running this example. You will also need to review and accept the license agreement on the [meta-llama/Llama-3.2-1B](https://huggingface.co/meta-llama/Llama-3.2-1B) model page.
3. Configure your AWS credentials. If you haven't already set up your AWS credentials, you can do this by installing the AWS CLI and running `aws configure`. You'll need to enter your AWS Access Key ID, Secret Access Key, default region, and output format. 
   ```bash
   aws configure
   AWS Access Key ID [None]: YOUR_ACCESS_KEY
   AWS Secret Access Key [None]: YOUR_SECRET_KEY
   Default region name [None]: YOUR_REGION
   Default output format [None]: json
   ```

## 2. Continuous Pretraining on SageMaker Hyperpod

In this toturial, we will use the pre-built script from Optimum-neuron. The script uses the `Trainer` class from the Optimum Neuron library, which is a specialized version of the Hugging Face Trainer optimized for training on AWS Trainium instances. 

Here's an overview of the main components in the script:

1. **Model Loading**: The model is loaded using `AutoModelForCausalLM.from_pretrained()` with lazy loading for parallelism.

2. **Data Processing**: The dataset is tokenized and processed into chunks suitable for language modeling.

3. **Training Arguments**: The script uses `NeuronTrainingArguments` to configure training hyperparameters, including options for tensor parallelism and pipeline parallelism.

4. **Trainer Setup**: A `Trainer` instance is created with the model, training arguments, datasets, and other necessary components.

5. **Training Loop**: The `trainer.train()` method is called to start the continuous pretraining process.

To run the pretraining job on the SageMaker Hyperpod cluster, we need to package the training script and dependencies into a Docker image and deploy it to the cluster. The Optimum Neuron repository includes a `Dockerfile` and a `generate-jobspec.sh` script that you can use as a starting point.

1. First download the script for the EKS job:
   ```bash
   git clone https://github.com/huggingface/optimum-neuron.git
   mkdir ~/pre-training
   cd pre-training

   cp -r ../optimum-neuron/docs/source/training_tutorials/amazon_eks/ .
   ```

2. Build and push the Docker image to your ECR registry:

   ```
   export AWS_REGION=$(aws ec2 describe-availability-zones --output text --query 'AvailabilityZones[0].[RegionName]')
   export ACCOUNT=$(aws sts get-caller-identity --query Account --output text)
   export REGISTRY=${ACCOUNT}.dkr.ecr.${AWS_REGION}.amazonaws.com/
   export IMAGE=optimum-neuron-llama-pretraining
   export TAG=:latest

   docker build -t ${REGISTRY}${IMAGE}${TAG} .
   aws ecr get-login-password | docker login --username AWS --password-stdin ${REGISTRY}
   docker push ${REGISTRY}${IMAGE}${TAG}
   ```

3. Next, you will generate the script to be used by the pre-training job. Begin by logging into Hugging Face using your access token mentioned in the prerequisite steps.
   Modify the `generate-jobspec.sh` script to include the Hugging Face access token before running it:

   ```bash
   export HF_ACCESS_TOKEN="<your_HF_token_here>"
   ```

4. Generate the Kubernetes job specification:

   ```
   ./generate-jobspec.sh
   ```

4. Deploy the training job to your Kubernetes cluster:

   ```
   kubectl apply -f llama_train.yaml
   ```

   The training job will now start running on the SageMaker Hyperpod cluster. You can monitor the progress through Kubernetes logs:

   ```
   kubectl logs -f -n kubeflow -l app=llama-training-eks
   ```

   Once the pretraining is complete, you can fine-tune the model for specific tasks using the techniques covered in the previous tutorials.