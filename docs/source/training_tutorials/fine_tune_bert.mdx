<!---
Copyright 2024 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

# Fine-tune BERT for Text Classification

> Transform text classification with lightning-fast BERT training on AWS Trainium accelerators

_Complete example available: [fine_tune_bert.py](https://github.com/huggingface/optimum-neuron/blob/main/examples/training/bert/fine_tune_bert.py)_

This guide demonstrates fine-tuning BERT for emotion classification in **under 8 minutes** and **$0.20** on AWS Trainium using the [emotion dataset](https://huggingface.co/datasets/dair-ai/emotion).

## Prerequisites

- AWS Trainium instance (`trn1.2xlarge` recommended)
- [Hugging Face Account](https://huggingface.co/join) for model sharing
- Optimum Neuron installed with training dependencies:

```bash
python -m pip install .[training]
```

## Quick Start

Download and run the complete training script:

```bash
# Get the training script
wget https://raw.githubusercontent.com/huggingface/optimum-neuron/main/examples/training/bert/fine_tune_bert.py

# Launch distributed training on 2 Neuron cores
torchrun --nproc_per_node=2 fine_tune_bert.py
```

## What happens during training?

The script automatically:

1. **Loads the emotion dataset** - 16,000 English Twitter messages with 6 emotion labels
2. **Tokenizes text** - Converts text to BERT-compatible tokens (max length: 128)
3. **Fine-tunes BERT** - 3 epochs with optimized hyperparameters for Trainium
4. **Saves the model** - Ready-to-use emotion classifier

Key training configuration:

```python
# Essential training setup
from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments
from datasets import load_dataset

# Load emotion dataset and BERT model
dataset = load_dataset("dair-ai/emotion")
model = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-uncased", num_labels=6
)

# Optimized training arguments for Trainium
training_args = TrainingArguments(
    output_dir="bert-emotion-model",
    num_train_epochs=3,
    per_device_train_batch_size=8,
    learning_rate=5e-5,
    bf16=True,  # Faster training on Trainium
    eval_strategy="epoch",
)
```

## Results

**Training Performance:**
- ‚è±Ô∏è **Training time**: ~7.5 minutes (3 epochs)
- üí∞ **Cost**: ~$0.18 on `trn1.2xlarge`
- üìä **Final metrics**: 82.4% validation accuracy, 0.176 eval loss

**Sample output:**
```bash
üöÄ Starting BERT fine-tuning with model: bert-base-uncased
‚ö° Training started...
***** train metrics *****
  train_loss               =     0.2024
  train_runtime            = 0:07:27.14
  eval_loss                =     0.1761
‚úÖ Training completed! Model saved to bert-emotion-model
```

## Customization

Modify training parameters via command-line arguments:

```bash
# Custom model and training settings  
torchrun --nproc_per_node=2 fine_tune_bert.py \
  --model_id distilbert-base-uncased \
  --epochs 5 \
  --learning_rate 3e-5 \
  --batch_size 16 \
  --output_dir my-emotion-model
```

## Next Steps

üöÄ **Deploy your model**: Use [AWS Inferentia](https://huggingface.co/docs/optimum-neuron/guides/export_model) for cost-effective inference  
üì§ **Share on Hub**: Add `--repository_id your-username/model-name` to upload automatically  
üîß **Adapt to your data**: Replace the emotion dataset with your own text classification dataset

<Tip>

üí° **Remember**: Terminate your EC2 instance to avoid unnecessary charges!

</Tip>