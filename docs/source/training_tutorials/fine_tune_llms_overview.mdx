<!---
Copyright 2025 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

# ğŸš€  Tutorials: How To Fine-tune & Run LLMs

Learn how to run and fine-tune models for optimal performance with AWS Trainium.

<div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 16px; margin: 32px 0;">

<div style="border: 1px solid #e1e5e9; border-radius: 8px; overflow: hidden; background-color: white; box-shadow: 0 2px 4px rgba(0,0,0,0.1); transition: transform 0.2s ease-in-out;">
  <div style="height: 120px; background: linear-gradient(135deg, #FF6B35 0%, #F7931E 100%); display: flex; align-items: center; justify-content: center; position: relative;">
    <img src="../assets/training_tutorials/llama-logo.png" alt="Llama 3" style="width: 120px; height: 60px; object-fit: contain;" onerror="this.outerHTML='<div style=\'color: white; font-size: 48px; font-weight: bold;\'>ğŸ¦™</div>'"/>
  </div>
  <div style="padding: 16px;">
    <h3 style="margin: 0 0 6px 0; font-size: 16px; font-weight: 600; color: #24292e;">
      <a href="./sft_lora_finetune_llm" style="text-decoration: none; color: #0366d6;">Llama 3 8B</a>
    </h3>
    <p style="margin: 0; font-size: 12px; color: #586069; line-height: 1.3;">
      Fine-tune Llama 3 8B with LoRA and the SFTTrainer
    </p>
  </div>
</div>

<div style="border: 1px solid #e1e5e9; border-radius: 8px; overflow: hidden; background-color: white; box-shadow: 0 2px 4px rgba(0,0,0,0.1); transition: transform 0.2s ease-in-out;">
  <div style="height: 120px; background: linear-gradient(135deg, #4f46e5 0%, #7c3aed 100%); display: flex; align-items: center; justify-content: center; position: relative;">
    <img src="../assets/training_tutorials/qwen3-logo.png" alt="Qwen3" style="width: 120px; height: 60px; object-fit: contain;" onerror="this.outerHTML='<div style=\'color: white; font-size: 48px; font-weight: bold;\'>ğŸ”·</div>'"/>
  </div>
  <div style="padding: 16px;">
    <h3 style="margin: 0 0 6px 0; font-size: 16px; font-weight: 600; color: #24292e;">
      <a href="./finetune_qwen3" style="text-decoration: none; color: #0366d6;">Qwen3</a>
    </h3>
    <p style="margin: 0; font-size: 12px; color: #586069; line-height: 1.3;">
      Fine-tune Qwen 3 on AWS Trainium
    </p>
  </div>
</div>

<div style="border: 1px solid #e1e5e9; border-radius: 8px; overflow: hidden; background-color: white; box-shadow: 0 2px 4px rgba(0,0,0,0.1); transition: transform 0.2s ease-in-out;">
  <div style="height: 120px; background: linear-gradient(135deg, #059669 0%, #0891b2 100%); display: flex; align-items: center; justify-content: center; position: relative;">
    <img src="../assets/training_tutorials/sagemaker-logo.png" alt="SageMaker Hyperpod" style="width: 120px; height: 60px; object-fit: contain;" onerror="this.outerHTML='<div style=\'color: white; font-size: 28px; font-weight: bold; text-align: center;\'>â˜ï¸<br/>SageMaker</div>'"/>
  </div>
  <div style="padding: 16px;">
    <h3 style="margin: 0 0 6px 0; font-size: 16px; font-weight: 600; color: #24292e;">
      <a href="./pretraining_hyperpod_llm" style="text-decoration: none; color: #0366d6;">SageMaker Hyperpod</a>
    </h3>
    <p style="margin: 0; font-size: 12px; color: #586069; line-height: 1.3;">
      Continuous Pretraining of Llama 3.2 1B on SageMaker Hyperpod
    </p>
  </div>
</div>

</div>

## What you'll learn

These tutorials will guide you through the complete process of fine-tuning large language models on AWS Trainium:

- **ğŸ“Š Data Preparation**: Load and preprocess datasets for supervised fine-tuning
- **ğŸ”§ Model Configuration**: Set up LoRA adapters and distributed training parameters
- **âš¡ Training Optimization**: Leverage tensor parallelism, gradient checkpointing, and mixed precision
- **ğŸ’¾ Checkpoint Management**: Consolidate and merge model checkpoints for deployment
- **ğŸš€ Model Deployment**: Export and test your fine-tuned models for inference

Choose the tutorial that best fits your use case and start fine-tuning your LLMs on AWS Trainium today!
