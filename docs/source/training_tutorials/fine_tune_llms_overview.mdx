<!---
Copyright 2025 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

# ğŸš€ Tutorials: How To Fine-tune & Run LLMs

Learn how to run and fine-tune models for optimal performance 100% locally with AWS Trainium.

<div className="tutorial-grid">

<div className="tutorial-card">
  <div className="tutorial-header llama-header">
    <span className="tutorial-emoji">ğŸ¦™</span> Llama 3
  </div>
  <div className="tutorial-content">
    <h3><a href="/training_tutorials/sft_lora_finetune_llm">Llama 3 8B</a></h3>
    <p>Fine-tune Llama 3 8B with LoRA and the SFTTrainer</p>
  </div>
</div>

<div className="tutorial-card">
  <div className="tutorial-header qwen-header">
    <span className="tutorial-emoji">ğŸ”·</span> Qwen3
  </div>
  <div className="tutorial-content">
    <h3><a href="/training_tutorials/finetune_qwen3">Qwen3</a></h3>
    <p>Fine-tune Qwen 3 on AWS Trainium</p>
  </div>
</div>

<div className="tutorial-card">
  <div className="tutorial-header sagemaker-header">
    <span className="tutorial-emoji">â˜ï¸</span> SageMaker<br/>Hyperpod
  </div>
  <div className="tutorial-content">
    <h3><a href="/training_tutorials/pretraining_hyperpod_llm">SageMaker Hyperpod</a></h3>
    <p>Continuous Pretraining of Llama 3.2 1B on SageMaker Hyperpod</p>
  </div>
</div>

</div>

<style jsx>{`
  .tutorial-grid {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
    gap: 20px;
    margin: 40px 0;
  }

  .tutorial-card {
    border: 1px solid #e1e5e9;
    border-radius: 12px;
    overflow: hidden;
    background-color: white;
    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    transition: transform 0.2s ease-in-out, box-shadow 0.2s ease-in-out;
  }

  .tutorial-card:hover {
    transform: translateY(-2px);
    box-shadow: 0 4px 8px rgba(0,0,0,0.15);
  }

  .tutorial-header {
    height: 200px;
    background-color: #f8f9fa;
    display: flex;
    align-items: center;
    justify-content: center;
    font-size: 48px;
    font-weight: bold;
    text-align: center;
  }

  .llama-header {
    color: #007acc;
  }

  .llama-header .tutorial-emoji {
    color: #FF6B35;
  }

  .qwen-header {
    color: #4f46e5;
  }

  .qwen-header .tutorial-emoji {
    color: #4f46e5;
  }

  .sagemaker-header {
    color: #059669;
    font-size: 36px;
  }

  .sagemaker-header .tutorial-emoji {
    color: #f59e0b;
  }

  .tutorial-content {
    padding: 20px;
    border-top: 1px solid #e1e5e9;
  }

  .tutorial-content h3 {
    margin: 0 0 8px 0;
    font-size: 18px;
    font-weight: 600;
    color: #24292e;
  }

  .tutorial-content h3 a {
    text-decoration: none;
    color: #0366d6;
  }

  .tutorial-content h3 a:hover {
    text-decoration: underline;
  }

  .tutorial-content p {
    margin: 0;
    font-size: 14px;
    color: #586069;
    line-height: 1.4;
  }
`}</style>

## What you'll learn

These tutorials will guide you through the complete process of fine-tuning large language models on AWS Trainium:

- **ğŸ“Š Data Preparation**: Load and preprocess datasets for supervised fine-tuning
- **ğŸ”§ Model Configuration**: Set up LoRA adapters and distributed training parameters
- **âš¡ Training Optimization**: Leverage tensor parallelism, gradient checkpointing, and mixed precision
- **ğŸ’¾ Checkpoint Management**: Consolidate and merge model checkpoints for deployment
- **ğŸš€ Model Deployment**: Export and test your fine-tuned models for inference

Choose the tutorial that best fits your use case and start fine-tuning your LLMs on AWS Trainium today!