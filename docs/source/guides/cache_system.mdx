<!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Neuron Model Cache

## What You'll Learn

After reading this guide, you'll understand:
- How the Neuron model cache system works and its benefits
- The difference between training cache (Trainium) and inference cache (Inferentia)
- When and how to use public vs private cache repositories
- How to troubleshoot common caching issues

## Overview

The Neuron Model Cache is a remote caching system that stores compiled Neuron models to avoid lengthy recompilation times. Instead of compiling models from scratch every time (which can take up to an hour for large models), the cache allows you to download pre-compiled artifacts in seconds.

The cache system operates on two levels:
- **Local cache**: Fast, device-specific storage (usually `/var/tmp/neuron-compile-cache`)
- **HuggingFace Hub cache**: Shared, remote repository for team collaboration and public models

The system first checks your local cache, then falls back to the Hub cache if artifacts aren't found locally.

## Cache Types and Platform Support

Optimum Neuron uses different caching approaches depending on your use case:

| Cache Type | Platform | Use Case | Availability |
|------------|----------|----------|-------------|
| **Training Cache** | Trainium | Model training with `NeuronTrainer` | ✅ Public & Private repos |
| **Inference Cache** | Inferentia | Pre-compiled model lookup | ✅ Public repo only |
| **Traced Model Cache** | Both | Ahead-of-time compiled models | ✅ Public & Private repos |

**Note:** Currently, models exported using `NeuronModelForSequenceClassification`, `NeuronModelForQuestionAnswering`, and other task-specific classes use a different export mechanism and don't support the cache system. Only `NeuronModelForCausalLM` and `NeuronTrainer` integrate with the cache.

## How Neuron Compilation Works

Before diving into caching, it's important to understand the compilation process:

1. **Model Export**: When you train or load a model on Neuron platforms, it must be exported to Neuron format using [`torch-neuronx`](https://github.com/aws-neuron/aws-neuron-samples/tree/master/torch-neuronx)

2. **Graph Conversion**: The model is converted into [XLA](https://github.com/pytorch/xla/) subgraphs

3. **NEFF Compilation**: Each subgraph is compiled with the Neuron compiler into **NEFF** (Neuron Executable File Format) binary files

The graph conversion is relatively fast, but **NEFF compilation can take 30-60 minutes for large models**.

## The Caching Solution

To avoid recompiling NEFFs every time:

1. **Local Cache**: [`torch-neuronx`](https://github.com/aws-neuron/aws-neuron-samples/tree/master/torch-neuronx) stores NEFF files locally in `/var/tmp/neuron-compile-cache`
2. **Hub Cache**: Optimum Neuron extends this with a **shared remote cache** on the [Hugging Face Hub](https://huggingface.co/aws-neuron/optimum-neuron-cache)

The Hub cache solves the key limitation of local caching: **every new host or team member can benefit from previously compiled models**, rather than waiting hours for recompilation.

### Cache Priority
When loading a model, the system checks caches in this order:
1. **Local cache first** (fastest - immediate access)
2. **HuggingFace Hub cache** (fast - download in seconds)
3. **Recompile from scratch** (slow - up to 1 hour)

## Basic Usage

The cache system works automatically with supported model classes - no additional configuration needed!

### Training on Trainium
```python
from optimum.neuron import NeuronTrainer

# The trainer automatically uses the cache
trainer = NeuronTrainer(...)
trainer.train()  # Will download cached NEFFs if available
```

### Inference on Trainium/Inferentia
```python
from optimum.neuron import NeuronModelForCausalLM

# Model loading automatically uses the cache
model = NeuronModelForCausalLM.from_pretrained("model_id")
# Will download cached NEFFs if available, otherwise compile
```

### What Happens Behind the Scenes

1. **Cache Check**: During model export, Optimum Neuron generates a unique hash for your model configuration
2. **Local Lookup**: First checks if NEFFs exist in your local cache
3. **Hub Lookup**: If not found locally, searches the Hub cache repository
4. **Download or Compile**: Downloads cached NEFFs if found, otherwise compiles from scratch
5. **Cache Update**: Newly compiled NEFFs are stored locally and can be uploaded to your Hub cache

## Understanding Cache Types

Optimum Neuron provides different caching mechanisms depending on your platform and use case:

### Training Cache (Trainium)

Used during model training with `NeuronTrainer`. This cache:
- **Operates at runtime**: Caches compiled graphs as they're generated during training
- **Uses context managers**: Automatically patches the Neuron compiler to intercept and cache compilations
- **Supports private repos**: You can create and use private cache repositories
- **Uploads automatically**: New compilations are automatically uploaded to your cache repo during training

```python
from optimum.neuron import NeuronTrainer

# Training automatically uses and updates the cache
trainer = NeuronTrainer(model=model, args=training_args)
trainer.train()  # Downloads cached NEFFs if available, uploads new ones
```

### Inference Cache (Inferentia)

Used for loading pre-compiled models for inference. This cache:
- **Uses a registry system**: Maintains a searchable index of cached model configurations
- **Enables lookup**: You can search for compatible cached models before compiling
- **Read-only for users**: Only the Optimum team populates the public inference cache
- **Works with NeuronModelForCausalLM**: Automatically downloads cached models if available

### Traced Model Cache

Used for ahead-of-time (AOT) compiled models. This cache:
- **Manual upload**: You explicitly cache compiled artifacts after model tracing
- **Works on both platforms**: Supports both Trainium and Inferentia
- **Good for production**: Pre-compile models once, deploy everywhere

## How Cache Identification Works

The cache system is built on top of the [NeuronX compiler cache](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-features/neuron-caching.html) and operates on **NEFF binaries, not models directly**.

### Cache Key Generation

Each cached NEFF file is identified by a unique hash generated from:

**Model-specific factors:**
- Transformers model configuration (architecture, hidden sizes, etc.)
- Input shapes selected during export
- Model precision (fp32, fp16, bf16)
- Task type (text-generation, text-classification, etc.)

**Compilation-specific factors:**
- NeuronX compiler version
- Number of Neuron cores used
- Compilation flags and optimization level

### Important Cache Behavior

⚠️ **Key Limitations:**
- **Runtime identification**: Cache compatibility can only be determined during actual export
- **Configuration sensitivity**: Even small config changes create different NEFF files
- **Compiler dependency**: Different compiler versions produce different NEFFs

This means you cannot predict cache availability in advance - the system must attempt compilation to know if cached versions exist.

## Inference Cache Lookup (Inferentia)

For Inferentia users, the cache system includes a **registry** that lets you search for compatible pre-compiled models before starting compilation.

### How the Registry Works

The registry stores model configurations in a structured format:

```
neuronxcc-2.12.54.0+f631c2365/          # Compiler version
├── 0_REGISTRY/                          # Registry folder
│   └── 0.0.18/                         # Optimum-neuron version  
│       └── llama/                       # Model architecture
│           └── meta-llama/              # Model organization
│               └── Llama-2-7b-chat-hf/  # Model name
│                   └── 54c1f6689cd.json # Configuration hash
```

Each JSON file contains the exact compilation parameters used, allowing you to check compatibility before compilation.

### Using Cache Lookup

Search for cached configurations using the CLI:

```bash
optimum-cli neuron cache lookup meta-llama/Llama-2-7b-chat-hf
```

Example output:
```
*** 1 entries found in cache for meta-llama/Llama-2-7b-chat-hf ***

task: text-generation
batch_size: 1
num_cores: 24
auto_cast_type: fp16
sequence_length: 2048
compiler_type: neuronx-cc
compiler_version: 2.12.54.0+f631c2365
checkpoint_id: meta-llama/Llama-2-7b-chat-hf
checkpoint_revision: c1b0db933684edbfe29a06fa47eb19cc48025e93
```

⚠️ **Important**: Finding registry entries doesn't guarantee cache hits. If you've modified compilation parameters or updated NeuronX packages, the model may still need recompilation.

## Advanced Features

### Private Cache Repositories (Trainium)

The repository for the public cache is `aws-neuron/optimum-neuron-cache`. This repository includes all precompiled files for commonly used models so that it is publicly available and free to use for everyone. But there are two limitations:

1. You will not be able to push your own compiled files on this repo
2. It is public and you might want to use a private repo for private models

To alleviate that you can create your own private cache repository using the `optimum-cli` or set the environment variable `CUSTOM_CACHE_REPO`.

#### Using the Optimum CLI

The Optimum CLI offers 2 subcommands for cache creation and setting:

- `create`: To create a new cache repository that you can use as a private Neuron Model cache.
- `set`: To set the name of the Neuron cache repository locally, the repository needs to exist
and will be used by default by `optimum-neuron`.

Create a new Neuron cache repository:

```
optimum-cli neuron cache create --help

usage: optimum-cli neuron cache create [-h] [-n NAME] [--public]

optional arguments:
  -h, --help            show this help message and exit
  -n NAME, --name NAME  The name of the repo that will be used as a remote cache for the compilation files.
  --public              If set, the created repo will be public. By default the cache repo is private.

```

The `-n` / `--name` option allows you to specify a name for the Neuron cache repo, if not set the default name will be used. The `--public` flag allows you to make your Neuron cache public as it will be created as a private repository by default.

Example:

```
optimum-cli neuron cache create

Neuron cache created on the Hugging Face Hub: michaelbenayoun/optimum-neuron-cache [private].
Neuron cache name set locally to michaelbenayoun/optimum-neuron-cache in /home/michael/.cache/huggingface/optimum_neuron_custom_cache.
```

Set a different Trainium cache repository:

```
usage: optimum-cli neuron cache set [-h] name

positional arguments:
  name        The name of the repo to use as remote cache.

optional arguments:
  -h, --help  show this help message and exit
```

Example:

```
optimum-cli neuron cache set michaelbenayoun/optimum-neuron-cache

Neuron cache name set locally to michaelbenayoun/optimum-neuron-cache in /home/michael/.cache/huggingface/optimum_neuron_custom_cache
```

<Tip>

The `optimum-cli neuron cache set` command is useful when working on a new instance to use your own cache.

</Tip>

#### Using the environment variable `CUSTOM_CACHE_REPO`

Using the CLI is not always feasible, and not very practical for small testing. In this case, you can simply set the environment variable `CUSTOM_CACHE_REPO`.

For example, if your cache repo is called `michaelbenayoun/my_custom_cache_repo`, you just need to do:

```bash
CUSTOM_CACHE_REPO="michaelbenayoun/my_custom_cache_repo" torchrun ...
```

or:

```bash
export CUSTOM_CACHE_REPO="michaelbenayoun/my_custom_cache_repo"
torchrun ...
```

You have to be [logged into the Hugging Face Hub](https://huggingface.co/docs/huggingface_hub/quick-start#login) to be able to push  and pull files from your private cache repository.

### Training Cache Workflow

<p align="center">
  <img alt="Cache system flow" src="https://huggingface.co/datasets/optimum/documentation-images/resolve/main/neuron/cache_system_flow.jpg">
  <br>
  <em style="color: grey">Training cache system flow</em>
</p>

**During Training:**

1. **Hash Generation**: For each model subgraph, `NeuronTrainer` computes a unique hash
2. **Cache Check**: Searches both official and custom cache repositories on the Hub
3. **Download or Compile**: 
   - If cached: Downloads NEFF files directly to local cache
   - If not cached: Compiles from scratch and stores locally
4. **Upload**: Newly compiled NEFFs are uploaded to your cache repo (if you have write access)

**Key Benefits:**
- **Team sharing**: Team members automatically benefit from each other's compilations
- **Iterative training**: Resume training jobs without recompilation
- **CI/CD friendly**: Automated workflows can leverage cached compilations


## Optimum CLI

The Optimum CLI can be used to perform various cache-related tasks, as described by the `optimum-cli neuron cache` command usage message:

```
usage: optimum-cli neuron cache [-h] {create,set,add,list} ...

positional arguments:
  {create,set,add,list,synchronize,lookup}
    create              Create a model repo on the Hugging Face Hub to store Neuron X compilation files.
    set                 Set the name of the Neuron cache repo to use locally (trainium only).
    add                 Add a model to the cache of your choice (trainium only).
    list                List models in a cache repo (trainium only).
    synchronize         Synchronize local compiler cache with the hub cache (inferentia only).
    lookup              Lookup the neuronx compiler hub cache for the specified model id (inferentia only).

optional arguments:
  -h, --help            show this help message and exit
```

### Adding Models to Cache (Trainium)

It is possible to add a model compilation files to a cache repo via the `optimum-cli neuron cache add` command:

```
usage: optimum-cli neuron cache add [-h] -m MODEL --task TASK --train_batch_size TRAIN_BATCH_SIZE [--eval_batch_size EVAL_BATCH_SIZE] [--sequence_length SEQUENCE_LENGTH]
                                    [--encoder_sequence_length ENCODER_SEQUENCE_LENGTH] [--decoder_sequence_length DECODER_SEQUENCE_LENGTH]
                                    [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS] --precision {fp,bf16} --num_cores
                                    {1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32} [--max_steps MAX_STEPS]
```

When running this command a small training session will be run and the resulting compilation files will be pushed.

<Tip warning={true}>
Make sure that the Neuron cache repo to use is set up locally, this can be done by running the `optimum-cli neuron cache set` command.
You also need to make sure that you are logged in to the Hugging Face Hub and that you have the writing rights for the specified cache repo,
this can be done via the `huggingface-cli login` command.

If at least one of those requirements is not met, the command will fail.
</Tip>


Example:

```
optimum-cli neuron cache add \
  --model prajjwal1/bert-tiny \
  --task text-classification \
  --train_batch_size 16 \
  --eval_batch_size 16 \
  --sequence_length 128 \
  --gradient_accumulation_steps 32 \
  --num_cores 32 \
  --precision bf16
```

This will push compilation files for the `prajjwal1/bert-tiny` model on the Neuron cache repo that was set up for the specified parameters.

### Listing Cache Contents

It can also be convenient to request the cache repo to know which compilation files are available. This can be done via the `optimum-cli neuron cache list` command:

```
usage: optimum-cli neuron cache list [-h] [-m MODEL] [-v VERSION] [name]

positional arguments:
  name                  The name of the repo to list. Will use the locally saved cache repo if left unspecified.

optional arguments:
  -h, --help            show this help message and exit
  -m MODEL, --model MODEL
                        The model name or path of the model to consider. If left unspecified, will list all available models.
  -v VERSION, --version VERSION
                        The version of the Neuron X Compiler to consider. Will list all available versions if left unspecified.
```

As you can see, it is possible to:

- List all the models available for all compiler versions.
- List all the models available for a given compiler version by specifying the `-v / --version` argument.
- List all compilation files for a given model, there can be many for different input shapes and so on, by specifying the `-m / --model` argument.


Example:

```
optimum-cli neuron cache list aws-neuron/optimum-neuron-cache
```
