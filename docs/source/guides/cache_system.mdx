<!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Neuron Model Cache

## Why Use the Cache?

**Problem**: Neuron compilation takes 30-60 minutes for large models  
**Solution**: Download pre-compiled models in seconds

The cache system stores compiled Neuron models on HuggingFace Hub, eliminating recompilation time for your team.

## Quick Start

### Training (Trainium)
```python
from optimum.neuron import NeuronTrainer

# Cache works automatically - no configuration needed
trainer = NeuronTrainer(model=model, args=training_args)
trainer.train()  # Downloads cached models if available
```

### Inference (Inferentia/Trainium)
```python
from optimum.neuron import NeuronModelForCausalLM

# Cache works automatically 
model = NeuronModelForCausalLM.from_pretrained("model_id")
```

That's it! The cache works automatically for supported model classes.

## How It Works

**Cache Priority** (fastest to slowest):
1. **Local cache** → Instant access from `/var/tmp/neuron-compile-cache`
2. **Hub cache** → Download in seconds from HuggingFace
3. **Compile** → 30-60 minutes for large models

**Cache Key**: Each compiled model gets a unique hash based on:
- Model configuration (architecture, precision, input shapes)
- Neuron compiler version and settings
- Number of cores used

## Supported Models

| Model Class | Cache Support | Notes |
|-------------|---------------|-------|
| `NeuronTrainer` | ✅ Full | Auto download + upload |
| `NeuronModelForCausalLM` | ✅ Full | Auto download |
| Other `NeuronModelForXXX` | ❌ None | Use different export system |

## Private Cache Setup

**Why private cache?**
- Store your team's compiled models
- Keep private models secure
- Upload your own compilations

### Method 1: CLI Setup (Recommended)

```bash
# Create private cache repository
optimum-cli neuron cache create

# Set as default cache
optimum-cli neuron cache set your-org/your-cache-name
```

### Method 2: Environment Variable

```bash
# Use for single training run
CUSTOM_CACHE_REPO="your-org/your-cache" python train.py

# Or export for session
export CUSTOM_CACHE_REPO="your-org/your-cache"
```

**Prerequisites:**
- Login: `huggingface-cli login`
- Write access to cache repository

## CLI Commands

```bash
# Create new cache repository
optimum-cli neuron cache create [-n NAME] [--public]

# Set default cache repository  
optimum-cli neuron cache set REPO_NAME

# Search for cached models (Inferentia)
optimum-cli neuron cache lookup MODEL_ID

# Sync local cache with Hub
optimum-cli neuron cache synchronize
```

## Advanced Usage

### Manual Cache Control
```python
from optimum.neuron.cache import hub_neuronx_cache
from optimum.neuron.cache.entries import SingleModelCacheEntry

# Create cache entry
cache_entry = SingleModelCacheEntry(model_id, task, config, neuron_config)

# Use specific cache repository
with hub_neuronx_cache(entry=cache_entry, cache_repo_id="my-org/cache"):
    model = load_model()  # Will use specified cache
```

### Cache Lookup (Inferentia)
```bash
optimum-cli neuron cache lookup meta-llama/Llama-2-7b-chat-hf
```

Example output:
```
*** 1 entries found in cache ***
task: text-generation
batch_size: 1, sequence_length: 2048
num_cores: 24, precision: fp16
compiler_version: 2.12.54.0
```

## Troubleshooting

### "Cache repository does not exist"
```
Fix: Check repository name and login status
→ huggingface-cli login
→ Verify repo format: org/repo-name
```

### "Graph will be recompiled" 
```
Cause: No cached model matches your exact configuration
Fix: Use lookup to find compatible configurations
→ optimum-cli neuron cache lookup MODEL_ID
```

### Cache not uploading during training
```
Cause: No write permissions to cache repository  
Fix: Verify access and authentication
→ huggingface-cli whoami
→ Check cache repo permissions
```

### Slow downloads
```
Cause: Large compiled models (GBs) downloading
Fix: Ensure good internet connection
→ Monitor logs for download progress
```

### Clear corrupted local cache
```bash
rm -rf /var/tmp/neuron-compile-cache/*
```

## Cache Types

### Training Cache (Trainium)
- **Runtime caching**: Compiled graphs cached during training
- **Private repos**: Supports team-specific caches

### Inference Cache (Inferentia) 
- **Pre-compiled lookup**: Search registry before compilation
- **Public only**: Read-only access to shared cache
- **Registry search**: Find compatible model configurations

### Traced Model Cache (Both platforms)
- **Manual upload**: Explicitly cache after model tracing  
- **AOT compilation**: Pre-compile for production deployment
- **Cross-platform**: Works on Trainium and Inferentia

## Best Practices

✅ **Use consistent configurations** across your team  
✅ **Create private cache** for your organization  
✅ **Check cache hits** in logs ("Fetched cached" messages)  
✅ **Standardize parameters** (batch size, sequence length, precision)

❌ **Don't** frequently change compilation parameters  
❌ **Don't** skip authentication setup  
❌ **Don't** ignore "recompiling" warnings
