<!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Neuron Model Cache

## What You'll Learn

After reading this guide, you'll understand:
- How the Neuron model cache system works and its benefits
- The difference between training cache (Trainium) and inference cache (Inferentia)
- When and how to use public vs private cache repositories
- How to troubleshoot common caching issues

## Overview

The Neuron Model Cache is a remote caching system that stores compiled Neuron models to avoid lengthy recompilation times. Instead of compiling models from scratch every time (which can take up to an hour for large models), the cache allows you to download pre-compiled artifacts in seconds.

The cache system operates on two levels:
- **Local cache**: Fast, device-specific storage (usually `/var/tmp/neuron-compile-cache`)
- **HuggingFace Hub cache**: Shared, remote repository for team collaboration and public models

The system first checks your local cache, then falls back to the Hub cache if artifacts aren't found locally.

## Cache Types and Platform Support

Optimum Neuron uses different caching approaches depending on your use case:

| Cache Type | Platform | Use Case | Availability |
|------------|----------|----------|-------------|
| **Training Cache** | Trainium | Model training with `NeuronTrainer` | ✅ Public & Private repos |
| **Inference Cache** | Inferentia | Pre-compiled model lookup | ✅ Public repo only |
| **Traced Model Cache** | Both | Ahead-of-time compiled models | ✅ Public & Private repos |

**Note:** Currently, models exported using `NeuronModelForSequenceClassification`, `NeuronModelForQuestionAnswering`, and other task-specific classes use a different export mechanism and don't support the cache system. Only `NeuronModelForCausalLM` and `NeuronTrainer` integrate with the cache.

## How Neuron Compilation Works

Before diving into caching, it's important to understand the compilation process:

1. **Model Export**: When you train or load a model on Neuron platforms, it must be exported to Neuron format using [`torch-neuronx`](https://github.com/aws-neuron/aws-neuron-samples/tree/master/torch-neuronx)

2. **Graph Conversion**: The model is converted into [XLA](https://github.com/pytorch/xla/) subgraphs

3. **NEFF Compilation**: Each subgraph is compiled with the Neuron compiler into **NEFF** (Neuron Executable File Format) binary files

The graph conversion is relatively fast, but **NEFF compilation can take 30-60 minutes for large models**.

## The Caching Solution

To avoid recompiling NEFFs every time:

1. **Local Cache**: [`torch-neuronx`](https://github.com/aws-neuron/aws-neuron-samples/tree/master/torch-neuronx) stores NEFF files locally in `/var/tmp/neuron-compile-cache`
2. **Hub Cache**: Optimum Neuron extends this with a **shared remote cache** on the [Hugging Face Hub](https://huggingface.co/aws-neuron/optimum-neuron-cache)

The Hub cache solves the key limitation of local caching: **every new host or team member can benefit from previously compiled models**, rather than waiting hours for recompilation.

### Cache Priority
When loading a model, the system checks caches in this order:
1. **Local cache first** (fastest - immediate access)
2. **HuggingFace Hub cache** (fast - download in seconds)
3. **Recompile from scratch** (slow - up to 1 hour)

## Basic Usage

The cache system works automatically with supported model classes - no additional configuration needed!

### Training on Trainium
```python
from optimum.neuron import NeuronTrainer

# The trainer automatically uses the cache
trainer = NeuronTrainer(...)
trainer.train()  # Will download cached NEFFs if available
```

### Inference on Trainium/Inferentia
```python
from optimum.neuron import NeuronModelForCausalLM

# Model loading automatically uses the cache
model = NeuronModelForCausalLM.from_pretrained("model_id")
# Will download cached NEFFs if available, otherwise compile
```

### What Happens Behind the Scenes

1. **Cache Check**: During model export, Optimum Neuron generates a unique hash for your model configuration
2. **Local Lookup**: First checks if NEFFs exist in your local cache
3. **Hub Lookup**: If not found locally, searches the Hub cache repository
4. **Download or Compile**: Downloads cached NEFFs if found, otherwise compiles from scratch
5. **Cache Update**: Newly compiled NEFFs are stored locally and can be uploaded to your Hub cache

## Understanding Cache Types

Optimum Neuron provides different caching mechanisms depending on your platform and use case:

### Training Cache (Trainium)

Used during model training with `NeuronTrainer`. This cache:
- **Operates at runtime**: Caches compiled graphs as they're generated during training
- **Uses context managers**: Automatically patches the Neuron compiler to intercept and cache compilations
- **Supports private repos**: You can create and use private cache repositories
- **Uploads automatically**: New compilations are automatically uploaded to your cache repo during training

```python
from optimum.neuron import NeuronTrainer

# Training automatically uses and updates the cache
trainer = NeuronTrainer(model=model, args=training_args)
trainer.train()  # Downloads cached NEFFs if available, uploads new ones
```

### Inference Cache (Inferentia)

Used for loading pre-compiled models for inference. This cache:
- **Uses a registry system**: Maintains a searchable index of cached model configurations
- **Enables lookup**: You can search for compatible cached models before compiling
- **Read-only for users**: Only the Optimum team populates the public inference cache
- **Works with NeuronModelForCausalLM**: Automatically downloads cached models if available

### Traced Model Cache

Used for ahead-of-time (AOT) compiled models. This cache:
- **Manual upload**: You explicitly cache compiled artifacts after model tracing
- **Works on both platforms**: Supports both Trainium and Inferentia
- **Good for production**: Pre-compile models once, deploy everywhere

## How Cache Identification Works

The cache system is built on top of the [NeuronX compiler cache](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-features/neuron-caching.html) and operates on **NEFF binaries, not models directly**.

### Cache Key Generation

Each cached NEFF file is identified by a unique hash generated from:

**Model-specific factors:**
- Transformers model configuration (architecture, hidden sizes, etc.)
- Input shapes selected during export
- Model precision (fp32, fp16, bf16)
- Task type (text-generation, text-classification, etc.)

**Compilation-specific factors:**
- NeuronX compiler version
- Number of Neuron cores used
- Compilation flags and optimization level

### Important Cache Behavior

⚠️ **Key Limitations:**
- **Runtime identification**: Cache compatibility can only be determined during actual export
- **Configuration sensitivity**: Even small config changes create different NEFF files
- **Compiler dependency**: Different compiler versions produce different NEFFs

This means you cannot predict cache availability in advance - the system must attempt compilation to know if cached versions exist.

## Inference Cache Lookup (Inferentia)

For Inferentia users, the cache system includes a **registry** that lets you search for compatible pre-compiled models before starting compilation.

### How the Registry Works

The registry stores model configurations in a structured format:

```
neuronxcc-2.12.54.0+f631c2365/          # Compiler version
├── 0_REGISTRY/                          # Registry folder
│   └── 0.0.18/                         # Optimum-neuron version  
│       └── llama/                       # Model architecture
│           └── meta-llama/              # Model organization
│               └── Llama-2-7b-chat-hf/  # Model name
│                   └── 54c1f6689cd.json # Configuration hash
```

Each JSON file contains the exact compilation parameters used, allowing you to check compatibility before compilation.

### Using Cache Lookup

Search for cached configurations using the CLI:

```bash
optimum-cli neuron cache lookup meta-llama/Llama-2-7b-chat-hf
```

Example output:
```
*** 1 entries found in cache for meta-llama/Llama-2-7b-chat-hf ***

task: text-generation
batch_size: 1
num_cores: 24
auto_cast_type: fp16
sequence_length: 2048
compiler_type: neuronx-cc
compiler_version: 2.12.54.0+f631c2365
checkpoint_id: meta-llama/Llama-2-7b-chat-hf
checkpoint_revision: c1b0db933684edbfe29a06fa47eb19cc48025e93
```

⚠️ **Important**: Finding registry entries doesn't guarantee cache hits. If you've modified compilation parameters or updated NeuronX packages, the model may still need recompilation.

## Advanced Features

### Private Cache Repositories (Trainium)

The repository for the public cache is `aws-neuron/optimum-neuron-cache`. This repository includes all precompiled files for commonly used models so that it is publicly available and free to use for everyone. But there are two limitations:

1. You will not be able to push your own compiled files on this repo
2. It is public and you might want to use a private repo for private models

To alleviate that you can create your own private cache repository using the `optimum-cli` or set the environment variable `CUSTOM_CACHE_REPO`.

#### Using the Optimum CLI

The Optimum CLI offers 2 subcommands for cache creation and setting:

- `create`: To create a new cache repository that you can use as a private Neuron Model cache.
- `set`: To set the name of the Neuron cache repository locally, the repository needs to exist
and will be used by default by `optimum-neuron`.

Create a new Neuron cache repository:

```
optimum-cli neuron cache create --help

usage: optimum-cli neuron cache create [-h] [-n NAME] [--public]

optional arguments:
  -h, --help            show this help message and exit
  -n NAME, --name NAME  The name of the repo that will be used as a remote cache for the compilation files.
  --public              If set, the created repo will be public. By default the cache repo is private.

```

The `-n` / `--name` option allows you to specify a name for the Neuron cache repo, if not set the default name will be used. The `--public` flag allows you to make your Neuron cache public as it will be created as a private repository by default.

Example:

```
optimum-cli neuron cache create

Neuron cache created on the Hugging Face Hub: michaelbenayoun/optimum-neuron-cache [private].
Neuron cache name set locally to michaelbenayoun/optimum-neuron-cache in /home/michael/.cache/huggingface/optimum_neuron_custom_cache.
```

Set a different Trainium cache repository:

```
usage: optimum-cli neuron cache set [-h] name

positional arguments:
  name        The name of the repo to use as remote cache.

optional arguments:
  -h, --help  show this help message and exit
```

Example:

```
optimum-cli neuron cache set michaelbenayoun/optimum-neuron-cache

Neuron cache name set locally to michaelbenayoun/optimum-neuron-cache in /home/michael/.cache/huggingface/optimum_neuron_custom_cache
```

<Tip>

The `optimum-cli neuron cache set` command is useful when working on a new instance to use your own cache.

</Tip>

#### Using the environment variable `CUSTOM_CACHE_REPO`

Using the CLI is not always feasible, and not very practical for small testing. In this case, you can simply set the environment variable `CUSTOM_CACHE_REPO`.

For example, if your cache repo is called `michaelbenayoun/my_custom_cache_repo`, you just need to do:

```bash
CUSTOM_CACHE_REPO="michaelbenayoun/my_custom_cache_repo" torchrun ...
```

or:

```bash
export CUSTOM_CACHE_REPO="michaelbenayoun/my_custom_cache_repo"
torchrun ...
```

You have to be [logged into the Hugging Face Hub](https://huggingface.co/docs/huggingface_hub/quick-start#login) to be able to push  and pull files from your private cache repository.

### Training Cache Workflow

<p align="center">
  <img alt="Cache system flow" src="https://huggingface.co/datasets/optimum/documentation-images/resolve/main/neuron/cache_system_flow.jpg">
  <br>
  <em style="color: grey">Training cache system flow</em>
</p>

**During Training:**

1. **Hash Generation**: For each model subgraph, `NeuronTrainer` computes a unique hash
2. **Cache Check**: Searches both official and custom cache repositories on the Hub
3. **Download or Compile**: 
   - If cached: Downloads NEFF files directly to local cache
   - If not cached: Compiles from scratch and stores locally
4. **Upload**: Newly compiled NEFFs are uploaded to your cache repo (if you have write access)

**Key Benefits:**
- **Team sharing**: Team members automatically benefit from each other's compilations
- **Iterative training**: Resume training jobs without recompilation
- **CI/CD friendly**: Automated workflows can leverage cached compilations


## CLI Reference

The Optimum CLI provides comprehensive cache management capabilities:

```bash
optimum-cli neuron cache --help
```

### Available Commands

| Command | Platform | Description |
|---------|----------|--------------|
| `create` | Both | Create a new cache repository on HuggingFace Hub |
| `set` | Trainium | Set the default cache repository to use locally |
| `add` | Trainium | Add a model's compilation files to cache |
| `list` | Trainium | List cached models in a repository |
| `synchronize` | Inferentia | Sync local cache with Hub cache |
| `lookup` | Inferentia | Search for compatible cached model configurations |

### Command Details

**Create Cache Repository:**
```bash
optimum-cli neuron cache create [-n NAME] [--public]
```

**Set Default Cache (Trainium):**
```bash
optimum-cli neuron cache set REPO_NAME
```

**Add Model to Cache (Trainium):**
```bash
optimum-cli neuron cache add -m MODEL --task TASK [options]
```

**List Cache Contents:**
```bash
optimum-cli neuron cache list [REPO_NAME] [-m MODEL] [-v VERSION]
```

**Synchronize Cache (Inferentia):**
```bash
optimum-cli neuron cache synchronize [--cache-dir PATH] [--repo-id REPO]
```

**Lookup Models (Inferentia):**
```bash
optimum-cli neuron cache lookup MODEL_ID
```

### Adding Models to Cache (Trainium)

It is possible to add a model compilation files to a cache repo via the `optimum-cli neuron cache add` command:

```
usage: optimum-cli neuron cache add [-h] -m MODEL --task TASK --train_batch_size TRAIN_BATCH_SIZE [--eval_batch_size EVAL_BATCH_SIZE] [--sequence_length SEQUENCE_LENGTH]
                                    [--encoder_sequence_length ENCODER_SEQUENCE_LENGTH] [--decoder_sequence_length DECODER_SEQUENCE_LENGTH]
                                    [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS] --precision {fp,bf16} --num_cores
                                    {1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32} [--max_steps MAX_STEPS]
```

When running this command a small training session will be run and the resulting compilation files will be pushed.

<Tip warning={true}>
Make sure that the Neuron cache repo to use is set up locally, this can be done by running the `optimum-cli neuron cache set` command.
You also need to make sure that you are logged in to the Hugging Face Hub and that you have the writing rights for the specified cache repo,
this can be done via the `huggingface-cli login` command.

If at least one of those requirements is not met, the command will fail.
</Tip>


**Example - Adding a Model to Cache:**

```bash
optimum-cli neuron cache add \
  --model prajjwal1/bert-tiny \
  --task text-classification \
  --train_batch_size 16 \
  --eval_batch_size 16 \
  --sequence_length 128 \
  --gradient_accumulation_steps 32 \
  --num_cores 32 \
  --precision bf16
```

This will:
1. Run a short training session with the specified parameters
2. Compile the model for your configuration  
3. Upload the resulting NEFF files to your cache repository

**Prerequisites:**
- Cache repository set up locally (`optimum-cli neuron cache set`)
- HuggingFace authentication (`huggingface-cli login`)
- Write access to the specified cache repository

## Feature Comparison by Platform

This table summarizes all cache features and their platform availability:

| Feature | Trainium | Inferentia | Description |
|---------|----------|------------|-------------|
| **Automatic cache usage** | ✅ | ✅ | Downloads cached NEFFs automatically during model loading |
| **Automatic cache upload** | ✅ | ❌ | Uploads new compilations automatically during training |
| **Private cache repositories** | ✅ | ❌ | Create and use team-specific private cache repos |
| **Registry lookup** | ❌ | ✅ | Search for compatible cached configurations before compilation |
| **Manual cache synchronization** | ✅ | ✅ | Upload local cache to Hub using CLI |
| **CLI cache management** | ✅ | ✅ | Create, list, and manage cache repositories |
| **Context manager caching** | ✅ | ❌ | Runtime cache integration during training |
| **Traced model caching** | ✅ | ✅ | Cache ahead-of-time compiled models |

### Supported Model Classes

| Model Class | Cache Support | Platform | Notes |
|-------------|---------------|----------|-------|
| `NeuronTrainer` | ✅ Full | Trainium | Automatic cache download and upload |
| `NeuronModelForCausalLM` | ✅ Full | Both | Automatic cache download |
| Other `NeuronModelForXXX` | ❌ None | Both | Use different export mechanism |
| Custom traced models | ✅ Manual | Both | Requires explicit cache operations |

### Listing Cache Contents

It can also be convenient to request the cache repo to know which compilation files are available. This can be done via the `optimum-cli neuron cache list` command:

```
usage: optimum-cli neuron cache list [-h] [-m MODEL] [-v VERSION] [name]

positional arguments:
  name                  The name of the repo to list. Will use the locally saved cache repo if left unspecified.

optional arguments:
  -h, --help            show this help message and exit
  -m MODEL, --model MODEL
                        The model name or path of the model to consider. If left unspecified, will list all available models.
  -v VERSION, --version VERSION
                        The version of the Neuron X Compiler to consider. Will list all available versions if left unspecified.
```

As you can see, it is possible to:

- List all the models available for all compiler versions.
- List all the models available for a given compiler version by specifying the `-v / --version` argument.
- List all compilation files for a given model, there can be many for different input shapes and so on, by specifying the `-m / --model` argument.


Example:

```
optimum-cli neuron cache list aws-neuron/optimum-neuron-cache
```

### Quick Start Examples

**For Trainium users (Training):**
```bash
# Set up private cache (one-time setup)
optimum-cli neuron cache create --name my-team-cache

# Train with automatic caching
python train.py  # Uses NeuronTrainer - cache works automatically
```

**For Inferentia users (Inference):**
```bash
# Check what's available before loading
optimum-cli neuron cache lookup microsoft/DialoGPT-medium

# Load model (will use cache if available)
python inference.py  # Uses NeuronModelForCausalLM
```

**For manual cache operations:**
```bash
# Sync local cache to Hub
optimum-cli neuron cache synchronize

# Add specific model configuration
optimum-cli neuron cache add --model MODEL_NAME --task text-generation
```

## Troubleshooting

### Common Issues and Solutions

#### "Cache repository does not exist or you don't have access to it"
**Cause**: Invalid repository name or insufficient permissions  
**Solution**: 
- Verify the repository name format: `org/repo-name`
- Ensure you're logged in: `huggingface-cli login`
- Check repository permissions if using a private cache

#### "Graph will be recompiled" warnings
**Cause**: No cached NEFF files match your exact configuration  
**Why this happens**:
- Different model configurations (batch size, sequence length, precision)
- Updated NeuronX compiler version
- Modified compilation flags

**Solutions**:
- Use `optimum-cli neuron cache lookup` to find compatible configurations
- Check if your configuration matches available cached entries
- Consider adjusting parameters to match cached configurations

#### Slow cache downloads
**Cause**: Large NEFF files being downloaded from Hub  
**Solutions**:
- Ensure good internet connectivity
- Consider using a private cache closer to your infrastructure
- Monitor download progress in logs

#### Cache not updating during training
**Cause**: No write permissions to cache repository  
**Solutions**:
- Verify you have write access to the cache repo
- Check authentication: `huggingface-cli whoami`
- Ensure the cache repo exists and is accessible

#### Local cache corruption
**Cause**: Interrupted compilation or disk issues  
**Solution**: Clear local cache and retry
```bash
rm -rf /var/tmp/neuron-compile-cache/*
```

### Cache Performance Tips

1. **Use consistent configurations**: Standardize batch sizes, sequence lengths, and precision across your team
2. **Monitor cache hits**: Check logs for "Fetched cached" messages to verify cache usage
3. **Batch compilation**: Compile multiple model variants together to populate cache efficiently
4. **Private cache for teams**: Create team-specific cache repositories for better collaboration
