<!---
Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

# Neuron Model Inference

*The APIs presented in the following documentation are relevant for the inference on [INF2](https://aws.amazon.com/ec2/instance-types/inf2/), 
[TRN1](https://aws.amazon.com/ec2/instance-types/trn1/) and [INF1](https://aws.amazon.com/ec2/instance-types/inf1/).*

[ðŸ¤— Optimum-neuron](https://github.com/huggingface/optimum-neuron) is a utility package to ease the use of accelerators like AWS Inferentia2, 
Trainium1 and Inferentia1. With Neuron models APIs, you will be able to run inference on neuron devices with the same experience as 
[ðŸ¤— Transformers](https://huggingface.co/docs/transformers/index).

`NeuronModelForXXX` classes help to load models from the [Hugging Face Hub](hf.co/models) and compile them to a serialized format optimized for 
neuron devices. You will then be able to load the model and run inference with the acceleration powered by AWS Neuron devices.

## Switching from Transformers to Optimum

The `optimum.neuron.NeuronModelForXXX` model classes are APIs compatible with Hugging Face Transformers models. This means seamless integration 
with Hugging Face's ecosystem. You can just replace your `AutoModelForXXX` class with the corresponding `NeuronModelForXXX` class in `optimum.neuron`.

If you already use Transformers, you will be able to reuse your code just by replacing model classes:

```diff
from transformers import AutoTokenizer
-from transformers import AutoModelForSequenceClassification
+from optimum.neuron import NeuronModelForSequenceClassification

# PyTorch checkpoint
-model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english") 

# Compile your model during the first time
+input_shapes = {"batch_size": 1, "sequence_length": 64}
+model = NeuronModelForSequenceClassification.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english", export=True, **input_shapes) 

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
inputs = tokenizer("Hamilton is considered to be the best musical of human history.", return_tensors="pt")

logits = model(**inputs).logits
predicted_class_id = logits.argmax().item()
model.config.id2label[predicted_class_id]

# 'POSITIVE'
```

As shown above, when you use `NeuronModelForXXX` for the first time, you will need to set `export=True` to compile your model from PyTorch to a neuron-compatible format.

`input_shapes` are mandatory static shape information that you need to send to the neuron compiler. Wondering what shapes are mandatory for your model? Check it out 
with the following code:

```python
>>> from transformers import AutoModelForSequenceClassification
>>> from optimum.exporters import TasksManager

>>> model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")

# Infer the task name if you don't know
>>> task = TasksManager.infer_task_from_model(model)  # 'text-classification'

>>> neuron_config_constructor = TasksManager.get_exporter_config_constructor(model=model, exporter="neuron", task='text-classification')
>>> print(neuron_config_constructor.func.get_mandatory_axes_for_task(task))

# ('batch_size', 'sequence_length') 
```

<Tip>

Be careful, __the input shapes used for compilation should be inferior than the size of inputs that you will feed into the model during the inference.__ 

</Tip>

<Tip>

__What if real input sizes are smaller than compilation input shapes?__

No worries, `NeuronModelForXXX` class will pad your inputs to an eligible shape. And you can set `dynamic_batch_size=True` in the `from_pretrained`
method to enable dynamic batching, which means that your inputs can have variable batch size.

*Just one thing to keep in mind: dynamicity and padding comes with not only flexibility but also performance drop. Fair enough!*

</Tip>

Once your model is compiled, you can save it either on your local or in the [Hugging Face Model Hub](https://hf.co/models):

```python
>>> from optimum.neuron import NeuronModelForSequenceClassification

>>> # Load the model from the hub and export it to the Neuron optimized format
>>> input_shapes = {"batch_size": 1, "sequence_length": 64}
>>> model = NeuronModelForSequenceClassification.from_pretrained(
...     "distilbert-base-uncased-finetuned-sst-2-english", export=True, **input_shapes
... )

>>> # Save the compiled model
>>> model.save_pretrained("a_local_path_for_compiled_neuron_model")

# Push the onnx model to HF Hub
>>> model.push_to_hub(  # doctest: +SKIP
...   "a_local_path_for_compiled_neuron_model", repository_id="my-neuron-repo", use_auth_token=True
... )
```

And the next time when you want to run inference, just load your compiled model which will save you the compilation time:

```python
>>> from optimum.neuron import NeuronModelForSequenceClassification
>>> model = NeuronModelForSequenceClassification.from_pretrained("my-neuron-repo")
```

As you see, there is no need to precise shape information and compilation arguments used during the compilation as they are 
saved in a `config.json` file, and will be restored automatically by `NeuronModelForXXX` class.

Happy inference with Neuron! ðŸš€
