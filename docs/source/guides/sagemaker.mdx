<!---
Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

# Using Optimum Neuron on Amazon SageMaker

[Optimum Neuron](https://github.com/huggingface/optimum-neuron) is integrated into Amazon SageMaker through the Hugging Face Deep Learning Containers for AWS Accelerators like Inferentia2 and Trainium1. This allows you to easily train and deploy ðŸ¤— Transformers and Diffusers models on Amazon SageMaker leveraging AWS accelerators.

The Hugging Face DLC images come with pre-installed Optimum Neuron and tools to compile models for efficient inference on Inferentia2 and Trainium1. This makes deploying large transformer models simple and optimized out of the box.

Below is a list of available end-to-end tutorials on using Optimum Neuron via the Hugging Face DLC to train and deploy models on Amazon SageMaker. Follow the end-to-end examples to learn how Optimum Neuron integrates with SageMaker through the Hugging Face DLC images to unlock performance and cost benefits.

## Deploy Embedding Models on Inferentia2 for Efficient Similarity Search

Tutorial on how to deploy a text embedding model (BGE-Base) for efficient and fast embedding generation on AWS Inferentia2 using Amazon SageMaker; The post shows how Inferentia2 can be a great option for not only efficient and fast but also cost-effective inference of embeddings compared to GPUs or services like OpenAI and Amazon Bedrock.

- [Tutorial](https://www.philschmid.de/inferentia2-embeddings)
- [GitHub Repo](https://github.com/philschmid/huggingface-inferentia2-samples/blob/main/llama2-7b/sagemaker-notebook.ipynb)

## Deploy Llama 2 7B on AWS inferentia2 with Amazon SageMaker

Tutorial on how to deploy the conversational Llama 2 model on AWS Inferentia2 using Amazon SageMaker for low-latency inference; Shows how to leverage Inferentia2 and SageMaker to go from model training to production deployment with just a few lines of code.

- [Tutorial](https://www.philschmid.de/inferentia2-llama-7b)
- [GitHub Repo](https://github.com/philschmid/huggingface-inferentia2-samples/blob/main/stable-diffusion-xl/sagemaker-notebook.ipynb)

## Deploy Stable Diffusion XL on AWS inferentia2 with Amazon SageMaker

Tutorial on how to deploy Stable Diffusion XL model on AWS Inferentia2 using Optimum Neuron and Amazon SageMaker for efficient 1024x1024 image generation achieving ~6 seconds per image; The post shows how a single `inf2.xlarge` instance costing $0.99/hour can achieve ~10 images per minute, making Inferentia2 a great option for not only efficient and fast but also cost-effective inference of images compared to GPUs.

- [Tutorial](https://www.philschmid.de/inferentia2-stable-diffusion-xl) 
- [GitHub Repo](https://github.com/Placeholder/stable-diffusion-inferentia)

## Deploy BERT for Text Classification on AWS inferentia2 with Amazon SageMaker

Tutorial on how to optimize and deploy BERT model on AWS Inferentia2 using Optimum Neuron and Amazon SageMaker for efficient text classification achieving 4ms latency; The post shows how a single inf2.xlarge instance costing $0.99/hour can achieve 116 inferences/sec and 500 inferences/sec without network overhead, making Inferentia2 a great option for low-latency and cost-effective inference compared to GPUs.

- [Tutorial](https://www.philschmid.de/optimize-deploy-bert-inf2)
- [GitHub Repo](https://github.com/philschmid/huggingface-inferentia2-samples/blob/main/bert-transformers/sagemaker-notebook.ipynb)

