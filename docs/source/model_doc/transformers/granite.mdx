<!---
Copyright 2025 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
# Granite

## Overview

[Granite](https://huggingface.co/papers/2408.13359) is a 3B parameter language model trained with the Power scheduler. Discovering a good learning rate for pretraining large language models is difficult because it depends on so many variables (batch size, number of training tokens, etc.) and it is expensive to perform a hyperparameter search. The Power scheduler is based on a power-law relationship between the variables and their transferability to larger models. Combining the Power scheduler with Maximum Update Parameterization (MUP) allows a model to be pretrained with one set of hyperparameters regardless of all the variables.

You can find all the original Granite checkpoints under the [IBM-Granite](https://huggingface.co/ibm-granite) organization.

## Export to Neuron

To deploy ðŸ¤— [Transformers](https://huggingface.co/docs/transformers/index) models on Neuron devices, you first need to compile the models and export them to a serialized format for inference. Below are two approaches to compile the model, you can choose the one that best suits your needs. Here we take the `text-generation` as an example:

### Option 1: CLI
  
You can export the model using the Optimum command-line interface as follows:

```bash
optimum-cli export neuron --model ibm-granite/granite-3.3-2b-base --task text-generation --batch_size 1 --sequence_length 128 granite_text_generation_neuronx/
```

> [!TIP]
> Execute `optimum-cli export neuron --help` to display all command line options and their description.

### Option 2: Python API

```python
from optimum.neuron import NeuronModelForCausalLM

input_shapes = {"batch_size": 1, "sequence_length": 128}
compiler_args = {"auto_cast": "matmul", "auto_cast_type": "bf16"}
neuron_model = NeuronModelForCausalLM.from_pretrained(
    "ibm-granite/granite-3.3-2b-base",
    export=True,
    **input_shapes,
    **compiler_args,
)

# Save locally
neuron_model.save_pretrained("granite_text_generation_neuronx")

# Upload to the HuggingFace Hub
neuron_model.push_to_hub(
    "granite_text_generation_neuronx", repository_id="my-neuron-repo"  # Replace with your HF Hub repo id
)
```

## NeuronGraniteDecoderLayer

[[autodoc]] models.inference.granite.NeuronGraniteDecoderLayer

## NxDGraniteEmbedding

[[autodoc]] models.inference.granite.NxDGraniteEmbedding

## NxDGraniteHead

[[autodoc]] models.inference.granite.NxDGraniteHead

## NxDGraniteModel

[[autodoc]] models.inference.granite.NxDGraniteModel

## GraniteNxDModelForCausalLM

[[autodoc]] models.inference.granite.GraniteNxDModelForCausalLM