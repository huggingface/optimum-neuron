# coding=utf-8
# Copyright 2025 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# Adapted from https://github.com/aws-neuron/neuronx-distributed-inference/blob/main/src/neuronx_distributed_inference/models/gpt_oss/modeling_gpt_oss.py
"""PyTorch GPT-OSS model for NXD inference."""

import gc
import logging
import math
from typing import Optional, Tuple

import torch
import torch.nn as nn
from neuronx_distributed.parallel_layers.layers import (
    ColumnParallelLinear,
    ParallelEmbedding,
    RowParallelLinear,
)
from torch import nn as nn_module

from ..backend.config import NxDNeuronConfig
from ..backend.modules.attention.attention_base import NeuronAttentionBase
from ..backend.modules.attention.utils import RotaryEmbedding
from ..backend.modules.decoder import NxDDecoderModelForCausalLM, NxDModelForCausalLM
from ..backend.modules.moe import initialize_moe_module
from ..backend.modules.rms_norm import NeuronRMSNorm

logger = logging.getLogger(__name__)


def dequantize_mxfp4_weight(blocks: torch.Tensor, scales: torch.Tensor) -> torch.Tensor:
    """
    Dequantize MXFP4 weights to bfloat16.

    MXFP4 format stores weights as 4-bit values with shared exponent scales.
    Each block of values shares a scale factor.

    Args:
        blocks: Quantized weight blocks with packed 4-bit values
        scales: Scale factors for each block

    Returns:
        Dequantized bfloat16 tensor
    """
    # MXFP4 lookup table for converting 4-bit values to float
    # Values represent the mantissa for each 4-bit code
    mxfp4_lut = torch.tensor([
        0.0, 0.5, 1.0, 1.5, 2.0, 3.0, 4.0, 6.0,
        -0.0, -0.5, -1.0, -1.5, -2.0, -3.0, -4.0, -6.0
    ], dtype=torch.float32, device=blocks.device)

    # Get original shape from scales
    original_shape = scales.shape[:-1]  # Remove last dimension (block dimension)
    elements_per_block = 32  # MXFP4 uses 32 elements per block

    # Unpack 4-bit values from blocks
    # blocks is uint8, each byte contains 2 4-bit values
    block_flat = blocks.reshape(-1)
    unpacked = torch.zeros(block_flat.numel() * 2, dtype=torch.uint8, device=blocks.device)
    unpacked[0::2] = (block_flat >> 4) & 0x0F  # Upper 4 bits
    unpacked[1::2] = block_flat & 0x0F  # Lower 4 bits

    # Look up float values
    float_values = mxfp4_lut[unpacked.long()]

    # Reshape to blocks and apply scales
    num_blocks = scales.numel()
    float_values = float_values[:num_blocks * elements_per_block].reshape(num_blocks, elements_per_block)
    scales_expanded = scales.reshape(-1, 1)

    # Dequantize: multiply by scale
    dequantized = float_values * scales_expanded

    # Reshape to original shape
    total_elements = original_shape.numel() if len(original_shape) > 0 else 1
    dequantized = dequantized.reshape(-1)[:total_elements]
    dequantized = dequantized.reshape(original_shape)

    return dequantized.to(torch.bfloat16)


class GptOssRotaryEmbedding(nn.Module):
    """GPT-OSS RoPE with YaRN scaling support."""

    def __init__(self, dim, base=10000.0, max_position_embeddings=2048,
                 initial_context_length=None, scaling_factor=1.0,
                 ntk_alpha=1.0, ntk_beta=1.0):
        super().__init__()
        self.dim = dim
        self.base = base
        self.max_position_embeddings = max_position_embeddings
        self.initial_context_length = initial_context_length or max_position_embeddings
        self.scaling_factor = scaling_factor
        self.ntk_alpha = ntk_alpha
        self.ntk_beta = ntk_beta
        self.register_buffer("inv_freq", None, persistent=False)
        self.inv_freq_calculated = False
        self.concentration_value = 1.0

    def get_inv_freqs_and_concentration(self, device):
        freq = self.base ** (
            torch.arange(0, self.dim, 2, dtype=torch.float, device=device) / self.dim
        )
        if self.scaling_factor > 1.0:
            concentration = 0.1 * math.log(self.scaling_factor) + 1.0
            d_half = self.dim / 2
            low = (d_half * math.log(self.initial_context_length / (self.ntk_beta * 2 * math.pi)) / math.log(self.base))
            high = (d_half * math.log(self.initial_context_length / (self.ntk_alpha * 2 * math.pi)) / math.log(self.base))
            assert 0 < low < high < d_half - 1
            interpolation = 1.0 / (self.scaling_factor * freq)
            extrapolation = 1.0 / freq
            ramp = (torch.arange(d_half, dtype=torch.float32, device=freq.device) - low) / (high - low)
            mask = 1 - ramp.clamp(0, 1)
            inv_freq = interpolation * (1 - mask) + extrapolation * mask
        else:
            concentration = 1.0
            inv_freq = 1.0 / freq
        return inv_freq, concentration

    @torch.no_grad()
    def forward(self, x, position_ids):
        # x: [bs, num_attention_heads, seq_len, head_size]
        if not self.inv_freq_calculated:
            self.inv_freq, self.concentration_value = self.get_inv_freqs_and_concentration(x.device)
            self.inv_freq_calculated = True
        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)
        position_ids_expanded = position_ids[:, None, :].float()
        freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
        emb = torch.cat((freqs, freqs), dim=-1)
        cos = emb.cos() * self.concentration_value
        sin = emb.sin() * self.concentration_value
        # Return with shape [bs, seq_len, head_dim] (apply_rotary_pos_emb will unsqueeze)
        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)


class NeuronGptOssAttention(NeuronAttentionBase):
    """
    GPT-OSS attention with sliding window and learned sink tokens.
    Supports GQA (Grouped Query Attention).
    """

    def __init__(self, config, neuron_config: NxDNeuronConfig):
        super().__init__(config, neuron_config)

        # GPT-OSS specific parameters
        self.sliding_window = getattr(config, "sliding_window", None)
        self.num_sink_tokens = getattr(config, "num_sink_tokens", 4)

        # Initialize rotary embeddings with YaRN support
        head_dim = config.hidden_size // config.num_attention_heads
        self.rotary_emb = GptOssRotaryEmbedding(
            dim=head_dim,
            base=config.rope_theta,
            max_position_embeddings=config.max_position_embeddings,
            initial_context_length=getattr(config, "initial_context_length", config.max_position_embeddings),
            scaling_factor=getattr(config, "rope_scaling_factor", 1.0),
            ntk_alpha=getattr(config, "rope_ntk_alpha", 1.0),
            ntk_beta=getattr(config, "rope_ntk_beta", 1.0),
        )


class NeuronGptOssDecoderLayer(nn.Module):
    """
    GPT-OSS decoder layer with attention and MoE.
    Uses initialize_moe_module for proper Neuron-compatible MoE implementation.
    """

    def __init__(self, config, neuron_config: NxDNeuronConfig, layer_idx: int = 0):
        super().__init__()
        self.hidden_size = config.hidden_size

        # Self-attention
        self.self_attn = NeuronGptOssAttention(config, neuron_config)

        # MoE using initialize_moe_module (following Mixtral/Qwen3 pattern)
        self.mlp = initialize_moe_module(
            neuron_config=neuron_config,
            num_experts=config.num_local_experts,
            top_k=config.num_experts_per_tok,
            hidden_size=config.hidden_size,
            intermediate_size=config.intermediate_size,
            hidden_act=config.hidden_act,
        )

        # Layer norms
        self.input_layernorm = NeuronRMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = NeuronRMSNorm(config.hidden_size, eps=config.rms_norm_eps)

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.Tensor] = None,
        past_key_value: Optional[Tuple[torch.Tensor]] = None,
        **kwargs,
    ):
        """
        Forward pass through decoder layer.

        Args:
            hidden_states: Input tensor [batch_size, seq_len, hidden_size]
            attention_mask: Attention mask
            position_ids: Position indices
            past_key_value: Cached key/value states

        Returns:
            Tuple of (output_hidden_states, present_key_value, cos_cache, sin_cache)
        """
        residual = hidden_states

        # Self-attention with pre-norm
        hidden_states = self.input_layernorm(hidden_states)
        hidden_states, present_key_value, cos_cache, sin_cache = self.self_attn(
            hidden_states,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_value=past_key_value,
            **kwargs,
        )
        hidden_states = residual + hidden_states

        # MoE with pre-norm
        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        # MoE returns tuple: (output, router_logits)
        hidden_states = self.mlp(hidden_states)[0]
        hidden_states = residual + hidden_states

        return hidden_states, present_key_value, cos_cache, sin_cache


class NxDGptOssModel(NxDDecoderModelForCausalLM):
    """Base GPT-OSS model."""

    _decoder_layer_cls = NeuronGptOssDecoderLayer

    def __init__(self, config, neuron_config: NxDNeuronConfig):
        super().__init__(config, neuron_config)

        self.embed_tokens = ParallelEmbedding(
            config.vocab_size,
            config.hidden_size,
            config.pad_token_id,
            dtype=neuron_config.torch_dtype,
            shard_across_embedding=True,
        )
        self.layers = nn.ModuleList(
            [
                NeuronGptOssDecoderLayer(config, neuron_config, layer_idx)
                for layer_idx in range(config.num_hidden_layers)
            ]
        )
        self.norm = NeuronRMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.lm_head = ColumnParallelLinear(
            config.hidden_size,
            config.vocab_size,
            gather_output=not neuron_config.on_device_sampling,
            bias=False,
        )


class GptOssNxDModelForCausalLM(NxDModelForCausalLM):
    """
    GPT-OSS model for causal language modeling on Neuron.
    Extends NxDModelForCausalLM for Neuron-specific tracing and compilation.
    """

    _model_cls = NxDGptOssModel

    @staticmethod
    def convert_hf_to_neuron_state_dict(state_dict: dict, config, neuron_config: NxDNeuronConfig) -> dict:
        """
        Convert HuggingFace format state dict to Neuron format.

        Handles:
        1. MXFP4 dequantization (HF checkpoints are quantized)
        2. MoE expert weight batching (following Mixtral pattern)
        3. Prefix conversion (model. -> no prefix)

        Args:
            state_dict: HuggingFace checkpoint state dict
            config: Model configuration
            neuron_config: Neuron-specific configuration

        Returns:
            Converted state dict for Neuron model
        """
        # First pass: dequantize MXFP4 weights
        dequantized_state_dict = {}
        keys_to_remove = set()

        for key in state_dict.keys():
            # Check if this is a quantized weight (has corresponding _blocks and _scales)
            if key.endswith("_blocks"):
                # Get base key without _blocks suffix
                base_key = key[:-7]  # Remove "_blocks"
                scales_key = base_key + "_scales"

                if scales_key in state_dict:
                    # Dequantize the weight
                    blocks = state_dict[key]
                    scales = state_dict[scales_key]

                    logger.info(f"Dequantizing MXFP4 weight: {base_key}")
                    dequantized = dequantize_mxfp4_weight(blocks, scales)

                    # Store dequantized weight
                    dequantized_state_dict[base_key] = dequantized

                    # Mark quantized tensors for removal
                    keys_to_remove.add(key)
                    keys_to_remove.add(scales_key)
            elif key.endswith("_scales"):
                # Skip scales, they're handled with blocks
                if key[:-7] + "_blocks" in state_dict:
                    keys_to_remove.add(key)

        # Merge dequantized weights with original state dict
        for key, value in state_dict.items():
            if key not in keys_to_remove:
                dequantized_state_dict[key] = value

        # Second pass: convert HuggingFace "model." prefix to Neuron format
        new_state_dict = {}
        for key, value in dequantized_state_dict.items():
            if key.startswith("model."):
                key = key[6:]  # Remove "model." prefix

            # Skip MoE expert weights and router - handled separately below
            if ".mlp.experts." in key or key.endswith("mlp.router"):
                continue

            new_state_dict[key] = value

        state_dict = new_state_dict

        # Third pass: Convert MoE expert weights to batched format (following Mixtral pattern)
        for l in range(config.num_hidden_layers):
            # Handle router weights
            router_weight_key = f"layers.{l}.mlp.router.weight"
            if router_weight_key in dequantized_state_dict:
                router_weight = dequantized_state_dict[router_weight_key]
                state_dict[f"layers.{l}.mlp.router.linear_router.weight"] = router_weight.detach().clone()

            # Check for gate_up_proj (GPT-OSS has combined gate and up projections)
            gate_up_key = f"layers.{l}.mlp.experts.gate_up_proj"
            down_key = f"layers.{l}.mlp.experts.down_proj"

            # Check if expert weights exist in dequantized state dict
            if gate_up_key not in dequantized_state_dict:
                continue

            gate_up_proj = dequantized_state_dict[gate_up_key]
            down_proj = dequantized_state_dict[down_key]

            # GPT-OSS format: [num_experts, 2*intermediate, hidden]
            num_experts = gate_up_proj.shape[0]
            combined_intermediate = gate_up_proj.shape[1]  # 2*intermediate
            hidden_size = gate_up_proj.shape[2]
            intermediate_size = combined_intermediate // 2

            # Create batched gate_up_proj tensor [num_experts, hidden, 2*intermediate]
            # Need to transpose from [num_experts, 2*intermediate, hidden] to [num_experts, hidden, 2*intermediate]
            gate_up_proj_transposed = gate_up_proj.permute(0, 2, 1).contiguous()
            state_dict[f"layers.{l}.mlp.expert_mlps.mlp_op.gate_up_proj.weight"] = gate_up_proj_transposed

            # Create batched down_proj tensor [num_experts, intermediate, hidden]
            # down_proj is [num_experts, hidden, intermediate], transpose to [num_experts, intermediate, hidden]
            down_proj_transposed = down_proj.permute(0, 2, 1).contiguous()
            state_dict[f"layers.{l}.mlp.expert_mlps.mlp_op.down_proj.weight"] = down_proj_transposed

            # Clean up memory
            gc.collect()

        return state_dict

    @staticmethod
    def update_state_dict_for_tied_weights(state_dict):
        """Handle tied weights between embedding and output layer."""
        state_dict["lm_head.weight"] = state_dict["embed_tokens.weight"].clone()

    @classmethod
    def _get_neuron_config(
        cls,
        checkpoint_id: str,
        checkpoint_revision: str,
        instance_type: str,
        batch_size: int,
        sequence_length: int,
        tensor_parallel_size: int,
        dtype: torch.dtype,
    ):
        """Generate NxDNeuronConfig for GPT-OSS from user parameters."""
        return NxDNeuronConfig(
            checkpoint_id=checkpoint_id,
            checkpoint_revision=checkpoint_revision,
            batch_size=batch_size,
            sequence_length=sequence_length,
            tp_degree=tensor_parallel_size,
            torch_dtype=dtype,
            target=instance_type,
        )
